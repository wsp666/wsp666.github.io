<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习(八)</title>
      <link href="/posts/f7d407e6.html"/>
      <url>/posts/f7d407e6.html</url>
      
        <content type="html"><![CDATA[<h1 id="提升方法"><a href="#提升方法" class="headerlink" title="提升方法"></a>提升方法</h1><h2 id="笔记摘要"><a href="#笔记摘要" class="headerlink" title="笔记摘要"></a>笔记摘要</h2><ul><li>在PAC（概率近似正确(PAC, Probably approximately correct)）学习框架下，一个概念是强可学习的<strong>充分必要条件</strong>是这个概念是弱可学习的。</li><li>提升方法的两个问题</li></ul><ol><li>在每一轮如何改变训练数据的权值或概率分布</li><li>如何将<strong>弱分类器</strong>组合成一个<strong>强分类器</strong></li></ol><ul><li>Adaboost的解决方案：</li></ul><ol><li>提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值</li><li>加权多数表决的方法，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值</li></ol><h3 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h3><ul><li>输入：弱学习算法和训练数据集<br>$T=\{(x_1,y_1), (x_2,y_2),…,(x_N,y_N)\}, x\in X \subseteq R^n$</li><li>输出：最终分类器$G(x)$</li><li>步骤</li></ul><ol><li>初始化训练数据的权值分布 $D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N},w_{1i}=\frac{1}{N})​$</li><li>m = 1,2, $\cdots$,M<br> ( a ) 使用具有权值分布$D_m$的训练数据集学习，得到基本的分类器 <script type="math/tex; mode=display">G_m(x):X→\{-1,+1\}</script> ( b ) 计算$G_m(x)$在训练集上的分类误差率  <script type="math/tex; mode=display">e_m=\sum_{i=1}^{N}P(G_m(x_i)\not= y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\not=y_i)</script> ( c ) 计算$G_m(x)$的系数<script type="math/tex; mode=display">\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}</script> ( d ) 更新训练数据集的权值分布<script type="math/tex; mode=display">w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))​</script><script type="math/tex; mode=display">Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))</script></li><li>$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$</li><li>最终分类器$G(x)=sign(f(x))=sign(\sum_{m=1}^M\alpha_mG_m(x))$</li></ol><ul><li>误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用， 这是AdaBoost的一个特点</li><li>利用基本分类器的线性组合构建最终分类器使AdaBoost的另一特点</li></ul><h3 id="AdaBoost算法的训练误差分析"><a href="#AdaBoost算法的训练误差分析" class="headerlink" title="AdaBoost算法的训练误差分析"></a>AdaBoost算法的训练误差分析</h3><ul><li>AdaBoost算法最终分类器的训练误差界为<script type="math/tex; mode=display">\frac{1}{N}\sum\limits_{i=1}\limits^N I(G(x_i)\neq y_i)\le\frac{1}{N}\sum\limits_i\exp(-y_i f(x_i))=\prod\limits_m Z_m</script>这个的意思就是说指数损失是0-1损失的上界，这个上界使通过递推得到的，是归一化系数的连乘</li></ul><h3 id="AdaBoost算法的解释"><a href="#AdaBoost算法的解释" class="headerlink" title="AdaBoost算法的解释"></a>AdaBoost算法的解释</h3><ul><li>模型为加法模型， 损失函数为指数函数， 学习算法为前向分步算法时的二分类学习方法。根据这些条件可以推导出AdaBoost</li></ul><h4 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h4><ul><li><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N, y_N)}, x_i \in  X \subseteq R^n, y_i\in \{-1, 1\}$， 损失函数$L(y, f(x))$; 基函数集合$\{b(x;\gamma)\}$</p></li><li><p>输出：加法模型$f(x)$</p></li><li><p>步骤：</p></li></ul><ol><li><p>初始化$f_0(x)=0$</p></li><li><p>对$m=1,2,\cdots,M$, 极小化损失函数</p><script type="math/tex; mode=display">(\beta_m,\gamma_m)=\arg\min \limits_ {\beta,\gamma}\sum_{i=1}^NL(y_i, f_{m-1}(x_i)+\beta b(x_i;\gamma))</script></li><li><p>更新</p><script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+\beta _mb(x;\gamma_m)</script></li><li><p>得到加法模型</p><script type="math/tex; mode=display">f(x)=f_M(x)=\sum_{m=1}^M\beta_m b(x;\gamma_m)</script></li></ol><h3 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h3><ul><li>提升树是以分类树或回归树为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一</li><li>提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法</li></ul><h4 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h4><ul><li>以决策树为基函数的提升方法称为提升树</li><li>提升树模型可以表示成决策树的加法模型<script type="math/tex; mode=display">f_M(x)=\sum_{m=1}^MT(x;\Theta_m)</script></li></ul><h4 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h4><ul><li>针对不同问题的提升树学习算法， 其主要区别在于使用的损失函数不同：</li></ul><ol><li>平方误差损失函数用于回归问题</li><li>指数损失函数用于分类问题</li><li>一般损失函数的一般决策问题</li></ol><h4 id="回归问题的提升树算法"><a href="#回归问题的提升树算法" class="headerlink" title="回归问题的提升树算法"></a>回归问题的提升树算法</h4><ul><li><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N, y_N)}, x_i \in  X \subseteq R^n,y_i \in Y \subseteq R$</p></li><li><p>输出：提升树$f_M(x)$</p></li><li><p>步骤：</p></li></ul><ol><li>初始化$f_0(x)=0$</li><li>对$m=1,2,\dots,M$<ol><li>计算残差<script type="math/tex; mode=display">r_{mi}=y_i-f_{m-1}(x_i), i=1,2,\dots,N</script></li><li><strong>拟合残差</strong>$r_{mi}$学习一个回归树，得到$T(x;\Theta_m)$</li><li>更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$</li></ol></li><li>得到回归问题提升树<script type="math/tex; mode=display">f(x)=f_M(x)=\sum_{m=1}^MT(x;\Theta_m)</script></li></ol><h3 id="梯度提升-GBDT"><a href="#梯度提升-GBDT" class="headerlink" title="梯度提升(GBDT)"></a>梯度提升(GBDT)</h3><p>输入： 训练数据集$T={(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}, x_i \in  x \subseteq R^n, y_i \in  y \subseteq R$；损失函数$L(y,f(x))$<br>输出：回归树$\hat{f}(x)$<br>步骤：</p><ol><li><p>初始化</p><script type="math/tex; mode=display">f_0(x)=\arg\min\limits_c\sum_{i=1}^NL(y_i, c)</script></li><li><p>对$m=1,2,\cdots,M$</p></li></ol><p>（ a ）对$i=1,2,\cdots,N$,计算</p><script type="math/tex; mode=display">   r_{mi}=-\left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x)=f_{m-1}(x)}</script><p>（ b ）对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{mj}, j=1,2,\dots,J$</p><p>   （ c  ）   对$j=1,2,\dots,J$，计算</p><script type="math/tex; mode=display">   c_{mj}=\arg\min_c\sum_{xi\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)</script><ol><li><p>更新</p><script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x\in R_{mj})</script></li><li><p>得到回归树</p><script type="math/tex; mode=display">\hat{f}(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x\in R_{mj})</script></li></ol><h2 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h2><ul><li><p>比较支持向量机、 AdaBoost 、逻辑斯谛回归模型的学习策略与算法。</p><ul><li>支持向量机的学习策略是当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，其学习算法是SMO序列最小最优化算法</li><li>AdaBoost的学习策略是通过极小化加法模型的指数损失，得到一个强分类器，其学习算法是前向分步算法</li><li>逻辑斯谛回归模型的学习策略是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计，其学习算法可以是改进的迭代尺度算法（IIS），梯度下降法，牛顿法以及拟牛顿法</li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(七)</title>
      <link href="/posts/3e86b8ea.html"/>
      <url>/posts/3e86b8ea.html</url>
      
        <content type="html"><![CDATA[<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="笔记摘要"><a href="#笔记摘要" class="headerlink" title="笔记摘要"></a>笔记摘要</h2><ul><li>SVM的基本模型是定义在特征空间上的间隔最大的线性分类器</li><li>线性可分支持向量机和线性支持向量机假设输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间的特征向量；非线性支持向量机利用一个从<strong>输入空间到特征空间的非线性映射</strong>将输入映射为特征向量</li><li>支持向量机的学习策略就是<strong>间隔最大化</strong>，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数最小化问题</li><li>仿射变换是保凸变换</li><li><p>通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机</p><h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3></li><li><p>对于给定数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为</p><script type="math/tex; mode=display">\hat \gamma_i=y_i(w\cdot x_i+b)</script></li><li>定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即<script type="math/tex; mode=display">\hat \gamma=\min_{i=1,\cdots,N}\hat\gamma_i</script></li><li>函数间隔可以表示分类预测的<strong>正确性</strong>及<strong>确信度</strong></li></ul><h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><ul><li>对于给定数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为<script type="math/tex; mode=display"> \gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})</script></li><li>定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即<script type="math/tex; mode=display"> \gamma=\min_{i=1,\cdots,N}\hat\gamma_i</script></li><li>超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离</li><li>如果超平面参数成比例地改变，此时超平面没有发生改变，但函数间隔按此比例改变，而几何间隔不变<h3 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h3></li><li><p>问题描述</p><script type="math/tex; mode=display">  \begin{aligned}  &\min_{w,b}\frac{1}{2}||w||^2\\  &s.t.\ \ \ y_i(w\cdot x_i+b)-1\geqslant0,i=1,2,\dots,N\\  \end{aligned}</script></li><li><p>这是个凸二次规划问题，如果求出了上述方程的解$w^<em>, b^</em>$，就可得到分离超平面</p><script type="math/tex; mode=display">  w^*\cdot x+b^*=0</script></li><li>以及相应的分类决策函数<script type="math/tex; mode=display">  f(x)=sign(w^*\cdot x+b^*)</script></li></ul><h4 id="对偶算法"><a href="#对偶算法" class="headerlink" title="对偶算法"></a>对偶算法</h4><ul><li>通过求解对偶问题得到原始问题地最优解的优点</li></ul><ol><li>对偶问题往往更容易求解</li><li>自然引入核函数，进而推广到非线性分类问题</li></ol><ul><li>针对每个不等式约束，定义拉格朗日乘子$\alpha_i\ge0$，定义拉格朗日函数<script type="math/tex; mode=display">\begin{aligned}L(w,b,\alpha)&=\frac{1}{2}w\cdot w-\left[\sum_{i=1}^N\alpha_i[y_i(w\cdot x_i+b)-1]\right]\\&=\frac{1}{2}\left\|w\right\|^2-\left[\sum_{i=1}^N\alpha_i[y_i(w\cdot x_i+b)-1]\right]\\&=\frac{1}{2}\left\|w\right\|^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i\end{aligned}</script><script type="math/tex; mode=display">\alpha_i \geqslant0, i=1,2,\dots,N</script>其中$\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)^T​$为拉格朗日乘子向量</li><li><strong>原始问题是极小极大问题</strong>，根据<strong>拉格朗日对偶性</strong>，原始问题的<strong>对偶问题是极大极小问题</strong>:<script type="math/tex; mode=display">\max\limits_\alpha\min\limits_{w,b}L(w,b,\alpha)</script></li><li>转换后的对偶问题<script type="math/tex; mode=display">\min\limits_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i\\s.t. \ \ \ \sum_{i=1}^N\alpha_iy_i=0\\\alpha_i\geqslant0, i=1,2,\dots,N</script></li></ul><ul><li>根据KKT条件求解，其中$\alpha$不为零的点对应的实例为<strong>支持向量</strong>，通过支持向量可以求得$b$值</li></ul><script type="math/tex; mode=display">  \begin{aligned}  w^*&=\sum_{i=1}^{N}\alpha_i^*y_ix_i\\  b^*&=y_j\color{black}-\sum_{i=1}^{N}\alpha_i^*y_i(x_i\cdot x_j\color{black})  \end{aligned}</script><ul><li>$b^<em>$的求解，通过$\arg\max \alpha^</em>$实现，因为支持向量共线，所以通过任意支持向量求解都可以</li></ul><h3 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h3><ul><li><p>问题描述</p><script type="math/tex; mode=display">  \begin{aligned}  \min_{w,b,\xi} &\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^N\xi_i\\  s.t. \ \ \ &y_i(w\cdot x_i+b)\geqslant1-\xi_i, i=1,2,\dots,N\\  &\xi_i\geqslant0,i=1,2,\dots,N  \end{aligned}</script></li></ul><ul><li><p>对偶问题描述</p><ul><li>原始问题里面有两部分约束，涉及到两个拉格朗日乘子向量<script type="math/tex; mode=display">\begin{aligned}\min_\alpha\ &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i\\s.t.\ \ \ &\sum_{i=1}^N\alpha_iy_i=0\\&0\leqslant \alpha_i \leqslant C,i=1,2,\dots,N\end{aligned}</script>通过求解对偶问题， 得到$\alpha$，然后求解$w,b$的过程和之前一样</li></ul></li><li><p><strong>线性支持向量机的解$w^<em>$唯一但$b^</em>$不一定唯一</strong></p></li><li><p>线性支持向量机是线性可分支持向量机的超集</p></li></ul><h4 id="合页损失"><a href="#合页损失" class="headerlink" title="合页损失"></a>合页损失</h4><ul><li><p>最小化目标函数</p><script type="math/tex; mode=display">\min\limits_{w,b} \sum\limits_{i=1}^N\left[1-y_i(w\cdot x+b)\right]_++\lambda\left\|w\right\|^2</script></li><li><p>其中</p><ul><li>第一项是经验损失或经验风险，函数$L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+$称为合页损失，可以表示成$L = \max(1-y(w\cdot x+b), 0)$</li><li>第二项是<strong>系数为$\lambda$的$w$的$L_2$范数的平方</strong>，是正则化项</li></ul></li><li><p>书中通过定理7.4说明了用合页损失表达的最优化问题和线性支持向量机原始最优化问题的关系</p><script type="math/tex; mode=display">\begin{aligned}\min_{w,b,\xi} &\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^N\xi_i\\s.t. \ \ \ &y_i(w\cdot x_i+b)\geqslant1-\xi_i, i=1,2,\dots,N\\&\xi_i\geqslant0,i=1,2,\dots,N\end{aligned}</script><ul><li>等价于<script type="math/tex; mode=display">\min\limits_{w,b} \sum\limits_{i=1}^N\left[1-y_i(w\cdot x+b)\right]_++\lambda\left\|w\right\|^2</script></li></ul></li><li>证明如下</li><li>令合页损失$\left[1-y_i(w\cdot x+b)\right]_+=\xi_i$，合页损失非负，所以有$\xi_i\ge0$，这个对应了原始最优化问题中的<strong>第二个约束</strong></li></ul><ul><li><p>还是根据合页损失非负，当$1-y_i(w\cdot x+b)\leq\color{red}0​$的时候，有$\left[1-y_i(w\cdot x+b)\right]_+=\color{red}\xi_i=0​$，所以有$1-y_i(w\cdot x+b)\leq\color{red}0=\xi_i$，这对应了原始最优化问题中的<strong>第一个约束</strong></p></li><li><p>所以，在满足这<strong>两个约束</strong>的情况下，有</p><script type="math/tex; mode=display">\begin{aligned}\min\limits_{w,b} &\sum\limits_{i=1}^N\left[1-y_i(w\cdot x+b)\right]_++\lambda\left\|w\right\|^2\\\min\limits_{w,b} &\sum\limits_{i=1}^N\xi_i+\lambda\left\|w\right\|^2\\\min\limits_{w,b} &\frac{1}{C}\left(\frac{1}{2}\left\|w\right\|^2+C\sum\limits_{i=1}^N\xi_i\right), with \  \lambda=\frac{1}{2C}\\\end{aligned}</script></li><li><p>合页损失函数<br><img src="https://img-blog.csdnimg.cn/20190626153036271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h3></li><li><p>核技巧的想法是在学习和预测中只定义核函数$K(x,z)$，而不是显式的定义映射函数$\phi$</p></li><li><p>通常，直接计算$K(x,z)$比较容易， 而通过$\phi(x)$和$\phi(z)$计算$K(x,z)$并不容易。</p><script type="math/tex; mode=display">W(\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i\\</script><script type="math/tex; mode=display">f(x)=sign\left(\sum_{i=1}^{N_s}\alpha_i^*y_i\phi(x_i)\cdot \phi(x)+b^*\right)=sign\left(\sum_{i=1}^{N_s}\alpha_i^*y_iK(x_i,x)+b^*\right)</script><p>学习是隐式地在特征空间进行的，不需要显式的定义特征空间和映射函数</p></li></ul><h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><ul><li><p>对于给定的核$K(x,z)$，特征空间$\mathcal H$和映射函数$\phi(x)$的取法并不唯一，可以取不同的特征空间，即便是同一特征空间里也可以取不同的映射</p></li><li><p>下面这个例子里面$\phi(x)$实现了从低维空间到高维空间的映射</p><script type="math/tex; mode=display">K(x,z)=(x\cdot z)^2\\{X}=\R^2, x=(x^{(1)},x^{(2)})^T\\{H}=\R^3, \phi(x)=((x^{(1)})^2, \sqrt2x^{(1)}x^{(2)}, (x^{(2)})^2)^T\\{H}=\R^4, \phi(x)=((x^{(1)})^2, x^{(1)}x^{(2)}, x^{(1)}x^{(2)}, (x^{(2)})^2)^T\\</script></li></ul><ul><li>核具有再生性，即满足下面条件的核称为再生核<script type="math/tex; mode=display">K(\cdot,x)\cdot f=f(x)\\K(\cdot,x)\cdot K(\cdot, z)=K(x,z)</script></li><li>通常所说的核函数就是<strong>正定核函数</strong></li><li><p>问题描述</p><ul><li>将向量内积替换成了核函数，而SMO算法求解的问题正是该问题</li><li><p>构建最优化问题：</p><script type="math/tex; mode=display">\begin{aligned}\min_\alpha\ &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i\\s.t.\ \ \ &\sum_{i=1}^N\alpha_iy_i=0\\&0\leqslant \alpha_i \leqslant C,i=1,2,\dots,N\end{aligned}</script></li><li><p>求解得到$\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,\cdots,\alpha_N^</em>)^T$<br>选择$\alpha^*$的一个正分量计算</p><script type="math/tex; mode=display">b^*=y_j-\sum_{i=1}^N\alpha_i^*y_iK(x_i,x_j)</script><p>构造决策函数</p><script type="math/tex; mode=display">f(x)=sign\left(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*\right)</script></li></ul></li></ul><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><script type="math/tex; mode=display">\begin{aligned}\min_\alpha\ &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i\\s.t.\ \ \ &\sum_{i=1}^N\alpha_iy_i=0\\&0\leqslant \alpha_i \leqslant C,i=1,2,\dots,N\end{aligned}</script><p>这个问题中，变量是$\alpha$，一个变量$\alpha_i$对应一个样本点$(x_i,y_i)$，变量总数等于$N$</p><h3 id="KKT-条件"><a href="#KKT-条件" class="headerlink" title="KKT 条件"></a>KKT 条件</h3><ul><li>KKT条件是该最优化问题的充分必要条件</li><li>简单来说，约束最优化问题包含$\leqslant0$，和$=0$两种约束条件<script type="math/tex; mode=display">\begin{aligned}  \min_{x \in R^n}\quad &f(x) \\ s.t.\quad&c_i(x) \leqslant 0 , i=1,2,\ldots,k\\ &h_j(x) = 0 , j=1,2,\ldots,l\end{aligned}</script><ul><li>引入广义拉格朗日函数</li></ul></li></ul><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x) + \sum_{i=0}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)</script><ul><li>在KKT的条件下，原始问题和对偶问题的最优值相等<script type="math/tex; mode=display">∇_xL(x^∗,α^∗,β^∗)=0\\∇_αL(x^∗,α^∗,β^∗)=0\\∇_βL(x^∗,α^∗,β^∗)=0\\α_i^∗c_i(x^*)=0,i=1,2,…,k\\c_i(x^*)≤0,i=1,2,…,k\\α^∗_i≥0,i=1,2,…,k\\h_j(x^∗)=0,j=1,2,…,l</script></li><li>前面三个条件是由解析函数的知识，对于各个变量的偏导数为0，后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束,第四个条件是<strong>KKT的对偶互补条件</strong></li></ul><h3 id="算法内容"><a href="#算法内容" class="headerlink" title="算法内容"></a>算法内容</h3><p>整个SMO算法包括两<strong>部分</strong>：</p><ol><li>求解两个变量二次规划的解析方法</li><li>选择变量的启发式方法</li></ol><script type="math/tex; mode=display">\begin{aligned}\min_\alpha\ &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)-\sum_{i=1}^N\alpha_i\\s.t.\ \ \ &\sum_{i=1}^N\alpha_iy_i=0\\&0\leqslant \alpha_i \leqslant C,i=1,2,\dots,N\end{aligned}</script><h4 id="Part-I"><a href="#Part-I" class="headerlink" title="Part I"></a>Part I</h4><ul><li>两个变量二次规划求解</li><li>选择两个变量$\alpha_1,\alpha_2​$，由等式约束可以得到$\alpha_1=-y_1\sum\limits_{i=2}^N\alpha_iy_i​$，所以这个问题等价于一个单变量优化问题</li></ul><script type="math/tex; mode=display">\begin{aligned}\min_{\alpha_1,\alpha_2} W(\alpha_1,\alpha_2)=&\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2\\&-(\alpha_1+\alpha_2)+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{il}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2}\\s.t. \ \ \ &\alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^Ny_i\alpha_i=\varsigma\\&0\leqslant\alpha_i\leqslant C, i=1,2\end{aligned}</script><ul><li>上面存在两个约束：</li></ul><ol><li><strong>线性</strong>等式约束</li><li>边界约束</li></ol><ul><li>根据简单的线性规划可以得出<strong>等式约束使得$(\alpha_1,\alpha_2)$在平行于盒子$[0,C]\times [0,C]$的对角线的直线上</strong></li></ul><p><img src="https://img-blog.csdnimg.cn/20190627105606348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ul><li><p>首先求沿着约束方向未经剪辑，即不考虑约束$0\leqslant\alpha_i\leqslant C$时$\alpha_2$的最优解，然后再求剪辑后的解</p><script type="math/tex; mode=display">E_i=g(x_i)-y_i=(\sum_{j=1}^N\alpha_jy_jK(x_i, x_j)+b)-y_i,i=1,2</script><p>$E_i$为函数$g(x)$对输入的预测值与真实输出$y_i$的差</p><h4 id="Part-II"><a href="#Part-II" class="headerlink" title="Part II"></a>Part II</h4></li><li><p>变量的选择方法</p></li></ul><ol><li>第一个变量$\alpha_1$外层循环，寻找违反KKT条件<strong>最严重</strong>的样本点</li><li>第二个变量$\alpha_2$内层循环，希望能使$\alpha_2$有足够大的变化</li><li>计算阈值$b$和差值$E_i$</li></ol><blockquote><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\dots, (x_N,y_N)}$，其中$x_i\in\mathcal X=\bf R^n, y_i\in\mathcal Y=\{-1,+1\}, i=1,2,\dots,N$,精度$\epsilon$</p><p>输出：近似解$\hat\alpha$</p><ol><li><p>取初值$\alpha_0=0$，令$k=0$</p></li><li><p><strong>选取</strong>优化变量$\alpha_1^{(k)},\alpha_2^{(k)}$，解析求解两个变量的最优化问题，求得最优解$\alpha_1^{(k+1)},\alpha_2^{(k+1)}$，更新$\alpha$为$\alpha^{k+1}$</p></li><li><p>若在精度$\epsilon$范围内满足停止条件</p><script type="math/tex; mode=display">\sum_{i=1}^{N}\alpha_iy_i=0\\0\leqslant\alpha_i\leqslant C,i=1,2,\dots,N\\y_i\cdot g(x_i)=\begin{cases}\geqslant1,\{x_i|\alpha_i=0\}\\=1,\{x_i|0<\alpha_i<C\}\\\leqslant1,\{x_i|\alpha_i=C\}\end{cases}\\g(x_i)=\sum_{j=1}^{N}\alpha_jy_jK(x_j,x_i)+b</script><p>则转4,否则，$k=k+1$转2</p></li><li><p>取$\hat\alpha=\alpha^{(k+1)}$</p></li></ol></blockquote><h1 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h1><ul><li>1.<strong>比较感知机的对偶形式与线性可分支持向量机的对偶形式</strong><ul><li>感知机的对偶形式<br>$f(x)=sign\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right),<br>\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$ </li><li>线性可分支持向量机的对偶形式<br>$f(x)=sign\left(\sum_{i=1}^N\alpha_i^<em>y_ix_i\cdot x+b^</em>\right),<br>\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,\cdots,\alpha_N^</em>)^T$<br>感知机学习算法的原始形式和对偶形式与线性可分支持向量机学习算法的原始形式和对偶形式相对应。在线性可分支持向量机的对偶形式中,$w$也是被表示为实例 $x_i$和标记$y_i$的线性组合的形式<script type="math/tex; mode=display">w=\sum_{i=1}^{N}\alpha_iy_ix_i</script>而它们的偏置$b$的形式不同，前者$b=\sum_{i=1}^{N}\alpha_iy_i$,而后者$b^<em>=y_j\color{black}-\sum_{i=1}^{N}\alpha_i^</em>y_i(x_i\cdot x_j)$</li></ul></li><li><p>2.<strong>已知正例点$x_1=(1,2)^T$，$x_2=(2,3)^T$，$x_3=(3,3)^T$,负例点$x_4=(2,1)^T$，$x_5=(3,2)^T$，<br>试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔边界及支持向量</strong></p><ul><li>根据书中算法，计算可得$w_1=-1$,$w_2=2$,$b=-2$,即最大间隔分离超平面为<script type="math/tex; mode=display">-x^{(1)}+2x^{(2)}-2=0</script>分类决策函数为<script type="math/tex; mode=display">f(x)=sign(-x^{(1)}+2x^{(2)}-2)</script><img src="https://img-blog.csdnimg.cn/20190708102330284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></li><li><p>3.<strong>线性支持向量机还可以定义为以下形式：</strong></p></li></ul><script type="math/tex; mode=display">\min_{w,b,\xi}{\frac{1}{2}\|w\|^2}+C\sum^N_{i=1}\xi_i^2\\s.t.{\quad}y_i(w{\cdot}x_i+b)\ge1-\xi_i,\,i=1,2,\cdots,N\\\xi_i\ge0,\,i=1,2,\cdots,N</script><pre><code>  **试求其对偶形式**</code></pre><ul><li><p>首先求得原始化最优问题的拉格朗日函数是：<br>$L(w,b,\alpha,\xi,μ)=\frac{1}{2}\left|w\right|^2+C\sum_{i=1}^N\xi_i^2-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b-1)+\xi_i)-\sum_{i=1}^Nμ_i\xi_i$</p><ul><li>对偶问题是拉格朗日的极大极小问题。首先求$L(w,b,\alpha,\xi,μ)$对$w,b,\xi$的极小,即对该三项求偏导，得<script type="math/tex; mode=display">w=\sum_{i=1}^{N}\alpha_iy_ix_i\\\sum_{i=1}^N\alpha_iy_i=0\\2C\xi_i-\alpha_i-μ_i=0</script>将上述带入拉格朗日函数，得<script type="math/tex; mode=display">-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-C\sum_{i=1}^N\xi_i^2+\sum_{i=1}^N\alpha_i\\-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\frac{1}{4C}\sum_{i=1}^N(\alpha_i+μ_i)^2+\sum_{i=1}^N\alpha_i</script></li><li>4.<strong>证明内积的正整数幂函数$K(x,z)=(x{\cdot}z)^p$是正定核函数，这里$p$是正整数，$x,z{\in}R^n$</strong></li><li>要证明正整数幂函数是正定核函数，只需证明其对应得Gram矩阵$K=[K(x_i,x_j)]_{m\times m}$是半正定矩阵</li><li><p>对任意$c_1,c_2…c_m\in R$,有</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i,j=1}^{m}c_ic_jK(x_i,x_j)\\=&\sum_{i,j=1}^{m}c_ic_j(x_i\cdot x_j)^p\\=&(\sum_{i=1}^{m}c_ix_i)(\sum_{j=1}^{m}c_jx_j)(x_ix_j)^{p-1}\\=&||\sum_{i=1}^{m}c_ix_i||^2(x_ix_j)^{p-1}\end{aligned}</script><ul><li>由于p大于等于1，该式子也大于等于0，即Gram矩阵半正定，所以正整数的幂函数是正定核函数</li></ul></li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(六)</title>
      <link href="/posts/a18ea060.html"/>
      <url>/posts/a18ea060.html</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑斯谛回归与最大熵模型"><a href="#逻辑斯谛回归与最大熵模型" class="headerlink" title="逻辑斯谛回归与最大熵模型"></a>逻辑斯谛回归与最大熵模型</h1><ul><li>logistic regression是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，推广到分类问题得到最大熵模型(maxium entropy model)</li><li>这两种模型都属于对数线性模型<h2 id="逻辑斯谛回归模型"><a href="#逻辑斯谛回归模型" class="headerlink" title="逻辑斯谛回归模型"></a>逻辑斯谛回归模型</h2></li><li>二项逻辑斯谛回归模型是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布。</li><li><p>分类问题，可以表示成one-hot的形式，而one-hot可以认为是一种确定概率的表达。而最大熵模型，是一种不确定的概率表达，其中这个概率，是一个条件概率，是构建的特征函数生成的概率。</p><h3 id="逻辑斯谛分布"><a href="#逻辑斯谛分布" class="headerlink" title="逻辑斯谛分布"></a>逻辑斯谛分布</h3></li><li><p>$X$是连续随机变量，$X$服从逻辑斯谛分布</p><script type="math/tex; mode=display">F(x)=P(X\leqslant x)=\frac{1}{1+\exp(-(x-\mu)/\gamma)}</script></li><li><p>关于逻辑斯谛， 更常见的一种表达是Logistic function</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+\exp(-z)}</script></li><li>这个函数把实数域映射到(0, 1)区间，这个范围正好是概率的范围， 而且可导，对于0输入， 得到的是0.5，可以用来表示等可能性。</li></ul><h3 id="二项逻辑斯谛回归模型"><a href="#二项逻辑斯谛回归模型" class="headerlink" title="二项逻辑斯谛回归模型"></a>二项逻辑斯谛回归模型</h3><ul><li>二项逻辑斯谛回归模型是如下的条件概率分布：(这里的$w$是对扩充的权值向量，包含参数$b$)<script type="math/tex; mode=display">\begin{aligned}P(Y=1|x)&=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\&=\frac{\exp(w\cdot x)/\exp(w\cdot x)}{(1+\exp(w\cdot x))/(\exp(w\cdot x))}\\&=\frac{1}{e^{-(w\cdot x)}+1}\\P(Y=0|x)&=\frac{1}{1+\exp(w\cdot x)}\\&=1-\frac{1}{1+e^{-(w\cdot x)}}\\&=\frac{e^{-(w\cdot x)}}{1+e^{-(w\cdot x)}}\end{aligned}</script><h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3></li><li>应用极大似然估计法估计模型参数，从而得到回归模型，具体步骤为求对数似然函数，并对$L(w)$求极大值，得到$w$的估计值</li></ul><script type="math/tex; mode=display">\begin{aligned}L(w)&=\sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\&=\sum_{i=1}^N[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))]\\\end{aligned}</script><ul><li>上述过程将$P(Y=1|x)=\pi(x)$代入$L(w)$中,从而对$L(w)$求极大值，得到$w$的估计值，这样问题就变成了以对数似然函数为目标函数的最优化问题。通常采用的方法是梯度下降法以及拟牛顿法。</li></ul><h3 id="多项逻辑斯谛回归"><a href="#多项逻辑斯谛回归" class="headerlink" title="多项逻辑斯谛回归"></a>多项逻辑斯谛回归</h3><ul><li>假设离散型随机变量$Y$的取值集合是${1,2,\dots,K}$, 多项逻辑斯谛回归模型是<script type="math/tex; mode=display">\begin{aligned}P(Y=k|x)&=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}, k=1,2,\dots,K-1\\P(Y=K|x)&=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}\\\end{aligned}</script></li><li>上述两式和为1</li></ul><h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><ul><li><p>最大熵模型是由最大熵原理推导实现的，而最大熵原理是概率模型的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型就是最好的模型。通常用约束条件来确定概率模型的集合。</p><h3 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h3><ul><li><p>$H(X, Y) = H(X) + H(Y|X) = H(Y)+H(X|Y) = H(X|Y)+H(Y|X)+I(X;Y)$</p></li><li><p>如果$X$和$Y$独立同分布，联合概率分布$P(X,Y)=P(X)P(Y)$ </p></li></ul></li></ul><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><ul><li><p>条件熵是最大熵原理提出的基础，最大的是条件熵，书中(定义6.3)</p></li><li><p>条件熵衡量了条件概率分布的均匀性</p></li></ul><script type="math/tex; mode=display">\begin{aligned}   p^*&=\arg\max\limits_{p\in \mathcal C}H(p)\\   &=\arg \max\limits_{p\in \mathcal C}(-\sum\limits_{x,y} {\tilde p(x)p(y|x)\log p(y|x) })   \end{aligned}</script><h3 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h3><ul><li><p>互信息(mutual information)，对应熵里面的交集，常用来描述差异性</p></li><li><p>一般的，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息</p><ul><li><strong>相关性主要刻画线性，互信息刻画非线性</strong></li><li>互信息和条件熵之间的关系<script type="math/tex; mode=display">I(x,y)=H(x)-H(x|y)=H(y)-H(y|x)</script></li></ul></li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><ul><li>这个对应的是Chapter5的内容，决策树学习应用信息增益准则选择特征<script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script></li><li><p>信息增益表示得知$X$的信息而使类$Y$的信息的不确定性减少的程度。</p></li><li><p>在决策树学习中，信息增益等价于训练数据集中类与特征的互信息。</p></li></ul><h3 id="相对熵-KL-散度"><a href="#相对熵-KL-散度" class="headerlink" title="相对熵 (KL 散度)"></a>相对熵 (KL 散度)</h3><ul><li><p>相对熵(Relative Entropy)描述差异性，从分布的角度描述差异性，可用于度量两个概率分布之间的差异</p><ul><li><p>KL散度不是一个度量，度量要满足交换性</p></li><li><p>KL散度满足非负性</p></li></ul></li></ul><h3 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h3><ul><li>最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优的问题。自然而然想到了拉格朗日，这里用到了拉格朗日的对偶性，将原始问题转化为对偶问题，通过解对偶问题而得到原始问题的解。</li><li>简单来说，约束最优化问题包含$\leqslant0$，和$=0$两种约束条件<script type="math/tex; mode=display">\begin{aligned}    \min_{x \in R^n}\quad &f(x) \\   s.t.\quad&c_i(x) \leqslant 0 , i=1,2,\ldots,k\\   &h_j(x) = 0 , j=1,2,\ldots,l  \end{aligned}</script></li><li>引入广义拉格朗日函数</li></ul><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x) + \sum_{i=0}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)</script><ul><li>在KKT的条件下，原始问题和对偶问题的最优值相等<script type="math/tex; mode=display">∇_xL(x^∗,α^∗,β^∗)=0\\∇_αL(x^∗,α^∗,β^∗)=0\\∇_βL(x^∗,α^∗,β^∗)=0\\α_i^∗c_i(x^*)=0,i=1,2,…,k\\c_i(x^*)≤0,i=1,2,…,k\\α^∗_i≥0,i=1,2,…,k\\h_j(x^∗)=0,j=1,2,…,l</script></li><li>前面三个条件是由解析函数的知识，对于各个变量的偏导数为0，后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束,第四个条件是<strong>KKT的对偶互补条件</strong></li></ul><ul><li>回到最大熵模型的学习，书中详细介绍了约束最优化问题</li><li>在$L(P, w)$对$P$求偏导并令其为零解得<script type="math/tex; mode=display">P(y|x)=\exp{\left(\sum_{i=1}^{n}w_if_i(x,y)+w_0-1\right)}=\frac{\exp{\left(\sum\limits_{i=1}^{n}w_if_i(x,y)\right)}}{\exp{\left(1-w_0\right)}}</script></li><li>因为$\sum\limits_{y}P(y|x)=1$，然后得到模型</li></ul><script type="math/tex; mode=display">P_w(y|x)=\frac{1}{Z_w(x)}\exp{\sum\limits_{i=1}^{n}w_if_i(x,y)}\\</script><script type="math/tex; mode=display">其中，Z_w(x)=\sum_{y}\exp({\sum_{i=1}^{n}w_if_i(x,y))}</script><ul><li><p>这里$Z_w(x)$先用来代替$\exp(1-w_0)$,$Z_w$是归一化因子</p></li><li><p>并不是因为概率为1推导出了$Z_w$的表达式，这样一个表达式是凑出来的，意思就是遍历$y$的所有取值，求分子表达式的占比</p></li><li><p>对偶函数的极大化等价于最大熵模型的极大似然估计</p></li><li>已知训练数据的经验分布$\widetilde {P}(X,Y)$,条件概率分布$P(Y|X)$的对数似然函数表示为</li></ul><script type="math/tex; mode=display">L_{\widetilde {P}}(P_w)=\log\prod_{x,y}P(y|x)^{\widetilde {P}(x,y)}=\sum \limits_{x,y}\widetilde {P}(x,y)\log{P}(y|x)</script><ul><li>当条件分布概率$P(y|x)$是最大熵模型时</li></ul><script type="math/tex; mode=display">\begin{aligned}L_{\widetilde {P}}(P_w)&=\sum \limits_{x,y}\widetilde {P}(x,y)\log{P}(y|x)\\&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x,y)\log{(Z_w(x))}\\&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x)P(y|x)\log{(Z_w(x))}\\&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}\sum_{y}P(y|x)\\&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}\end{aligned}</script><ul><li>推导过程用到了$\sum\limits_yP(y|x)=1$</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(五)</title>
      <link href="/posts/3881e812.html"/>
      <url>/posts/3881e812.html</url>
      
        <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="笔记摘要"><a href="#笔记摘要" class="headerlink" title="笔记摘要"></a>笔记摘要</h2><ul><li>决策树可以认为是if-then规则的集合，也可以认为是定义在特征空间上的条件概率分布</li><li>根据损失函数最小化的原则建立决策树模型</li><li>决策树的路径或其对应的if-then规则集合具有一个重要性质：互斥且完备</li><li>决策树的学习算法包含特征选择、决策树的生成与决策树的剪枝</li><li>决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择</li></ul><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><script type="math/tex; mode=display">H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i</script><ul><li><p><strong>熵只与$X$的分布有关，与$X$取值无关</strong></p></li><li><p>定义$0\log0=0$，熵是非负的</p></li><li>表示随机变量不确定性的度量</li></ul><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><ul><li>随机变量$(X,Y)$的联合概率分布为</li></ul><script type="math/tex; mode=display">P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\dots ,n;j=1,2,\dots ,m</script><ul><li>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性<script type="math/tex; mode=display">H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)</script>其中$p_i=P(X=x_i),i=1,2,\dots ,n$</li></ul><h3 id="经验熵，-经验条件熵"><a href="#经验熵，-经验条件熵" class="headerlink" title="经验熵， 经验条件熵"></a>经验熵， 经验条件熵</h3><ul><li>当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵</li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><ul><li><p>特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差</p><script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script></li><li><p>熵与条件熵的差称为互信息</p></li><li><p>决策树中的信息增益等价于训练数据集中的类与特征的互信息</p></li><li><p>考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征</p></li></ul><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><ul><li>使用信息增益比可以对上面倾向取值较多的特征的问题进行校正<script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}</script></li></ul><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><blockquote><p>输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$<br>输出：决策树$T$</p><ol><li>如果$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$</li><li>如果$A$是空集，则$T$为单节点树，并将实例数最多的类作为该节点类标记，返回$T$</li><li>计算$g$, 选择信息增益最大的特征$A_g$</li><li>如果$A_g$的信息增益小于$\epsilon$，则置$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$</li><li>否则，依$A_g=a_i$将D划分若干非空子集$D_i$，$D_i$中实例数最大的类$C_k$作为类标记，构建子结点，由结点及其子结点 构成树$T$，返回$T$</li><li>$D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$</li></ol></blockquote><h3 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h3><ul><li><p>改用信息增益比来选择特征</p><blockquote><p>输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$<br>输出：决策树$T$</p><ol><li>如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$</li><li>如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$</li><li>计算$g$, 选择<strong>信息增益比</strong>最大的特征$A_g$</li><li>如果$A_g$的<strong>信息增益比</strong>小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$</li><li>否则，依$A_g=a_i$将D划分若干非空子集$D_i$，$D_i$中实例数最大的类$C_k$作为类标记，构建子结点，由结点及其子结点 构成树$T$，返回$T$</li><li>$D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$</li></ol></blockquote></li><li><p>ID3和C4.5在生成上，差异只在准则的差异</p></li></ul><h3 id="树的剪枝"><a href="#树的剪枝" class="headerlink" title="树的剪枝"></a>树的剪枝</h3><ul><li>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现的</li><li>树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\alpha\geqslant 0$为参数，决策树学习的损失函数可以定义为<script type="math/tex; mode=display">C_\alpha(T)=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T|</script>其中<script type="math/tex; mode=display">H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}</script><script type="math/tex; mode=display">C(T)=\sum_{t=1}^{|T|}N_tH_t(T)\color{black}=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}</script>这时有<script type="math/tex; mode=display">C_\alpha(T)=C(T)+\alpha|T|</script>其中$C(T)$表示模型对训练数据的误差，$|T|$表示模型复杂度，参数$\alpha \geqslant 0$控制两者之间的影响</li></ul><h3 id="剪枝算法"><a href="#剪枝算法" class="headerlink" title="剪枝算法"></a>剪枝算法</h3><blockquote><p>输入：生成算法生成的整个树$T$，参数$\alpha$</p><p>输出：修剪后的子树$T_\alpha$</p><ol><li>计算每个结点的经验熵</li><li>递归地从树的叶结点向上回缩<br>假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\alpha(T_A)$和$C_\alpha(T_B)$，如果$C_\alpha(T_A)\leqslant C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点</li><li>返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$</li></ol><ul><li>决策树的剪枝算法可以由一种动态规划的算法实现</li></ul></blockquote><h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><ul><li>决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，并进行特征选择，生成二叉树<h4 id="最小二乘回归树生成算法"><a href="#最小二乘回归树生成算法" class="headerlink" title="最小二乘回归树生成算法"></a>最小二乘回归树生成算法</h4></li><li>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上地输出值，构建二叉决策树</li></ul><blockquote><p>输入：训练数据集$D$<br>输出：回归树$f(x)$<br>步骤：</p><ol><li>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，得到满足下面式子的$(j,s)$<script type="math/tex; mode=display">\min\limits_{j,s}\left[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]</script></li><li>用选定的$(j,s)$, 划分区域并决定相应的输出值<script type="math/tex; mode=display">R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}> s\} \\\hat{c}_m= \frac{1}{N}\sum\limits_{x_i\in R_m(j,s)} y_j, x\in R_m, m=1,2</script></li><li>对两个子区域调用(1)(2)步骤， 直至满足停止条件</li><li>将输入空间划分为$M$个区域$R_1, R_2,\dots,R_M$，生成决策树：<script type="math/tex; mode=display">f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)</script><ul><li>课后题有详细例子</li></ul></li></ol></blockquote><h4 id="CART分类树的生成"><a href="#CART分类树的生成" class="headerlink" title="CART分类树的生成"></a>CART分类树的生成</h4><ul><li><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点</p></li><li><p>概率分布的基尼指数定义</p></li></ul><script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2</script><ul><li>如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，则在特征$A$的条件下，集合$D$的基尼指数定义为<script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><blockquote><p>输入：训练数据集$D$，停止计算的条件<br>输出：CART决策树<br>根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树：</p><ol><li>设结点地训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取得每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数</li><li>在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依照最优特征和最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</li><li>对两个子结点递归地调用1、2，直至满足停止条件</li><li>生成CART决策树</li></ol></blockquote></li><li>算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于指定阈值，或者没有更多特征</li></ul><h2 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h2><ul><li><p>5.1 根据表 5.1 所给的训练数据集，利用信息增益比（C4.5 算法）生成决策树</p><ul><li>编写程序计算信息增益比，并利用C4.5算法生成决策树，得到结果如下<br>特征（年龄）的信息增益比为： 0.052<br>特征（有工作）的信息增益比为： 0.352<br>特征（有自己的房子）的信息增益比为： 0.433<br>特征（信贷情况）的信息增益比为： 0.232<br>特征（年龄）的信息增益比为： 0.164<br>特征（有工作）的信息增益比为： 1.000<br>特征（有自己的房子）的信息增益比为： 0.340<ul><li>决策树<br>{‘label:’: None, ‘feature’: 2, ‘tree’: {‘否’: {‘label:’: None, ‘feature’: 1, ‘tree’: {‘否’: {‘label:’: ‘否’, ‘feature’: None, ‘tree’: {}}, ‘是’: {‘label:’: ‘是’, ‘feature’: None, ‘tree’: {}}}}, ‘是’: {‘label:’: ‘是’, ‘feature’: None, ‘tree’: {}}}}</li></ul></li><li>结果与ID3算法完全相同<br><img src="https://img-blog.csdnimg.cn/2019060915344250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">  </li></ul></li><li><p>5.2 已知如表 5.2 所示的训练数据，试用平方误差损失准则生成一个二叉回归树<br><img src="https://img-blog.csdnimg.cn/201906091538427.png#pic_center" alt="在这里插入图片描述"></p></li><li><p>5.3 证明 CART 剪枝算法中，当$α$确定时，存在唯一的最小子树 $T_α$使损失函数 $C_α(T)$最小</p><ul><li>利用反证法，假设存在两个最小子树使损失函数最小<br>设这两棵最小子树 为$T_1$​ 和 $T_2$​ ，其剪枝位置分别是 $t_1$​ 和 $t_2$​ ，两者都能使得损失函数最小，即两者拥有相等的 $C_\alpha(T)​$<br>又有：<script type="math/tex; mode=display">C_\alpha(t_1)<C_\alpha(T_{t1})\\ C_\alpha(t_2)<C_\alpha(T_{t2})</script>即剪枝$t_1$,$t_2$ ，总能使得整体损失函数减小，因此对于子树 $T_1$, $T_2$ ，总存在进一步的剪枝，使得损失函数进一步减小（在 $T_1$ 中剪枝 $t_2$,在 $T_2$ 中剪枝 $t_1$ ），因此  $T_1$, $T_2$ 不是最优的子树。<br>即不可能存在两棵及以上的最优子树</li></ul></li><li><p>5.4 证明 CART 剪枝算法中求出的子树序列$\{T_0,T_1,⋅⋅⋅,T_n\}$分别是区间 $α∈[α_i,α_{i+1})$的最优子树$T_\alpha$ ，这里$i=0,1,⋅⋅⋅,n,0=\alpha_0&lt;\alpha_1&lt;\cdot\cdot\cdot&lt;\alpha_n&lt;+\infty$</p><ul><li>参见 <a href="https://www.taylorfrancis.com/books/9781315139470" target="_blank" rel="noopener">Breiman的著作</a></li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(四)</title>
      <link href="/posts/3e2abf6f.html"/>
      <url>/posts/3e2abf6f.html</url>
      
        <content type="html"><![CDATA[<h1 id="朴素贝叶斯法"><a href="#朴素贝叶斯法" class="headerlink" title="朴素贝叶斯法"></a>朴素贝叶斯法</h1><h2 id="笔记摘要"><a href="#笔记摘要" class="headerlink" title="笔记摘要"></a>笔记摘要</h2><ul><li><p>条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其实际估计是不可行的</p></li><li><p>指数级数量的参数    $K\prod_{j=1}^nS_j$，实际估计不可行是实际上没有那么多样本</p></li><li>朴素贝叶斯法是基于<strong>贝叶斯定理</strong>与<strong>特征条件独立假设</strong>的分类方法<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><script type="math/tex; mode=display">P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum _{j=1}^nP(B_j)P(A|B_j)}</script></li></ul><h3 id="条件独立假设"><a href="#条件独立假设" class="headerlink" title="条件独立假设"></a>条件独立假设</h3><p><strong>independent and identically distributed</strong> </p><ul><li>求$P(Y|X)$，其中$X\in\{X_1,X_2,\dots,X_n\}$，条件独立假设这里给定$Y$的情况下：</li></ul><ol><li>每一个$X_i$和其他的每个$X_k$是条件独立的</li><li>每一个$X_i$和其他的每个$X_k$的子集是条件独立的</li></ol><ul><li><p>条件独立性假设是:</p><script type="math/tex; mode=display">\begin{aligned}P(X=x|Y=c_k)&=P(X^{(1)},\dots,X^{(n)}|Y=c_k)\\&=\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)\end{aligned}</script></li><li><p>条件独立假设等于是说用于分类的<strong>特征</strong>在<strong>类确定</strong>的条件下都是<strong>条件独立</strong>的</p></li></ul><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><blockquote><p>为了估计状态变量的条件分布，利用贝叶斯法则，有</p><script type="math/tex; mode=display">   \underbrace{P(X|Y)}_{posterior}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{P(Y)}_{evidence}}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{\sum\limits_x P(Y|X)P(X)}_{evidence}}</script><p>其中$P(X|Y)$为给定$Y$下$X$的后验概率(Posterior)， $P(Y|X)$称为似然(Likelyhood)，$P(X)$称为先验(Prior)。</p></blockquote><ul><li><p>后验概率最大化的含义</p><p>朴素贝叶斯法将实例分到<strong>后验概率最大的类</strong>中， 这等价于<strong>期望风险最小化</strong>。</p></li><li><p>后验是指观察到$Y$之后，对$X$的信念</p></li></ul><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><ul><li>对于$x$的某个特征的取值没有在先验中出现的情况 ，如果用极大似然估计就会出现所要估计的概率值为0的情况。这样会影响后验概率的计算结果，使分类产生偏差</li><li><p>但是出现这种情况的原因通常是因为数据集不能全覆盖样本空间，出现未知的情况处理的策略就是做平滑</p><script type="math/tex; mode=display">P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_j=c_k)+S_j\lambda}</script></li><li><p>当$\lambda = 0$的时候，就是极大似然估计</p></li><li><p>当$\lambda=1$的时候，这个平滑方案叫做Laplace Smoothing。拉普拉斯平滑相当于给未知变量给定了先验概率</p></li></ul><h2 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h2><ul><li>4.1 用极大似然估计法推出朴素贝叶斯法中的概率估计公式(4.8)及公式 (4.9)<ul><li>由于朴素贝叶斯法假设Y是定义在输出空间上的随机变量，因此可以定义$P(Y=c_k)=p$,令$m=\sum _{i=1}^NI(y_i=c_k)$</li><li>得出似然函数 $L(p)=p^m(1-p)^{N-m}$</li><li>求导求最值：$mp^{m-1}(1-p)^{N-m}-(N-m)p^m(1-p)^{N-m-1}=0$</li><li>$p^{m-1}(1-p)^{N-m-1}(m-Np)=0$,易得$p=\frac mN$,即为公式（4.8）</li><li>公式（4.9）的证明与公式（4.8）完全相同，定义$P(X^{(j)}=a_{jl}{\mid}Y=c_k)=p$，令$m=\sum_{i=1}^NI(y_i=c_k)$，$q=\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)$即可</li></ul></li><li>4.2 用贝叶斯估计法推出朴素贝叶斯法中的慨率估计公式(4.10)及公式(4.11)<ul><li>贝叶斯估计和传统的极大似然估计的区别就是，参数值是固定的还是也当做随机变量。传统的极大似然估计，把参数$\theta$当做固定的一个值，不变的，只是目前还不知道，通过最大化$L$求出$\theta$；贝叶斯估计认为参数$\theta$也是随机变量，它也服从一个分布（β分布）</li><li>设$P(Y=c_k)=p$,$m=\sum _{i=1}^NI(y_i=c_k)$,加入先验概率,认为是均匀的$p=\frac{1}{K}$，对照上题极大似然概率下的条件概率约束</li><li>得到$\lambda (pK-1)+pN-m=0$,从而解出$P(Y=c_k)=\frac{m+\lambda}{N+K\lambda}$,即为公式（4.11）</li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(三)</title>
      <link href="/posts/c4695060.html"/>
      <url>/posts/c4695060.html</url>
      
        <content type="html"><![CDATA[<h1 id="k近邻法"><a href="#k近邻法" class="headerlink" title="k近邻法"></a>k近邻法</h1><ul><li>k值的选择、距离度量及分类决策规则是k近邻法的三要素</li><li>三要素在算法之中完整体现出来：<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3></li></ul><blockquote><p>输入: $T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}， x_i\in   X \subseteq{\bf{R}^n}, y_i\in Y=\{c_1,c_2,\dots, c_k\}$;   实例特征向量$x$</p><p>输出: 实例所属的$y$</p><p>步骤:</p><ol><li>根据指定的<strong>距离度量</strong>，在$T$中查找$x$的<strong>最近邻的$k$个点</strong>，覆盖这$k$个点的$x$的邻域定义为$N_k(x)$</li><li>在$N_k(x)$中应用<strong>分类决策规则</strong>决定$x$的类别$y$<script type="math/tex; mode=display">y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j), i=1,2,\dots,N, j=1,2,\dots,K</script><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3></li></ol><ul><li><strong>特征空间</strong>中的两个实例点的距离是两个实例点相似程度的反映。<ol><li>$p=1$ 对应 曼哈顿距离</li></ol></li></ul><ol><li>$p=2$ 对应 欧氏距离</li><li>任意$p$ 对应 闵可夫斯基距离</li></ol></blockquote><script type="math/tex; mode=display">L_p(x_i, x_j)=\left(\sum_{l=1}^{n}{\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^p}\right)^{\frac{1}{p}}</script><ul><li>范数是对向量或者矩阵的度量，是一个标量，这个里面两个点之间的$L_p$距离可以认为是两个点坐标差值的$p$范数</li></ul><h3 id="k值选择"><a href="#k值选择" class="headerlink" title="k值选择"></a>k值选择</h3><ul><li>k值选择会对算法结果产生重大影响。若选较小，只有与输入实例相似的训练实例才会对预测结果起作用，“学习”的近似误差会减小，但“学习”的估计误差会增大，会对近邻的实例点非常敏感，k值减少意味着整体模型变得复杂，容易发生过拟合；若选较大，与输入实例不相似的训练实例也对预测起作用，从而发生错误</li><li>通过<strong>交叉验证</strong>选取最优$k$，算是超参数，一般k值会取一个较小的数值</li><li>在二分类问题中，$k$选择奇数有助于避免平票</li></ul><h3 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h3><ul><li>决策规则往往是多数表决(Majority Voting Rule)</li><li>误分类率</li></ul><script type="math/tex; mode=display">\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i\ne c_i)}=1-\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i= c_i)}</script><ul><li>如果分类损失函数是0-1损失，误分类率最低即经验风险最小。</li></ul><h3 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h3><ul><li>k近邻最简单的实现方法是线性扫描，这时候要计算输入实例与每一个训练实例的距离</li><li>为了提高k近邻的搜索效率，考虑使用树结构存储训练数据，以减少计算距离的次数</li><li>kd树是二叉树，表示对<strong>k维空间的一个划分</strong>，注意这里的k和k近邻的k意义并不相同，<strong>只是习惯上的一致</strong></li><li>kdTree搜索时效率未必是最优的，这个和样本分布有关系。随机分布样本<strong>kdTree搜索</strong>(这里应该是<strong>最</strong>近邻搜索)的平均计算复杂度是$O(\log N)$，空间维数$K$接近训练样本数$N$时，搜索效率急速下降，几乎$O(N)$</li><li><p>kd树的构造：构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；通过递归的方法，不断对k维空间进行切分，生成子节点；直到子区域内没有实例时终止。<br>~~~python</p><pre><code>  k = len(data[0])  # 数据维度  def CreateNode(split, data_set): # 按第split维划分数据集exset创建KdNode      if not data_set:    # 数据集为空          return None      data_set.sort(key=lambda x: x[split])      split_pos = len(data_set) // 2      # //为Python中的整数除法      median = data_set[split_pos]        # 中位数分割点                   split_next = (split + 1) % k        # cycle coordinates      # 递归的创建kd树      return KdNode(median, split,                     CreateNode(split_next, data_set[:split_pos]),     # 创建左子树                    CreateNode(split_next, data_set[split_pos + 1:])) # 创建右子树  self.root = CreateNode(0, data)         # 从第0维分量开始构建kd树,返回根节点</code></pre></li></ul><p>~~~</p><h3 id="kd树最近邻搜索"><a href="#kd树最近邻搜索" class="headerlink" title="kd树最近邻搜索"></a>kd树最近邻搜索</h3><ul><li>算法<blockquote><p>输入：已构造的$kd$树，目标点$x$<br>输出：$x$的最近邻</p><ol><li>在$kd$树中找出包含目标点$x$的叶结点：从根结点出发，递归地向下访问$kd$树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。</li><li>以此叶结点为“当前最近点”。</li><li>递归地向上回退，在每个结点进行以下操作：<ul><li>如果该点保存地实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。</li><li>当前最近点一定存在于该结点一个子结点对应地区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。<br><strong>如果相交</strong>，可能在另一子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点。接着递归地进行最近邻搜索；<br><strong>如果不相交</strong>，向上回退。 </li></ul></li><li>当回退到根结点时，搜索结束。最后的“当前最近点”即为$x$的最近邻点。</li></ol></blockquote></li></ul><h2 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h2><ul><li><p>3.1 参照图 3.1，在 二维空间中给出实例点，画出 k 为 1 和 2 时的 K 近邻法构成的空间划分，并对其进行比较，体会 K 值选择与模型复杂度及预测准确率的关系。</p><ul><li>k为1时<br><img src="https://img-blog.csdnimg.cn/20190603093035295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><ul><li>k为2时<br><img src="https://img-blog.csdnimg.cn/20190603093201425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></li></ul></li><li><p>3.2 利用例题 3.2 构造的kd树求点 x=(3,4.5) 的最近邻点。</p><ul><li>首先找到包含(3,4.5)的叶节点(4,7)，将叶节点作为“当前最近点”；</li><li>回退到(4,7)的父节点(5,4)，将(5,4)作为“当前最近点”。以(3,4.5)为圆心，到“当前最近点”(5,4)距离为半径的圆显然和(5,4)的另一个子节点(2,3)区域相交，因此移动到(2,3)；</li><li>移动到(2,3)后发现，距离(3,4.5)更近，因此将(2,3)作为“当前最近点”，由于(2,3)是叶节点，因此直接回退；</li><li>回到(5,4)的根节点(7,2)，到(7,2)距离大于到“当前最近点”距离，同时(3,4.5)和“当前最近点”距离构成的圆和根节点的另一个子节点的区域不相交，所以搜索结束，得到最近点(2，3)。<br><img src="https://img-blog.csdnimg.cn/20190603094841314.png" alt="在这里插入图片描述"></li></ul></li><li>3.3 参照算法 3.3，写出输出为 x 的 K 近邻的算法。</li></ul><p>在寻找最近邻节点的时候需要维护一个”当前最近点“，而寻找 K 近邻的时候，就需要维护一个”当前 K 近邻点集“。首先定义一个”当前 K 近邻点集“插入新点操作：如果”当前 K 近邻点集“元素数量小于K，那么直接将新点插入集合；如果”当前 K 近邻点集“元素数量等于K，那么将新节点替换原来集合中最远的节点。</p><p>（1）在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问树。若目标点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止；</p><p>（2）如果”当前 K 近邻点集“元素数量小于K或者叶节点距离小于”当前 K 近邻点集“中最远点距离，那么将叶节点插入”当前 K 近邻点集“；</p><p>（3）递归地向上回退，在每个结点进行以下操作：</p><ul><li><p>如果”当前 K 近邻点集“元素数量小于K或者当前节点距离小于”当前 K 近邻点集“中最远点距离，那么将该节点插入”当前 K 近邻点集“，</p></li><li><p>检查另一子结点对应的区域是否与以目标点为球心、以目标点与于”当前 K 近邻点集“中最远点间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点 . 接着，递归地进行最近邻搜索；如果不相交，向上回退；</p></li></ul><p>（4）当回退到根结点时，搜索结束，最后的”当前 K 近邻点集“即为 x 的 K 近邻点集。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(二)</title>
      <link href="/posts/ba9a704b.html"/>
      <url>/posts/ba9a704b.html</url>
      
        <content type="html"><![CDATA[<h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><h2 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h2><ul><li>感知机是二类分类的线性模型,属于<strong>判别模型</strong></li><li>感知机学习旨在求出将训练数据进行线性划分的分离超平面。是神经网络和支持向量机的基础</li><li><p>损失函数选择</p><ul><li>损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数不是参数$w,b$的连续可导函数，不易优化</li><li>损失函数的另一个选择是误分类点到超平面$S$的总距离，这正是感知机所采用的</li></ul></li><li><p>感知机学习的经验风险函数(损失函数)</p><script type="math/tex; mode=display">L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)</script><p>其中$M$是误分类点的集合，给定训练数据集$T$，损失函数$L(w,b)$是$w$和$b$的连续可导函数</p></li></ul><h4 id="原始形式算法"><a href="#原始形式算法" class="headerlink" title="原始形式算法"></a>原始形式算法</h4><blockquote><p>输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in R^n,y_i\in {\{+1,-1\}},i=1,2,3,\dots,N;学习率0&lt;\eta\leqslant 1$</p><p>输出：$w,b;感知机模型f(x)=sign(w\cdot x+b)$</p><ol><li><p>选取初值$w_0,b_0$</p></li><li><p>训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i(w\cdot x_i+b)\leqslant 0$</p><script type="math/tex; mode=display">w\leftarrow w+\eta y_ix_i</script><script type="math/tex; mode=display">b\leftarrow b+\eta y_i</script></li><li>转至(2)，直至训练集中没有误分类点</li></ol></blockquote><ul><li>这个是原始形式中的迭代公式，可以对$x$补1，将$w$和$b$合并在一起，合在一起的这个叫做扩充权重向量</li></ul><h4 id="对偶形式算法"><a href="#对偶形式算法" class="headerlink" title="对偶形式算法"></a>对偶形式算法</h4><ul><li>对偶形式的基本思想是<strong>将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$</strong>。</li></ul><blockquote><p>输入：$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in R^n,y_i\in {\{+1,-1\}},i=1,2,3,\dots,N;学习率0&lt;\eta\leqslant 1$</p><p>输出：<br>$\alpha ,b; 感知机模型f(x)=sign\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right),<br>\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$</p><ol><li><p>$\alpha \leftarrow 0,b\leftarrow 0​$</p></li><li><p>训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_i\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right) \leqslant 0$</p><script type="math/tex; mode=display">\alpha_i\leftarrow \alpha_i+\eta</script><script type="math/tex; mode=display">b\leftarrow b+\eta y_i</script></li><li><p>转至(2)，直至训练集中没有误分类点</p></li></ol></blockquote><ul><li><p><strong>Gram matrix</strong></p><p> 对偶形式中，训练实例仅以内积的形式出现。</p><p> 为了方便可预先将训练集中的实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵</p><script type="math/tex; mode=display">  G=[x_i\cdot x_j]_{N\times N}</script></li></ul><h2 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h2><ul><li>2.1 Minsky 与 Papert 指出：感知机因为是线性模型，所以不能表示复杂的函数，如异或 (XOR)。验证感知机为什么不能表示异或。<ul><li>我们显然无法使用一条直线将两类样本划分，异或问题是线性不可分的。</li><li>可以借助下面动图理解</li></ul></li></ul><p><img src="https://img-blog.csdnimg.cn/20190527153433865.gif#pic_center" alt="在这里插入图片描述"></p><ul><li>2.2 模仿例题 2.1，构建从训练数据求解感知机模型的例子。<ul><li>感知机的训练过程如上所述，取与原例题相同的数据，计算出不同的结果</li><li>x = [3 3; 4 3; 1 1];<br>y = [1; 1; -1]; </li><li>根据程序运行可得：</li><li>eg1:<blockquote><p>误分类点为： [3 3] 此时的w和b为： [[0.][0.]]  0</p><p>误分类点为： [1 1] 此时的w和b为： [[3.][3.]]  1  </p><p>误分类点为： [1 1] 此时的w和b为： [[2.][2.]]  0</p><p>误分类点为： [1 1] 此时的w和b为： [[1.][1.]]  -1</p><p>误分类点为： [3 3] 此时的w和b为： [[0.][0.]]  -2</p><p>误分类点为： [1 1] 此时的w和b为： [[3.][3.]] -1</p><p>误分类点为： [1 1] 此时的w和b为： [[2. [2.]] -2</p><p>最终训练得到的w和b为： [[1.] [1.]] -3 </p><p>即$f(x)=sign(x^{(1)}+x^{(2)}-3)$</p></blockquote></li><li>eg2:<blockquote><p>误分类点为： [1 1] 此时的w和b为： [[0.] [0.]] 0</p><p>误分类点为： [4 3] 此时的w和b为： [[-1.] [-1.]] -1</p><p>误分类点为： [1 1] 此时的w和b为： [[3.] [2.]] 0</p><p>误分类点为： [1 1] 此时的w和b为： [[2.] [1.]] -1</p><p>最终训练得到的w和b为： [[1.] [0.]] -2</p><p>即$f(x)=sign(x^{(1)}-2)$</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>``` python<br>import numpy as np<br>import matplotlib.pyplot as plt</p></blockquote></li></ul></li></ul><h1 id="1、创建数据集"><a href="#1、创建数据集" class="headerlink" title="1、创建数据集"></a>1、创建数据集</h1><p>def createdata():<br>    samples=np.array([[3,3],[4,3],[1,1]])<br>    labels=np.array([1,1,-1])<br>    return samples,labels</p><h1 id="训练感知机模型"><a href="#训练感知机模型" class="headerlink" title="训练感知机模型"></a>训练感知机模型</h1><p>class Perceptron:<br>    def <strong>init</strong>(self,x,y,a=1):<br>        self.x=x<br>        self.y=y<br>        self.w=np.zeros((x.shape[1],1))#初始化权重，w1,w2均为0<br>        self.b=0<br>        self.a=1#学习率<br>        self.numsamples=self.x.shape[0]<br>        self.numfeatures=self.x.shape[1]</p><pre><code>def sign(self,w,b,x):    y=np.dot(x,w)+b    return int(y)def update(self,label_i,data_i):    tmp=label_i*self.a*data_i    tmp=tmp.reshape(self.w.shape)     #更新w和b    self.w+=tmp    self.b+=label_i*self.adef train(self):    isFind=False    while not isFind:        count=0        for i in range(self.numsamples):            tmpY=self.sign(self.w,self.b,self.x[i,:])            if tmpY*self.y[i]&lt;=0:                #如果是一个误分类实例点                print(&#39;误分类点为：&#39;,self.x[i,:],&#39;此时的w和b为：&#39;,self.w,self.b)                count+=1                self.update(self.y[i],self.x[i,:])        if count==0:            print(&#39;最终训练得到的w和b为：&#39;,self.w,self.b)            isFind=True    return self.w,self.b</code></pre><h1 id="画图描绘"><a href="#画图描绘" class="headerlink" title="画图描绘"></a>画图描绘</h1><p>class Picture:<br>    def <strong>init</strong>(self,data,w,b):<br>        self.b=b<br>        self.w=w<br>        plt.figure(1)<br>        plt.title(‘Perceptron Learning Algorithm’,size=14)<br>        plt.xlabel(‘x0-axis’,size=14)<br>        plt.ylabel(‘x1-axis’,size=14)</p><pre><code>    xData=np.linspace(0,5,100)    yData=self.expression(xData)    plt.plot(xData,yData,color=&#39;r&#39;,label=&#39;sample data&#39;)    plt.scatter(data[0][0],data[0][1],s=50)    plt.scatter(data[1][0],data[1][1],s=50)    plt.scatter(data[2][0],data[2][1],s=50,marker=&#39;x&#39;)    plt.savefig(&#39;2d.png&#39;,dpi=75)def expression(self,x):    y=(-self.b-self.w[0]*x)/self.w[1]#注意在此，把x0，x1当做两个坐标轴，把x1当做自变量，x2为因变量    return ydef Show(self):    plt.show()</code></pre><p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    samples,labels=createdata()<br>    myperceptron=Perceptron(x=samples,y=labels)<br>    weights,bias=myperceptron.train()<br>    Picture=Picture(samples,weights,bias)<br>    Picture.Show()<br>```<br><img src="https://pic.rmb.bdstatic.com/bjh/2a1db1249b1bbc3fbc66800e83de7d4b.jpeg" alt="image.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A City-Wide Crowdsourcing Delivery System with Reinforcement Learning</title>
      <link href="/posts/e2002213.html"/>
      <url>/posts/e2002213.html</url>
      
        <content type="html"><![CDATA[<h1 id="A-City-Wide-Crowdsourcing-Delivery-System-with-Reinforcement-Learning"><a href="#A-City-Wide-Crowdsourcing-Delivery-System-with-Reinforcement-Learning" class="headerlink" title="A City-Wide Crowdsourcing Delivery System with Reinforcement Learning"></a>A City-Wide Crowdsourcing Delivery System with Reinforcement Learning</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="众包交付系统"><a href="#众包交付系统" class="headerlink" title="众包交付系统"></a>众包交付系统</h2><h3 id="众包交付的实际因素"><a href="#众包交付的实际因素" class="headerlink" title="众包交付的实际因素"></a>众包交付的实际因素</h3><ul><li>时间限制：订单到顾客接受货物的时间具有限制</li><li>多跳：从送货站到收货站存在多跳现象。</li><li>利润和价格的设置：作为参数影响。<h3 id="工作模式"><a href="#工作模式" class="headerlink" title="工作模式"></a>工作模式</h3>本文设想一个实用的公共交通为基础的搭便车交付如下。搭车送货系统由图1所示的四部分组成: 云服务器、邮箱、客户端和客户端。邮箱通过 Wi-Fi 或者蜂窝网络连接到云服务器。现成的盒子产品已经开发和部署作为智能储物柜在公共领域：<br><img src="https://i.loli.net/2021/10/26/2pmGWe4ktfu3hVE.png" alt="众包交付系统概述"><br>具体注意：</li><li>客户(发送方)将包放在他/她的原始站的邮箱阵列的邮箱中，然后在客户端中输入包的目的地。<a id="jump0">客户可选择两小时、半天(6小时)或当日保修期服务，但价格不同。</a></li><li>服务器根据所有包和环境的及时信息(如时间、天气、供需比)，决定所有包跨所有站点的下一跳。</li><li>当参与的乘客来到一个邮箱阵列时，她可以在智能手机上报告她的目的地。根据乘客的输入和邮箱阵列中现有包裹的预定路线，系统决定哪些包裹由乘客携带。多个包裹将被发送给乘客，如果他们共享同一个下一跳，这不是超出乘客的能力。<strong>具体措施就是下面要讲的TA-Aware RL-Dispatch算法</strong></li><li>当旅客到达目的地(不一定是包裹的目的地)时，他/她将包裹放入指定的邮箱，并声称任务已经完成。另一名旅客会前来重复上述程序，直至包裹抵达目的地。旅客只需在自己跳跃的源头和目的地拿起包裹，不需要绕道而行。因此，交付是纯粹的搭便车，乘客的路线或行为没有改变。</li><li>当包到达目的地时，客户(接收方)得到通知并打开邮箱获取包。如果超过时间限制，平台将对客户进行赔偿，即如果包裹超过时间保证，客户将获得退款。<h2 id="以利润为导向的调度模型"><a href="#以利润为导向的调度模型" class="headerlink" title="以利润为导向的调度模型"></a>以利润为导向的调度模型</h2>针对云服务器上的订单分发问题，考虑到包的时间约束，使平台利润最大化。首先介绍了一个具有实际因素的利润模型; 然后展示了<strong>ETA模块的细节来模拟估计站之间的交付时间</strong>; 最后，设计了一个强化学习算法来解决包装工的路由问题。<h3 id="利润模型"><a href="#利润模型" class="headerlink" title="利润模型"></a>利润模型</h3>最大化利润：<script type="math/tex; mode=display">\max _{h_{i}, M} \text {Profit}=\sum_{i=1}^{N} \text {CustPay}_{i}-\sum_{i=1}^{N} \sum_{j=1}^{h_{i}} \operatorname{Hop} \operatorname{Cost}_{i}^{j}-\sum_{k=1}^{M} \text {CustPay}_{k}</script>$N$为包的个数，$\text {CustPay}_{i}$是顾客$i$采用的<a href="#jump0">配送方式</a>，$h_{i}$是递送包裹$i$经过的跳数，$\operatorname{Hop} \operatorname{Cost}_{i}^{j}$是支付给乘客第$i$个包裹，第$j$跳的费用。$M$为为满足时间要求的包裹数目。</li></ul><p>要最大化利润就要每个包更少的跳数，以及更少的过期（违约）包。也就是平衡$h_{i}$。因此文章的重点是选取<strong>ETA 模型来评估不同站对(station pair)之间的旅行时间，和一个 RL 模型来选择最优路径。在包调度中，我们主要关注如何减少每个包所需的跳数，同时使更多的包在时间限制内交付</strong></p><h3 id="ETA模块"><a href="#ETA模块" class="headerlink" title="ETA模块"></a>ETA模块</h3><p>ETA是任何两个站点之间的包裹估计递送估计情况，使用ETA模块对交付时间信息进行显式聚合和建模，以缩小动作空间，提高学习性能。</p><p>配送时间被分为<strong>等待时间waiting time</strong>和<strong>运行时间running time</strong>，总体配送时间表示为$T_D$:</p><script type="math/tex; mode=display">T_{\mathrm{D}}=\sum_{i=1}^{n} T_{\mathrm{W}}^{i}+\sum_{i=1}^{n} T_{\mathrm{R}}^{i}</script><p>等待时间是指包裹在源站或中转站的邮箱中的时间，运行时间是指乘客在移动中携带包裹的时间。$T_{\mathrm{W}}^{i}$是指在第$i$个中转站的等待时间，$T_{\mathrm{R}}^{i}$是指到第$i$个中转站的运行时间。$n$为总共的跳数。</p><h4 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h4><p>引进分位点函数</p><script type="math/tex; mode=display">\mu+\sigma \sqrt{2} \operatorname{erf}^{-1}(2 p-1)</script><p>保证真实配送时间小于或等于估计时间的概率为$p$，（$p$为订单及时率）</p><h4 id="waiting-time"><a href="#waiting-time" class="headerlink" title="waiting time"></a>waiting time</h4><p><strong>包裹到下一站$i$站的等待时间$t_{\mathrm{W}}^{i}$为从包裹放置到乘客刷卡进入下一站的时间。</strong> <font color='red'> 这里我有一个问题，$t_{\mathrm{W}}^{i}$是根据什么样的数据集进行计算的呢？目前的数据是从某个点到目的地的位置相信和出发到达信息，以及订单数目信息</font><br>将一天化为24个时隙，每个时隙有固定的$\mu_{W}^{i}$和$\sigma_{W}^{i}$并采用极大似然估计得到估计量：</p><script type="math/tex; mode=display">\hat{\mu}_{\mathrm{W}}^{i}=\frac{\sum^{n} t_{\mathrm{W}}^{i}}{n}, \quad\left(\hat{\sigma}_{\mathrm{W}}^{i}\right)^{2}=\frac{\sum^{n}\left(t_{\mathrm{W}}^{i}-\hat{\mu}_{W}^{i}\right)^{2}}{n}</script><p>论文通过了Kolmogorov-Smirnov检验。</p><h4 id="乘客意愿"><a href="#乘客意愿" class="headerlink" title="乘客意愿"></a>乘客意愿</h4><p>本文利用参与率指标$\rho$来模拟乘客愿意接受包裹的意愿。</p><p><font color='red'>so we use the first arrival passenger between the station pair under the participate rate to estimate the waiting time. Therefore, the estimation of the waiting time is independent of the dispatching policy.这句话没理解</font></p><h4 id="running-time"><a href="#running-time" class="headerlink" title="running time"></a>running time</h4><p>运行时间是根据乘客刷卡进入和刷卡离开的时间来计算的，<font color='red'>这一点我觉得奇怪：没有刷卡数据，只有订单目的地和起始地点的位置数据</font> 在固定时间间隔和间隔率的情况下，特定OD对(数据集)的运行时间和等待时间可以用高斯函数拟合，并用深圳地铁一个月数据的 K-S 检验进行了验证。用不同的$\rho$参与率计算城市公共交通的ETA结果，将其存储为一个4-D 张量$\left[\mu_{\mathrm{W}}, \sigma_{\mathrm{W}}, \mu_{\mathrm{R}}, \sigma_{\mathrm{R}}\right]=H(\rho, t, o, d)<br>$ ，而$\mu_{\mathrm{W}}$和$\sigma_{\mathrm{W}}$<font color='red'>这里我有些疑问，$H$函数是怎么确定的，整篇文章没看到</font>是估计等待时间的高斯参数，$\mu_{\mathrm{R}}$和$\sigma_{\mathrm{R}}$是估计运行时间的高斯参数，分别在站点$o$和$d$之间，$\rho$是参与率，$t$是当前时段。</p><h2 id="ETA-Aware-RL-Dispatch算法"><a href="#ETA-Aware-RL-Dispatch算法" class="headerlink" title="ETA-Aware RL-Dispatch算法"></a>ETA-Aware RL-Dispatch算法</h2><p>我们使用深rl方法来学习来自大规模历史传输数据和包数据的每个包的最佳路线，不同的价格设置（$Cortpay$和$Hopcost$）和环境因素（天气，时间，供应和需求）。ETA感知的动作过滤器旨在消除基于ETA的不可行的动作，以加速离线培训并改善在线路由。<br><img src="https://i.loli.net/2021/10/27/27bieEaXYn4IlLH.png" alt="调度问题"></p><p>随着包裹和旅客在现实世界中的动态状态，包裹路由问题自然是一个序贯决策问题。在我们的问题设置中，如图2所示，代理人是系统中的包裹。决策中心充当元代理，以集中的方式为所有包做出决策。元代理在连续的时间段(例如，每个包5分钟)做出决定(例如，为每个包选择下一跳) ，并预测未来几个小时的总收入回报(例如，平台利润)。环境是包裹和乘客在系统中移动，以及一些环境，如天气。RL 的组成部分定义如下。</p><ul><li>代理：我们认为每一个包装都是一个代理商。决策中心作为一个元代理，为所有代理作出决策，从而使各站之间的所有包的路由可以被了解并在各站的所有包之间共享。</li><li>状态空间：在每个时间段$t$有两个状态：<strong>global state</strong>和<strong>private states.</strong> 我们使用供应和需求信息作为状态特性来进一步模拟实际应用程序，因此<strong>global state</strong>又分为：<ul><li>Demand states：所有站点的等待包裹的一维分布,所有站对中正在递送的包装的2维分布($N_{station}\times N_{station}$)。</li><li>Supply states:所有站点进站乘客的一维分布,所有站对中正在通勤乘客的二维分布($N_{station}\times N_{station}$)</li><li>Contextual states:该信息表示为天气，日期，平日和时间。</li></ul></li><li>动作空间：单个代理的操作指定包的下一跳，特别是包应该在所有站之间传送的下一站。</li><li>奖励函数：针对总利润表达设置奖励函数。奖励函数在包到达目的地之前设置为0，否则设置为：<script type="math/tex; mode=display">r_{t}^{i}=\text { CustPay}_{i} \times \phi(i)-\sum_{j=1}^{h_{i}} \text { HopCost }</script>$\phi(i)$是一个指示函数，如果包裹$i$没有超时，则$\phi(i)=1$在包裹到达目的地之前，奖励不计算在内。在满足时间约束的前提下，通过奖励函数使算法选择跳数最小的路由。也就是不追求每个订单时间最短，只要在时间限制之内即可。</li><li>动作值计算：动作值<script type="math/tex; mode=display">Q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{l} \mid \mathbf{s}_{t}=s, \mathbf{a}_{t}=a\right]</script>是值按照策略$\pi$下，当前状态$s$采取动作$a$时获得的潜在期望价值。<br>其中$G_{l}$为t时刻潜在价值<script type="math/tex; mode=display">G_{l}=\sum_{k=0}^{\infty} \gamma^{k} r_{l+k+1}</script>由于每次的代理与环境交互并不一定将所有$Q$值填完，因此在基于价值的无模型强化学习方法中，动作价值函数使用函数逼近器来表示，比如神经网络。这里的DQN方法也是利用这一点，让$Q(s, a; \theta)$是一个带有参数$\theta$的近似动作值函数，<strong>简而言之，利用神经网络训练$Q$</strong><br>。在每次迭代中，DQN 使用以下损失函数更新参数:<script type="math/tex; mode=display">\mathcal{L}(\theta)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim U(D)}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta^{-}\right)-Q(s, a ; \theta)\right)^{2}\right]</script></li><li>ETA感知动作过滤<br>由于行动是包装的下一个站选择，因此在城市中的数百个运输站，动作空间很大。大型动作空间导致训练中的缓慢收敛，并影响路由性能。鉴于我们离线收集的ETA信息，我们在动作过滤过程中使用了此先前知识，以加速培训并提供交货时间的统计保证。在过滤器中，动作$a_i$的潜在选择概率为：<script type="math/tex; mode=display">P_{l, a_{j}}= \begin{cases}1, & \text { if } T_{a_{j}} \leq r_{i} \\ 0, & \text { otherwise }\end{cases}</script>$T_{a_j}$是从当前站到目的地的预计到达时间(由ETA算出)，$r_i$是$i$包裹约定的到达时间。<script type="math/tex; mode=display">T_{a_{j}}=T_{\mathrm{R}}^{o_{t} \rightarrow s_{j}}+T_{\mathrm{W}}^{s_{j}}+T_{\mathrm{R}}^{s_{j} \rightarrow d_{i}}</script>$s_j$是由动作$a_j$决定的下一站。如果下一站就是目的站，则：<script type="math/tex; mode=display">T_{a_{j}}=T^{o_{t} \rightarrow d_{i}}</script>我们基于ETA的结果$\left[\mu_{\mathrm{W}}, \sigma_{\mathrm{W}}, \mu_{\mathrm{R}}, \sigma_{\mathrm{R}}\right]=H(\rho, t, o, d)<br>$可求出每个时段，每个起始点的拟合参数$[\mu_{\mathrm{W}}, \sigma_{\mathrm{W}}, \mu_{\mathrm{R}}, \sigma_{\mathrm{R}}]$，计算相应的时间：<script type="math/tex; mode=display">T=\mu+\sigma \sqrt{2} \operatorname{erf}^{-1}(2 p-1)</script>$p$为置信度，参数$p$的选择会影响到交货率，从而影响到利润。较大的$p$值会提高交货率(即准时率)，但也会排除一些潜在的低成本路线。<font color='red'>特别指出，当$r_j$为负数时，也就是当前以及过期了，包裹将优先发货。</font><br>算法伪代码：<br><img src="https://pic.rmb.bdstatic.com/bjh/49aa8599e95eaca8f252700b587690e0.jpeg" alt="image.png"><br>在模型训练过程中，我们构建了能够基于乘客数据、包数据和上下文数据恢复环境演化的模拟器。<br><img src="https://pic.rmb.bdstatic.com/bjh/0047c81e92deea7b19b14b9f7c80f945.jpeg" alt="image.png"><h2 id="仿真"><a href="#仿真" class="headerlink" title="仿真"></a>仿真</h2>本文使用了75%的地铁/包裹递送数据来建立模拟器，25%的数据用于评估。对于每个包都有三个状态: “等待”、“在路上”和“交付”。“ Wating”意思是一个包裹正在原点站或者换乘站的邮箱中等待。“在途中”是指一个包裹已经被旅客提起，并且正在运送到中转站或目的地的过程中。“投递”是指最后一次飞行的旅客已将包裹投递到目的地。例如，对于带有两个跃点的包，它的状态顺序应该是“ Waiting”、“ On the way”、“ Waiting”、“ On the way”、“ Delivered”。为了模拟一个只有部分乘客愿意携带包裹的现实世界场景，我们通过随机选择相应的乘客百分比来设置不同的参与率，为一个包裹递送。</li></ul><p>对于调度系统的每个时间间隔，模拟器的工作方式为：</p><ul><li><strong>Add new packages</strong>： 检查包数据集，并将前一时刻中出现的包添加到包集中，并将其状态设置为“Waiting”。</li><li><strong>Update “On the way” packages’ states</strong>： 对于“在路上”的包裹，在$t$处查看相应的旅客数据，找到到达中转站或目的地的包裹，并将包裹的状态设置为“等候”或“送达”。</li><li><strong>Package dispatching</strong>:  对于所有未分发的“等待”包，使用 ETA-Aware RL-Dispatch 决定包的下一跳。</li></ul><p>离发送者和接收者最近的位置的地铁站被分配为源站和目标站。在时间限制方面，我们为每个包裹分别设定时间限制，由1至8小时不等，以确保当日送达。</p><p><br></p><div class="row">    <embed src="https://wspwhut6666.gitee.io/file/3478117.pdf" width="100%" height="550" type="application/pdf"></div><p><br></p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pandas教程</title>
      <link href="/posts/680fd426.html"/>
      <url>/posts/680fd426.html</url>
      
        <content type="html"><![CDATA[<pre><code class="lang-python">import numpy as npimport pandas as pd</code></pre><h2 id="生成对象"><a href="#生成对象" class="headerlink" title="生成对象"></a>生成对象</h2><ul><li>Series：维度——1；带标签的一维同构数组</li><li>DataFrame：维度——2；带标签的，大小可变的，二维异构表格</li></ul><pre><code class="lang-python">s = pd.Series([1, 3, 5, np.nan, 6, 8])s</code></pre><pre><code>0    1.01    3.02    5.03    NaN4    6.05    8.0dtype: float64</code></pre><p>含日期时间：</p><pre><code class="lang-python">datas = pd.date_range(&#39;20130101&#39;, periods=6)datas</code></pre><pre><code>DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;,               &#39;2013-01-05&#39;, &#39;2013-01-06&#39;],              dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;)</code></pre><p>含日期时间索引与标签的 NumPy 数组生成<code>DataFrame</code></p><pre><code class="lang-python">df = pd.DataFrame(np.random.randn(6,4),index=datas, columns=list(&#39;ABCD&#39;))df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>    </tr>  </tbody></table></div><p>用字典对象生成DataFrame </p><pre><code class="lang-python">df2 = pd.DataFrame({&#39;A&#39;: 1.,                    &#39;B&#39;: pd.Timestamp(&#39;20130102&#39;),                    &#39;C&#39;: pd.Series(1,index=list(range(4))),                    &#39;D&#39;: np.array([3]*4, dtype=&#39;int32&#39;),                    &#39;E&#39;: pd.Categorical([&quot;test&quot;, &quot;train&quot;, &quot;test&quot;, &quot;train&quot;]),                    &#39;F&#39;: &#39;foo&#39;})df2</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>      <th>F</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.0</td>      <td>2013-01-02</td>      <td>1</td>      <td>3</td>      <td>test</td>      <td>foo</td>    </tr>    <tr>      <th>1</th>      <td>1.0</td>      <td>2013-01-02</td>      <td>1</td>      <td>3</td>      <td>train</td>      <td>foo</td>    </tr>    <tr>      <th>2</th>      <td>1.0</td>      <td>2013-01-02</td>      <td>1</td>      <td>3</td>      <td>test</td>      <td>foo</td>    </tr>    <tr>      <th>3</th>      <td>1.0</td>      <td>2013-01-02</td>      <td>1</td>      <td>3</td>      <td>train</td>      <td>foo</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df2.dtypes</code></pre><pre><code>A           float64B    datetime64[ns]C             int64D             int32E          categoryF            objectdtype: object</code></pre><pre><code class="lang-python">df.head()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.info</code></pre><pre><code>&lt;bound method DataFrame.info of                    A         B         C         D2013-01-01  0.477423  0.183968 -0.139705 -0.7272232013-01-02 -0.168761 -0.922938  0.396889 -2.7791282013-01-03 -0.618984  0.576812  1.900378 -0.6249962013-01-04 -1.395511  0.525819  0.112159 -0.1608262013-01-05  0.280020 -2.186553 -0.644597  1.1639172013-01-06  1.945874  0.014268 -1.506800  0.405446&gt;</code></pre><pre><code class="lang-python">df.tail(3)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.index</code></pre><pre><code>DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;,               &#39;2013-01-05&#39;, &#39;2013-01-06&#39;],              dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;)</code></pre><pre><code class="lang-python">df.columns</code></pre><pre><code>Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype=&#39;object&#39;)</code></pre><p><code>DataFrame.to_numpy()</code>输出底层数据的 <code>NumPy</code> 对象。注意，<code>DataFrame</code>的列由多种数据类型组成时，该操作耗费系统资源较大，这也是<code>Pandas</code>和<code>NumPy</code>的本质区别：<code>NumPy</code>数组只有一种数据类型，<code>DataFrame</code>每列的数据类型各不相同。调用 <code>DataFrame.to_numpy()</code>时，Pandas 查找支持<code>DataFrame</code>里所有数据类型的<code>NumPy</code>数据类型。还有一种数据类型是<code>object</code>，可以把<code>DataFrame</code>列里的值强制转换为<code>Python</code>对象。</p><pre><code class="lang-python">df.to_numpy()</code></pre><pre><code>array([[ 0.47742267,  0.18396775, -0.13970455, -0.72722311],       [-0.168761  , -0.92293827,  0.3968894 , -2.77912839],       [-0.6189843 ,  0.57681245,  1.90037812, -0.62499646],       [-1.3955112 ,  0.52581862,  0.11215897, -0.16082579],       [ 0.2800197 , -2.18655291, -0.64459677,  1.16391673],       [ 1.94587401,  0.01426759, -1.50679977,  0.40544621]])</code></pre><pre><code class="lang-python">df2.to_numpy()</code></pre><pre><code>array([[1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1, 3, &#39;test&#39;, &#39;foo&#39;],       [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1, 3, &#39;train&#39;, &#39;foo&#39;],       [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1, 3, &#39;test&#39;, &#39;foo&#39;],       [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1, 3, &#39;train&#39;, &#39;foo&#39;]],      dtype=object)</code></pre><pre><code class="lang-python">df.describe()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>6.000000</td>      <td>6.000000</td>      <td>6.000000</td>      <td>6.000000</td>    </tr>    <tr>      <th>mean</th>      <td>0.086677</td>      <td>-0.301437</td>      <td>0.019721</td>      <td>-0.453802</td>    </tr>    <tr>      <th>std</th>      <td>1.131969</td>      <td>1.070597</td>      <td>1.138830</td>      <td>1.338086</td>    </tr>    <tr>      <th>min</th>      <td>-1.395511</td>      <td>-2.186553</td>      <td>-1.506800</td>      <td>-2.779128</td>    </tr>    <tr>      <th>25%</th>      <td>-0.506428</td>      <td>-0.688637</td>      <td>-0.518374</td>      <td>-0.701666</td>    </tr>    <tr>      <th>50%</th>      <td>0.055629</td>      <td>0.099118</td>      <td>-0.013773</td>      <td>-0.392911</td>    </tr>    <tr>      <th>75%</th>      <td>0.428072</td>      <td>0.440356</td>      <td>0.325707</td>      <td>0.263878</td>    </tr>    <tr>      <th>max</th>      <td>1.945874</td>      <td>0.576812</td>      <td>1.900378</td>      <td>1.163917</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.T</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>2013-01-01</th>      <th>2013-01-02</th>      <th>2013-01-03</th>      <th>2013-01-04</th>      <th>2013-01-05</th>      <th>2013-01-06</th>    </tr>  </thead>  <tbody>    <tr>      <th>A</th>      <td>0.477423</td>      <td>-0.168761</td>      <td>-0.618984</td>      <td>-1.395511</td>      <td>0.280020</td>      <td>1.945874</td>    </tr>    <tr>      <th>B</th>      <td>0.183968</td>      <td>-0.922938</td>      <td>0.576812</td>      <td>0.525819</td>      <td>-2.186553</td>      <td>0.014268</td>    </tr>    <tr>      <th>C</th>      <td>-0.139705</td>      <td>0.396889</td>      <td>1.900378</td>      <td>0.112159</td>      <td>-0.644597</td>      <td>-1.506800</td>    </tr>    <tr>      <th>D</th>      <td>-0.727223</td>      <td>-2.779128</td>      <td>-0.624996</td>      <td>-0.160826</td>      <td>1.163917</td>      <td>0.405446</td>    </tr>  </tbody></table></div><p>按轴排序</p><pre><code class="lang-python">df2.sort_index(axis=1,ascending=False)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>F</th>      <th>E</th>      <th>D</th>      <th>C</th>      <th>B</th>      <th>A</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>foo</td>      <td>test</td>      <td>3</td>      <td>1</td>      <td>2013-01-02</td>      <td>1.0</td>    </tr>    <tr>      <th>1</th>      <td>foo</td>      <td>train</td>      <td>3</td>      <td>1</td>      <td>2013-01-02</td>      <td>1.0</td>    </tr>    <tr>      <th>2</th>      <td>foo</td>      <td>test</td>      <td>3</td>      <td>1</td>      <td>2013-01-02</td>      <td>1.0</td>    </tr>    <tr>      <th>3</th>      <td>foo</td>      <td>train</td>      <td>3</td>      <td>1</td>      <td>2013-01-02</td>      <td>1.0</td>    </tr>  </tbody></table></div><p>按值排序</p><pre><code class="lang-python">df.sort_values(by=&#39;B&#39;)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>    </tr>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>  </tbody></table></div><h2 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h2><pre><code class="lang-python">df[&#39;A&#39;]</code></pre><pre><code>2013-01-01    0.4774232013-01-02   -0.1687612013-01-03   -0.6189842013-01-04   -1.3955112013-01-05    0.2800202013-01-06    1.945874Freq: D, Name: A, dtype: float64</code></pre><pre><code class="lang-python">df[0:3]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df[&#39;2013-01-02&#39;:&#39;20130104&#39;]       ## 时间的这两种书写方式一样</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>  </tbody></table></div><h3 id="按标签选择"><a href="#按标签选择" class="headerlink" title="按标签选择"></a>按标签选择</h3><pre><code class="lang-python">df.loc[datas[0]]</code></pre><pre><code>A    0.477423B    0.183968C   -0.139705D   -0.727223Name: 2013-01-01 00:00:00, dtype: float64</code></pre><pre><code class="lang-python">df.loc[:,[&#39;A&#39;,&#39;B&#39;]]# df[[&#39;A&#39;,&#39;B&#39;]]       ## 等同</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.loc[datas[1]:datas[3], [&#39;A&#39;, &#39;B&#39;]]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.loc[datas[0], &#39;A&#39;]   # 等同于df.at[datas[0], &#39;A&#39;]</code></pre><pre><code>0.4774226701301457</code></pre><h3 id="按位置选择-重要"><a href="#按位置选择-重要" class="headerlink" title="按位置选择(重要)"></a>按位置选择(重要)</h3><pre><code class="lang-python">df.iloc[3]</code></pre><pre><code>A   -1.395511B    0.525819C    0.112159D   -0.160826Name: 2013-01-04 00:00:00, dtype: float64</code></pre><pre><code class="lang-python">df.iloc[[1,2,4],[0,2]]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>C</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>0.396889</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>1.900378</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-0.644597</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.iloc[:, 1:3]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>B</th>      <th>C</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.183968</td>      <td>-0.139705</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.922938</td>      <td>0.396889</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>0.576812</td>      <td>1.900378</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>0.525819</td>      <td>0.112159</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>-2.186553</td>      <td>-0.644597</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>0.014268</td>      <td>-1.506800</td>    </tr>  </tbody></table></div><h3 id="布尔索引-重要"><a href="#布尔索引-重要" class="headerlink" title="布尔索引(重要)"></a>布尔索引(重要)</h3><pre><code class="lang-python"># df.Adf[df.A &gt; 0]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df[df &gt; 0]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>NaN</td>      <td>NaN</td>      <td>0.396889</td>      <td>NaN</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>NaN</td>      <td>0.576812</td>      <td>1.900378</td>      <td>NaN</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>NaN</td>      <td>0.525819</td>      <td>0.112159</td>      <td>NaN</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>NaN</td>      <td>NaN</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>NaN</td>      <td>0.405446</td>    </tr>  </tbody></table></div><h3 id="isin-筛选-重要"><a href="#isin-筛选-重要" class="headerlink" title="isin()筛选(重要)"></a>isin()筛选(重要)</h3><pre><code class="lang-python">df2 = df.copy()df2[&#39;E&#39;] = [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;three&#39;]df2[df2[&#39;E&#39;].isin([&#39;two&#39;, &#39;four&#39;])]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>      <td>two</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>      <td>four</td>    </tr>  </tbody></table></div><h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><pre><code class="lang-python">df1 = df.reindex(index=datas[0:4], columns=list(df.columns))df1.iloc[[1,3],[0,2]] = np.nandf1</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>NaN</td>      <td>-0.922938</td>      <td>NaN</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>NaN</td>      <td>0.525819</td>      <td>NaN</td>      <td>-0.160826</td>    </tr>  </tbody></table></div><p>删除所有含缺失值的行：</p><pre><code class="lang-python">df1.dropna(how=&#39;any&#39;)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>  </tbody></table></div><p>填充缺失值</p><pre><code class="lang-python">df1.fillna(value=5)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>5.000000</td>      <td>-0.922938</td>      <td>5.000000</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>5.000000</td>      <td>0.525819</td>      <td>5.000000</td>      <td>-0.160826</td>    </tr>  </tbody></table></div><p>提取nan值的布尔掩码</p><pre><code class="lang-python">pd.isna(df1)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>False</td>      <td>False</td>      <td>False</td>      <td>False</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>True</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>False</td>      <td>False</td>      <td>False</td>      <td>False</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>True</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>  </tbody></table></div><h2 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h2><h3 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h3><p>求每列的均值</p><pre><code class="lang-python">df.mean()</code></pre><pre><code>A    0.086677B   -0.301437C    0.019721D   -0.453802dtype: float64</code></pre><p>求每行的均值</p><pre><code class="lang-python">df.mean(1)</code></pre><pre><code>2013-01-01   -0.0513842013-01-02   -0.8684852013-01-03    0.3083022013-01-04   -0.2295902013-01-05   -0.3468032013-01-06    0.214697Freq: D, dtype: float64</code></pre><pre><code class="lang-python">s = pd.Series([1, 3, 5, np.nan, 6, 8], index=datas)s.shift(3)</code></pre><pre><code>2013-01-01    NaN2013-01-02    NaN2013-01-03    NaN2013-01-04    1.02013-01-05    3.02013-01-06    5.0Freq: D, dtype: float64</code></pre><pre><code class="lang-python">df.sub(s, axis=&#39;index&#39;)           # 减法运算</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>-0.522577</td>      <td>-0.816032</td>      <td>-1.139705</td>      <td>-1.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-3.168761</td>      <td>-3.922938</td>      <td>-2.603111</td>      <td>-5.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-5.618984</td>      <td>-4.423188</td>      <td>-3.099622</td>      <td>-5.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>-5.719980</td>      <td>-8.186553</td>      <td>-6.644597</td>      <td>-4.836083</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>-6.054126</td>      <td>-7.985732</td>      <td>-9.506800</td>      <td>-7.594554</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.apply(np.cumsum)                 # 累加操作</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>    </tr>    <tr>      <th>2013-01-02</th>      <td>0.308662</td>      <td>-0.738971</td>      <td>0.257185</td>      <td>-3.506352</td>    </tr>    <tr>      <th>2013-01-03</th>      <td>-0.310323</td>      <td>-0.162158</td>      <td>2.157563</td>      <td>-4.131348</td>    </tr>    <tr>      <th>2013-01-04</th>      <td>-1.705834</td>      <td>0.363661</td>      <td>2.269722</td>      <td>-4.292174</td>    </tr>    <tr>      <th>2013-01-05</th>      <td>-1.425814</td>      <td>-1.822892</td>      <td>1.625125</td>      <td>-3.128257</td>    </tr>    <tr>      <th>2013-01-06</th>      <td>0.520060</td>      <td>-1.808625</td>      <td>0.118325</td>      <td>-2.722811</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.apply(lambda x: x.max() - x.min())</code></pre><pre><code>A    3.341385B    2.763365C    3.407178D    3.943045dtype: float64</code></pre><h2 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h2><pre><code class="lang-python">s = pd.Series(np.random.randint(0, 7, size=10))s</code></pre><pre><code>0    41    32    43    14    35    16    67    28    69    0dtype: int32</code></pre><pre><code class="lang-python">s.value_counts()</code></pre><pre><code>4    23    21    26    22    10    1dtype: int64</code></pre><h2 id="删除与合并-重要"><a href="#删除与合并-重要" class="headerlink" title="删除与合并(重要)"></a>删除与合并(重要)</h2><pre><code class="lang-python">df.loc[:,&#39;E&#39;]=1.# df.drop(labels=[&#39;E&#39;],axis=0)df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <th>2013-01-01 00:00:00</th>      <td>0.477423</td>      <td>0.183968</td>      <td>-0.139705</td>      <td>-0.727223</td>      <td>1.0</td>    </tr>    <tr>      <th>2013-01-02 00:00:00</th>      <td>-0.168761</td>      <td>-0.922938</td>      <td>0.396889</td>      <td>-2.779128</td>      <td>1.0</td>    </tr>    <tr>      <th>2013-01-03 00:00:00</th>      <td>-0.618984</td>      <td>0.576812</td>      <td>1.900378</td>      <td>-0.624996</td>      <td>1.0</td>    </tr>    <tr>      <th>2013-01-04 00:00:00</th>      <td>-1.395511</td>      <td>0.525819</td>      <td>0.112159</td>      <td>-0.160826</td>      <td>1.0</td>    </tr>    <tr>      <th>2013-01-05 00:00:00</th>      <td>0.280020</td>      <td>-2.186553</td>      <td>-0.644597</td>      <td>1.163917</td>      <td>1.0</td>    </tr>    <tr>      <th>2013-01-06 00:00:00</th>      <td>1.945874</td>      <td>0.014268</td>      <td>-1.506800</td>      <td>0.405446</td>      <td>1.0</td>    </tr>  </tbody></table></div><pre><code class="lang-python">pieces = [df[:3], df[3:]]pieces</code></pre><pre><code>[                            A         B         C         D    E 2013-01-01 00:00:00  0.477423  0.183968 -0.139705 -0.727223  1.0 2013-01-02 00:00:00 -0.168761 -0.922938  0.396889 -2.779128  1.0 2013-01-03 00:00:00 -0.618984  0.576812  1.900378 -0.624996  1.0,                             A         B         C         D    E 2013-01-04 00:00:00 -1.395511  0.525819  0.112159 -0.160826  1.0 2013-01-05 00:00:00  0.280020 -2.186553 -0.644597  1.163917  1.0 2013-01-06 00:00:00  1.945874  0.014268 -1.506800  0.405446  1.0]</code></pre><pre><code class="lang-python">pd.concat(pieces)</code></pre><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><pre><code class="lang-python">left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;lval&#39;: [1, 2]})right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;rval&#39;: [4, 5]})pd.merge(left, right, on=&#39;key&#39;)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>key</th>      <th>lval</th>      <th>rval</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>foo</td>      <td>1</td>      <td>4</td>    </tr>    <tr>      <th>1</th>      <td>foo</td>      <td>1</td>      <td>5</td>    </tr>    <tr>      <th>2</th>      <td>foo</td>      <td>2</td>      <td>4</td>    </tr>    <tr>      <th>3</th>      <td>foo</td>      <td>2</td>      <td>5</td>    </tr>  </tbody></table></div><pre><code class="lang-python">left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;lval&#39;: [1, 2]})right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;rval&#39;: [4, 5]})pd.merge(left, right, on=&#39;key&#39;)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>key</th>      <th>lval</th>      <th>rval</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>foo</td>      <td>1</td>      <td>4</td>    </tr>    <tr>      <th>1</th>      <td>bar</td>      <td>2</td>      <td>5</td>    </tr>  </tbody></table></div><h2 id="添加（Append）"><a href="#添加（Append）" class="headerlink" title="添加（Append）"></a>添加（Append）</h2><pre><code class="lang-python">df = pd.DataFrame(np.random.randn(8, 4), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;])s = df.iloc[3]df.append(s, ignore_index=True)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.282560</td>      <td>1.578933</td>      <td>-1.835089</td>      <td>-0.570793</td>    </tr>    <tr>      <th>1</th>      <td>0.129816</td>      <td>0.697800</td>      <td>0.418487</td>      <td>-0.253009</td>    </tr>    <tr>      <th>2</th>      <td>-1.009696</td>      <td>-0.800243</td>      <td>0.324064</td>      <td>0.574360</td>    </tr>    <tr>      <th>3</th>      <td>-0.162016</td>      <td>0.497039</td>      <td>0.298132</td>      <td>-1.076649</td>    </tr>    <tr>      <th>4</th>      <td>0.630602</td>      <td>-1.781916</td>      <td>0.668464</td>      <td>0.663769</td>    </tr>    <tr>      <th>5</th>      <td>0.737596</td>      <td>-1.690360</td>      <td>-0.545418</td>      <td>-0.846643</td>    </tr>    <tr>      <th>6</th>      <td>1.418491</td>      <td>-1.698654</td>      <td>0.474682</td>      <td>-0.825363</td>    </tr>    <tr>      <th>7</th>      <td>-1.020948</td>      <td>1.351000</td>      <td>1.288640</td>      <td>-0.098584</td>    </tr>    <tr>      <th>8</th>      <td>-0.162016</td>      <td>0.497039</td>      <td>0.298132</td>      <td>-1.076649</td>    </tr>  </tbody></table></div><h2 id="分组（Grouping）"><a href="#分组（Grouping）" class="headerlink" title="分组（Grouping）"></a>分组（Grouping）</h2><p>“group by” 指的是涵盖下列一项或多项步骤的处理流程：</p><ul><li>分割：按条件把数据分割成多组；</li><li>应用：为每组单独应用函数；</li><li>组合：将处理结果组合成一个数据结构。</li></ul><pre><code class="lang-python">df = pd.DataFrame({&#39;A&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;,                          &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;],                   &#39;B&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;,                          &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;],                   &#39;C&#39;: np.random.randn(8),                   &#39;D&#39;: np.random.randn(8)})df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>foo</td>      <td>one</td>      <td>0.452364</td>      <td>0.296516</td>    </tr>    <tr>      <th>1</th>      <td>bar</td>      <td>one</td>      <td>-0.469170</td>      <td>-0.143944</td>    </tr>    <tr>      <th>2</th>      <td>foo</td>      <td>two</td>      <td>1.295369</td>      <td>-0.269286</td>    </tr>    <tr>      <th>3</th>      <td>bar</td>      <td>three</td>      <td>-0.198760</td>      <td>-0.871319</td>    </tr>    <tr>      <th>4</th>      <td>foo</td>      <td>two</td>      <td>0.726845</td>      <td>-0.584794</td>    </tr>    <tr>      <th>5</th>      <td>bar</td>      <td>two</td>      <td>-0.384160</td>      <td>0.197542</td>    </tr>    <tr>      <th>6</th>      <td>foo</td>      <td>one</td>      <td>-0.844166</td>      <td>2.579093</td>    </tr>    <tr>      <th>7</th>      <td>foo</td>      <td>three</td>      <td>-0.665659</td>      <td>-0.728335</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df.groupby([&#39;A&#39;, &#39;B&#39;]).sum()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>C</th>      <th>D</th>    </tr>    <tr>      <th>A</th>      <th>B</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="3" valign="top">bar</th>      <th>one</th>      <td>-0.469170</td>      <td>-0.143944</td>    </tr>    <tr>      <th>three</th>      <td>-0.198760</td>      <td>-0.871319</td>    </tr>    <tr>      <th>two</th>      <td>-0.384160</td>      <td>0.197542</td>    </tr>    <tr>      <th rowspan="3" valign="top">foo</th>      <th>one</th>      <td>-0.391803</td>      <td>2.875609</td>    </tr>    <tr>      <th>three</th>      <td>-0.665659</td>      <td>-0.728335</td>    </tr>    <tr>      <th>two</th>      <td>2.022214</td>      <td>-0.854080</td>    </tr>  </tbody></table></div><h2 id="重塑"><a href="#重塑" class="headerlink" title="重塑"></a>重塑</h2><pre><code class="lang-python">tuples = list(zip(*[[&#39;bar&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;baz&#39;,               &#39;foo&#39;, &#39;foo&#39;, &#39;qux&#39;, &#39;qux&#39;],              [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;,               &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;]]))tuples</code></pre><pre><code>[(&#39;bar&#39;, &#39;one&#39;), (&#39;bar&#39;, &#39;two&#39;), (&#39;baz&#39;, &#39;one&#39;), (&#39;baz&#39;, &#39;two&#39;), (&#39;foo&#39;, &#39;one&#39;), (&#39;foo&#39;, &#39;two&#39;), (&#39;qux&#39;, &#39;one&#39;), (&#39;qux&#39;, &#39;two&#39;)]</code></pre><pre><code class="lang-python">index = pd.MultiIndex.from_tuples(tuples, names=[&#39;first&#39;, &#39;second&#39;])index</code></pre><pre><code>MultiIndex([(&#39;bar&#39;, &#39;one&#39;),            (&#39;bar&#39;, &#39;two&#39;),            (&#39;baz&#39;, &#39;one&#39;),            (&#39;baz&#39;, &#39;two&#39;),            (&#39;foo&#39;, &#39;one&#39;),            (&#39;foo&#39;, &#39;two&#39;),            (&#39;qux&#39;, &#39;one&#39;),            (&#39;qux&#39;, &#39;two&#39;)],           names=[&#39;first&#39;, &#39;second&#39;])</code></pre><pre><code class="lang-python">df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[&#39;A&#39;, &#39;B&#39;])df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>A</th>      <th>B</th>    </tr>    <tr>      <th>first</th>      <th>second</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="2" valign="top">bar</th>      <th>one</th>      <td>-1.922649</td>      <td>1.871875</td>    </tr>    <tr>      <th>two</th>      <td>-0.092881</td>      <td>0.888805</td>    </tr>    <tr>      <th rowspan="2" valign="top">baz</th>      <th>one</th>      <td>-0.663312</td>      <td>-0.247438</td>    </tr>    <tr>      <th>two</th>      <td>0.199572</td>      <td>0.828977</td>    </tr>    <tr>      <th rowspan="2" valign="top">foo</th>      <th>one</th>      <td>-0.247583</td>      <td>-0.673409</td>    </tr>    <tr>      <th>two</th>      <td>-1.241566</td>      <td>0.452144</td>    </tr>    <tr>      <th rowspan="2" valign="top">qux</th>      <th>one</th>      <td>0.848055</td>      <td>2.258369</td>    </tr>    <tr>      <th>two</th>      <td>1.171787</td>      <td>1.768933</td>    </tr>  </tbody></table></div><pre><code class="lang-python">df2 = df[:4]df2</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>A</th>      <th>B</th>    </tr>    <tr>      <th>first</th>      <th>second</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="2" valign="top">bar</th>      <th>one</th>      <td>-1.922649</td>      <td>1.871875</td>    </tr>    <tr>      <th>two</th>      <td>-0.092881</td>      <td>0.888805</td>    </tr>    <tr>      <th rowspan="2" valign="top">baz</th>      <th>one</th>      <td>-0.663312</td>      <td>-0.247438</td>    </tr>    <tr>      <th>two</th>      <td>0.199572</td>      <td>0.828977</td>    </tr>  </tbody></table></div><p><img src="https://pic.rmb.bdstatic.com/bjh/8d777d647dd67bdd05a0eaeb99bc6f8b.jpeg" alt="image.png"></p><pre><code class="lang-python">stacked = df2.stack()stacked</code></pre><pre><code>first  second   bar    one     A   -1.922649               B    1.871875       two     A   -0.092881               B    0.888805baz    one     A   -0.663312               B   -0.247438       two     A    0.199572               B    0.828977dtype: float64</code></pre><p>解压：</p><pre><code class="lang-python">stacked.unstack()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>A</th>      <th>B</th>    </tr>    <tr>      <th>first</th>      <th>second</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="2" valign="top">bar</th>      <th>one</th>      <td>-1.922649</td>      <td>1.871875</td>    </tr>    <tr>      <th>two</th>      <td>-0.092881</td>      <td>0.888805</td>    </tr>    <tr>      <th rowspan="2" valign="top">baz</th>      <th>one</th>      <td>-0.663312</td>      <td>-0.247438</td>    </tr>    <tr>      <th>two</th>      <td>0.199572</td>      <td>0.828977</td>    </tr>  </tbody></table></div><pre><code class="lang-python">stacked.unstack(1)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>second</th>      <th>one</th>      <th>two</th>    </tr>    <tr>      <th>first</th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="2" valign="top">bar</th>      <th>A</th>      <td>-1.922649</td>      <td>-0.092881</td>    </tr>    <tr>      <th>B</th>      <td>1.871875</td>      <td>0.888805</td>    </tr>    <tr>      <th rowspan="2" valign="top">baz</th>      <th>A</th>      <td>-0.663312</td>      <td>0.199572</td>    </tr>    <tr>      <th>B</th>      <td>-0.247438</td>      <td>0.828977</td>    </tr>  </tbody></table></div><pre><code class="lang-python">stacked.unstack(0)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>first</th>      <th>bar</th>      <th>baz</th>    </tr>    <tr>      <th>second</th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="2" valign="top">one</th>      <th>A</th>      <td>-1.922649</td>      <td>-0.663312</td>    </tr>    <tr>      <th>B</th>      <td>1.871875</td>      <td>-0.247438</td>    </tr>    <tr>      <th rowspan="2" valign="top">two</th>      <th>A</th>      <td>-0.092881</td>      <td>0.199572</td>    </tr>    <tr>      <th>B</th>      <td>0.888805</td>      <td>0.828977</td>    </tr>  </tbody></table></div><h2 id="数据透视表"><a href="#数据透视表" class="headerlink" title="数据透视表"></a>数据透视表</h2><pre><code class="lang-python">df = pd.DataFrame({&#39;A&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;] * 3,                   &#39;B&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] * 4,                   &#39;C&#39;: [&#39;foo&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;, &#39;bar&#39;] * 2,                   &#39;D&#39;: np.random.randn(12),                   &#39;E&#39;: np.random.randn(12)})df</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>one</td>      <td>A</td>      <td>foo</td>      <td>-1.123799</td>      <td>-1.098840</td>    </tr>    <tr>      <th>1</th>      <td>one</td>      <td>B</td>      <td>foo</td>      <td>-1.913548</td>      <td>-0.119521</td>    </tr>    <tr>      <th>2</th>      <td>two</td>      <td>C</td>      <td>foo</td>      <td>-0.639072</td>      <td>0.099824</td>    </tr>    <tr>      <th>3</th>      <td>three</td>      <td>A</td>      <td>bar</td>      <td>-0.359609</td>      <td>1.435979</td>    </tr>    <tr>      <th>4</th>      <td>one</td>      <td>B</td>      <td>bar</td>      <td>0.423144</td>      <td>0.761188</td>    </tr>    <tr>      <th>5</th>      <td>one</td>      <td>C</td>      <td>bar</td>      <td>0.043997</td>      <td>0.928556</td>    </tr>    <tr>      <th>6</th>      <td>two</td>      <td>A</td>      <td>foo</td>      <td>0.025243</td>      <td>-0.610217</td>    </tr>    <tr>      <th>7</th>      <td>three</td>      <td>B</td>      <td>foo</td>      <td>-0.455484</td>      <td>-0.454505</td>    </tr>    <tr>      <th>8</th>      <td>one</td>      <td>C</td>      <td>foo</td>      <td>2.812657</td>      <td>-0.321527</td>    </tr>    <tr>      <th>9</th>      <td>one</td>      <td>A</td>      <td>bar</td>      <td>0.842464</td>      <td>-0.108601</td>    </tr>    <tr>      <th>10</th>      <td>two</td>      <td>B</td>      <td>bar</td>      <td>0.075289</td>      <td>0.824193</td>    </tr>    <tr>      <th>11</th>      <td>three</td>      <td>C</td>      <td>bar</td>      <td>0.352810</td>      <td>1.169305</td>    </tr>  </tbody></table></div><pre><code class="lang-python">pd.pivot_table(df, values=&#39;D&#39;, index=[&#39;A&#39;, &#39;B&#39;], columns=[&#39;C&#39;])</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>C</th>      <th>bar</th>      <th>foo</th>    </tr>    <tr>      <th>A</th>      <th>B</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="3" valign="top">one</th>      <th>A</th>      <td>0.842464</td>      <td>-1.123799</td>    </tr>    <tr>      <th>B</th>      <td>0.423144</td>      <td>-1.913548</td>    </tr>    <tr>      <th>C</th>      <td>0.043997</td>      <td>2.812657</td>    </tr>    <tr>      <th rowspan="3" valign="top">three</th>      <th>A</th>      <td>-0.359609</td>      <td>NaN</td>    </tr>    <tr>      <th>B</th>      <td>NaN</td>      <td>-0.455484</td>    </tr>    <tr>      <th>C</th>      <td>0.352810</td>      <td>NaN</td>    </tr>    <tr>      <th rowspan="3" valign="top">two</th>      <th>A</th>      <td>NaN</td>      <td>0.025243</td>    </tr>    <tr>      <th>B</th>      <td>0.075289</td>      <td>NaN</td>    </tr>    <tr>      <th>C</th>      <td>NaN</td>      <td>-0.639072</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习(一)</title>
      <link href="/posts/15abeb29.html"/>
      <url>/posts/15abeb29.html</url>
      
        <content type="html"><![CDATA[<h1 id="简要"><a href="#简要" class="headerlink" title="简要"></a>简要</h1><ul><li>统计学习或机器学习一般包括监督学习、无监督学习、强化学习，有时还包括半监督学习、主动学习<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2></li><li>监督学习指从标注数据中学习预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。</li><li>输入变量$X$和输出变量$Y$有不同的类型，可以是连续或是离散的。根据输入输出变量的不同类型，对预测任务给予不同的名称：输入与输出均为连续变量的预测问题称为<strong>回归问题</strong>；输出变量为有限个离散变量的预测问题称为<strong>分类问题</strong>；输入与输出变量均为变量序列的预测问题称为<strong>标注问题</strong>。<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2></li><li>无监督学习指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或内在结构。</li><li>无监督学习旨在从假设空间中选出在给定评价标准下的最优模型，模型可以实现对数据的聚类、降维或是概率估计。<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2></li><li>强化学习指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，其本质是学习最优的序贯决策。</li><li>智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错，以达到学习最优策略地目的。</li></ul><h2 id="半监督学习与主动学习"><a href="#半监督学习与主动学习" class="headerlink" title="半监督学习与主动学习"></a>半监督学习与主动学习</h2><ul><li>半监督学习指利用标注数据和未标注数据学习预测模型地机器学习问题。其旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。</li><li>主动学习是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。主动学习的目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价达到较好的学习效果。</li><li>这两种学习更接近监督学习。</li></ul><h2 id="实现统计学习方法的步骤"><a href="#实现统计学习方法的步骤" class="headerlink" title="实现统计学习方法的步骤"></a>实现统计学习方法的步骤</h2><ol><li>得到一个有限的训练数据集合</li><li>确定包含所有可能的模型的<strong>假设空间</strong>，即学习<strong>模型的集合</strong></li><li>确定模型选择的准则，即学习的<strong>策略</strong><ol><li>实现求解最优模型的算法，即学习的<strong>算法</strong></li></ol></li><li>通过学习方法选择最优的模型</li><li>利用学习的最优模型对新数据进行预测或分析</li></ol><ul><li>在上述步骤中涵盖了统计学习方法三要素：模型，策略，算法</li></ul><ul><li>在监督学习过程中，模型就是所要学习的<strong>条件概率分布</strong>或者<strong>决策函数</strong>。<br>注意书中的这部分描述，整理了一下到表格里：</li></ul><div class="table-container"><table><thead><tr><th></th><th>假设空间$\mathcal F$</th><th>输入空间$\mathcal X$</th><th>输出空间$\mathcal Y$</th><th>参数空间</th></tr></thead><tbody><tr><td>决策函数</td><td>$\mathcal F =\{f$\</td><td>$Y=f_{\theta}(x), \theta \in \bf R \it ^n\}$</td><td>变量</td><td>变量</td><td>$\bf R\it ^n$</td></tr><tr><td>条件概率分布</td><td>$\mathcal F =\{P$\</td><td>$P_\theta(Y$\</td><td>$X), \theta \in \bf R \it ^n\}$</td><td>随机变量</td><td>随机变量</td><td>$\bf R\it ^n$</td></tr></tbody></table></div><h2 id="损失函数与风险函数"><a href="#损失函数与风险函数" class="headerlink" title="损失函数与风险函数"></a>损失函数与风险函数</h2><ul><li><p><strong>损失函数</strong>度量模型<strong>一次预测</strong>的好坏，<strong>风险函数</strong>度量<strong>平均意义</strong>下模型预测的好坏。</p></li><li><p>损失函数(loss function)或代价函数(cost function)定义为给定输入$X$的<strong>预测值$f(X)$</strong>和<strong>真实值$Y$</strong>之间的<strong>非负实值</strong>函数，记作$L(Y,f(X))$。</p></li><li><p>风险函数(risk function)或期望损失(expected loss)和模型的泛化误差的形式是一样的<br>$R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y$<br>上式是模型$f(X)$关于联合分布$P(X,Y)$的<strong>平均意义下的</strong>损失(期望损失)，但是因为$P(X,Y)$是未知的，所以前面的用词是<strong>期望</strong>，以及<strong>平均意义下的</strong>。这个表示其实就是损失的均值，反映了对整个数据的预测效果的好坏。</p></li><li><p><strong>经验风险</strong>(empirical risk)或<strong>经验损失</strong>(empirical loss)<br> $R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$<br> 上式是模型$f$关于<strong>训练样本集</strong>的平均损失。根据大数定律，当样本容量N趋于无穷大时，经验风险趋于期望风险。</p></li><li><p><strong>结构风险</strong>(structural risk)<br> $R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$<br> 其中$J(f)$为模型复杂度, $\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。</p></li></ul><h2 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h2><p>损失函数数值越小，模型就越好</p><ul><li>0-1损失<br> $L(Y,f(X))=\begin{cases}1, Y \neq f(X) \\0, Y=f(X) \end{cases}$</li><li>平方损失<br> $L(Y,f(X))=(Y-f(X))^2$</li><li><p>绝对损失<br> $L(Y,f(X))=|Y-f(X)|$</p></li><li><p>对数损失<br>$L(Y,P(Y|X))=−logP(Y|X)$</p></li></ul><h2 id="ERM与SRM"><a href="#ERM与SRM" class="headerlink" title="ERM与SRM"></a>ERM与SRM</h2><p>经验风险最小化(ERM)与结构风险最小化(SRM)</p><ul><li><strong>极大似然估计</strong>是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计，下面习题1.2中给出了证明。</li><li>结构风险最小化等价于<strong>正则化</strong></li><li><strong>贝叶斯估计</strong>中的<strong>最大后验概率估计</strong>是结构风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数，<strong>模型复杂度由模型的先验概率表示</strong>时，结构风险最小化等价于最大后验概率估计。</li></ul><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li>算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么杨的计算方法来求解最优模型。</li></ul><h2 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h2><ul><li><p>训练误差和测试误差是模型关于数据集的平均损失。</p></li><li><p>注意：统计学习方法具体采用的损失函数未必是评估时使用的损失函数。</p></li><li>过拟合是指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。可以说模型选择旨在避免过拟合并提高模型的预测能力。</li></ul><h2 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h2><ul><li>模型选择的典型方法是<strong>正则化</strong>.。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化就越大。比如，正则化项可以是模型参数向量的范数。</li><li>$L(w)=\frac{1}{N}\sum_{i=1}^{N}(f(x_i;w)-y_i)^2+\frac{\lambda}{2}|w|^2$</li><li>$|w|$表示向量$w$的$L_2$范数</li><li>正则化符合奥卡姆剃刀原理：如无必要，勿增实体。在应用于模型选择中时，可以理解为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的才是最好的模型，也是应该选择的模型。</li><li>交叉验证的基本想法时重复地利用数据；把给定地数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。</li><li>主要有简单交叉验证，S折交叉验证，留一交叉验证三种。</li><li>在算法学习的过程中，测试集可能是固定的，但是验证集和训练集可能是变化的。比如S折交叉验证的情况下，分成S折之后，其中的S-1折作为训练集，1折作为验证集，计算这S个模型每个模型的平均测试误差，最后选择平均测试误差最小的模型。这个过程中用来验证模型效果的那一折数据就是验证集。</li></ul><h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><p><strong>监督学习方法</strong>可分为<strong>生成方法</strong>(generative approach)与<strong>判别方法</strong>(discriminative approach)</p><ul><li><p><strong>生成方法(generative approach)</strong></p><ul><li>可以还原出<strong>联合概率分布</strong>$P(X,Y)$</li><li>收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型</li><li>当存在隐变量时仍可以用</li></ul></li><li><p><strong>判别方法(discriminative approach)</strong></p><ul><li>直接学习<strong>条件概率</strong>$P(Y|X)$或者<strong>决策函数</strong>$f(X)$</li><li>直接面对预测, 往往学习准确率更高<ul><li>可以对数据进行各种程度的抽象,  定义特征并使用特征, 可以简化学习问题</li></ul></li></ul></li></ul><h1 id="习题解答"><a href="#习题解答" class="headerlink" title="习题解答"></a>习题解答</h1><ul><li><p><strong>1.1</strong>   说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学方法三要素</p><ul><li><p>伯努利模型是定义在取值为0与1的随机变量上的概率分布。统计学分为两派：经典统计学派和贝叶斯统计学派。两者的不同主要是，经典统计学派认为模型已定，参数未知，参数是固定的，只是还不知道；贝叶斯统计学派是通过观察到的现象对概率分布中的主观认定不断进行修正。</p></li><li><p><strong>极大似然估计</strong> 用的是经典统计学派的策略， <strong>贝叶斯估计</strong> 用的是贝叶斯统计学派的策略；为了得到使经验风险最小的参数值，使用的算法都是对经验风险求导，使导数为0。</p></li><li><p>定义随机变量$A$为一次伯努利试验的结果，$A$的取值为$\{0,1\}$，概率分布为$P(A)$：</p><script type="math/tex; mode=display">P(A=1)=\theta，P(A=0)=1-\theta</script></li><li><p>极大似然估计</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^nP(A_i)=\theta^k(1-\theta)^{n-k}</script><script type="math/tex; mode=display">\theta=\arg\max_{\theta}L(\theta)=\frac{k}{n}</script><p>上述估计通过取对数求导得到，$A_i$为第$i$次随机试验</p></li><li><p>贝叶斯估计</p><script type="math/tex; mode=display">P(\theta|A_1,A_2,…，A_n)=\frac{P(A_1,A_2,…，A_n|\theta)P(\theta)}{P(A_1,A_2,…，A_n)}</script><p>根据观察到的结果修正$\theta$，也就是假设$\theta$是随机变量，$\theta$服从β分布，有很多个可能的取值，我们要取的值是在已知观察结果的条件下使$\theta$出现概率最大的值。上式分母是不变的，求分子最大就可以。 </p><script type="math/tex; mode=display">\begin{aligned}\theta &=arg\max \limits_\theta {P(A_1,A_2,...,A_n|\theta)P(\theta)} \\ &= arg\max \limits_\theta {\prod_{i=1}^{n}P(A_i|\theta)P(\theta)}  \\&=arg \max \limits_\theta {\theta^k(1-\theta)^{n-k}\theta^{a-1}(1-\theta)^{b-1}} \\&=\frac{k+(a-1)}{n+(a-1)+(b-1)}\end{aligned}</script><p>$\beta$分布是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数，是指一组定义在$(0,1)$区间的连续概率分布，有两个参数$\alpha$，$\beta$&gt;0。选定参数后就可以确定$\theta$。</p></li><li><p>统计学习方法的三要素为<strong>模型，策略，算法</strong>。</p></li></ul></li></ul><p><img src="https://img-blog.csdnimg.cn/20190524102938764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5YzQ0ODEzNDE4,size_16,color_FFFFFF,t_70" alt="对比图"></p><ul><li><p><strong>1.2</strong> 通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。</p><ul><li>模型是条件概率分布：$P_θ(Y|X)$，<br>损失函数是对数损失函数：$L(Y,P_θ(Y|X))=−logP_θ(Y|X)$<br>经验风险为：<script type="math/tex; mode=display">\begin{aligned}R_{emp}(f)&=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i)) \\&=\frac{1}{N}\sum_{i=1}^{N}-logP(y_i|x_i) \\&=-\frac{1}{N}\sum_{i=1}^{N}logP(y_i|x_i)\end{aligned}</script></li><li>极大似然估计的似然函数为： <script type="math/tex; mode=display">L(\theta)=\prod_DP_{\theta}(Y|X)</script></li><li><p>取对数</p><script type="math/tex; mode=display">log(L(\theta))=\sum_DlogP_{\theta}(Y|X)</script><script type="math/tex; mode=display">arg\max_\theta\sum_DlogP_{\theta}(Y|X)=arg\min_{\theta}\sum_D-logP_{\theta}(Y|X)</script></li><li><p>因此，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 李航版 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch案例</title>
      <link href="/posts/e2128b1c.html"/>
      <url>/posts/e2128b1c.html</url>
      
        <content type="html"><![CDATA[<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>PyTorch有两个处理数据的库:<code>torch.utils.data.DataLoader</code>和<code>torch.utils.data.Dataset</code>。数据集存储样本及其对应的标签，<code>DataLoader</code>在数据集周围包装一个可迭代对象。</p><pre><code class="lang-python">import torchfrom torch import nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambda, Composeimport matplotlib.pyplot as plt</code></pre><p>torchvision。数据集模块包含许多真实世界的视觉数据的数据集对象，如CIFAR, COCO(完整列表在这里)。在本教程中，我们使用FashionMNIST数据集。每个TorchVision Dataset包含两个参数transform和target_transform，分别修改样本和标签。</p><pre><code class="lang-python"># Download training data from open datasets.training_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=True,    download=True,    transform=ToTensor(),)# Download test data from open datasets.test_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=False,    download=True,    transform=ToTensor(),)</code></pre><p>我们将 Dataset 作为参数传递给 DataLoader。它包装了一个遍历数据集的迭代器，并支持自动批处理、采样、洗牌和多进程数据加载。这里我们定义了一个64的批量大小，也就是说，dataloader 迭代器中的每个元素都将返回一个包含64个特性和标签的批量。</p><pre><code class="lang-python">batch_size = 64# Create data loaders.train_dataloader = DataLoader(training_data, batch_size=batch_size)test_dataloader = DataLoader(test_data, batch_size=batch_size)for X, y in test_dataloader:    print(&quot;Shape of X [N, C, H, W]: &quot;, X.shape)    print(&quot;Shape of y: &quot;, y.shape, y.dtype)    break</code></pre><h2 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h2><pre><code class="lang-python"># Get cpu or gpu device for training.device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;print(&quot;Using {} device&quot;.format(device))# Define modelclass NeuralNetwork(nn.Module):    def __init__(self):        super(NeuralNetwork, self).__init__()        self.flatten = nn.Flatten()        self.linear_relu_stack = nn.Sequential(            nn.Linear(28*28, 512),            nn.ReLU(),            nn.Linear(512, 512),            nn.ReLU(),            nn.Linear(512, 10)        )    def forward(self, x):        x = self.flatten(x)        logits = self.linear_relu_stack(x)        return logitsmodel = NeuralNetwork().to(device)print(model)</code></pre><h2 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h2><pre><code class="lang-python">loss_fn = nn.CrossEntropyLoss()                                            ## 定义损失函数---交叉熵optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)                   ## 定义随机梯度优化器SGD</code></pre><p>在一个单独的训练循环中，模型对训练数据集进行预测(分批输入)，并反向传播预测误差以调整模型的参数。</p><p>我们还根据测试数据集检查模型的性能，以确保它正在学习。</p><pre><code class="lang-python">def test(dataloader, model, loss_fn):    size = len(dataloader.dataset)    num_batches = len(dataloader)    model.eval()    test_loss, correct = 0, 0    with torch.no_grad():        for X, y in dataloader:            X, y = X.to(device), y.to(device)            pred = model(X)            test_loss += loss_fn(pred, y).item()            correct += (pred.argmax(1) == y).type(torch.float).sum().item()    test_loss /= num_batches    correct /= size    print(f&quot;Test Error: \n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \n&quot;)</code></pre><p>训练过程是在几个迭代(阶段)中进行的。在每个epoch期间，模型学习参数以做出更好的预测。我们打印模型在每个时期的准确性和损失;我们希望看到精确度的提高和损失的减少。</p><pre><code class="lang-python">epochs = 5for t in range(epochs):    print(f&quot;Epoch {t+1}\n-------------------------------&quot;)    train(train_dataloader, model, loss_fn, optimizer)    test(test_dataloader, model, loss_fn)print(&quot;Done!&quot;)</code></pre><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><pre><code class="lang-python">torch.save(model.state_dict(), &quot;model.pth&quot;)print(&quot;Saved PyTorch Model State to model.pth&quot;)</code></pre><h2 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h2><pre><code class="lang-python">model = NeuralNetwork()model.load_state_dict(torch.load(&quot;model.pth&quot;))</code></pre><p>对testdata进行预测：</p><pre><code class="lang-python">classes = [    &quot;T-shirt/top&quot;,    &quot;Trouser&quot;,    &quot;Pullover&quot;,    &quot;Dress&quot;,    &quot;Coat&quot;,    &quot;Sandal&quot;,    &quot;Shirt&quot;,    &quot;Sneaker&quot;,    &quot;Bag&quot;,    &quot;Ankle boot&quot;,]model.eval()x, y = test_data[0][0], test_data[0][1]with torch.no_grad():    pred = model(x)    predicted, actual = classes[pred[0].argmax(0)], classes[y]    print(f&#39;Predicted: &quot;{predicted}&quot;, Actual: &quot;{actual}&quot;&#39;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch模板</title>
      <link href="/posts/aea15e63.html"/>
      <url>/posts/aea15e63.html</url>
      
        <content type="html"><![CDATA[<h1 id="pytorch模板"><a href="#pytorch模板" class="headerlink" title="pytorch模板"></a>pytorch模板</h1><h2 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a><code>torch.nn.Module</code></h2><p>我们有时候不需要很复杂的网络或者想自己搭建一个网络，这时候我们就可以继承torch.nn.Module类，快速构建一个前向传播的网络结构，当然torch.nn.Module类还可以构建损失函数：</p><pre><code class="lang-python">import torchclass net_name(torch.nn.Module):    def __init__(self,other_arguments):        super(net_name,self).__init__()        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size)        # other network layer        # torch.nn.Linear()        # torch.nn.ReLU()        # torch.nn.MaxPool2d()        # torch.nn.Dropout()        # torch.nn.BatchNorm2d()    def forward(self, x):        x = self.conv1(x)        return x</code></pre><p><code>torch.nn.Module</code>也很容易构建损失函数，如多分类交叉熵，二分类交叉熵，均方误差等，以下列举几种：</p><pre><code class="lang-python">import torchloss_function = torch.nn.CrossEntropyLoss()loss_function = torch.nn.BCELoss()loss_function = torch.nn.MSELoss()loss_function = torch.nn.L1Loss()# 具体用时loss = loss_function(output, target)loss.backward()</code></pre><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a><code>torch.optim</code></h2><p><code>torch.optim</code>里面有很多的可以实现模型参数自动优化的类，我们可以很方便地调用，例如有随机梯度下降SGD（stochastic gradient descent），适应性矩估计Adam（adaptive moment estimation），适应性梯度算法（AdaGrad），均方根传播（RMSProp）:</p><pre><code class="lang-python">import torchoptimizer = torch.optim.SGD(models.parameters,lr)optimizer = torch.optim.Adam(models.parameters,lr)optimizer = torch.optim.Adagrad(models.parameters,lr)optimizer = torch.optim.RMSprop(models.parameters,lr)# 训练时optimizer.zero_grad()</code></pre><h2 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a><code>torch.autograd</code></h2><p><code>torch.autograd</code>包主要的功能是完成神经网络后向传播中的链式求导，在前向传播的时候构建了一张计算图，在后向传播的时候完成对参数的更新。</p><pre><code class="lang-python">import torchfrom torch.autograd import Variableuse_gpu = torch.cuda.is_available()if use_gpu:    data,y = Variable(data.cuda()),Variable(y.cuda())else:    data,y = Variable(data),Variable(y)</code></pre><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a><code>torchvision</code></h2><p>在pytorch里面有两个核心的包，分别为<code>torch</code>和<code>torchvision</code>，<code>torchvision</code>包的主要功能是实现数据的处理，导入，里面也有预训练的常见模型，例如下面的<code>datasets</code>,<code>transforms</code>:</p><pre><code class="lang-python">from torchvision import datasets,models,transformsdata = datasets.MNIST(root=save_path,train=True,transform=self_defined_transform)model = models.vgg16(pretrained=False)transform = transforms.Normalize(mean=mean,std=std)</code></pre><h2 id="torch-save-amp-torch-load"><a href="#torch-save-amp-torch-load" class="headerlink" title="torch.save &amp; torch.load"></a><code>torch.save &amp; torch.load</code></h2><p>训练模型完事之后肯定就是要保存模型了呀，之后调用就可以之间测试了呀，在pytorch里面模型的保存和加载都有两种方法：</p><pre><code class="lang-python">import torch#保存整个模型，包括结构信息和参数信息torch.save(model_name,&quot;model_saved_path/model_name.pth&quot;)#只保存参数信息torch.save(model_name.state_dict(),&quot;model_saved_path/model_name.pth&quot;)#对应第一种模型保存方法，加载完整整个模型load_model = torch.load(&quot;model_saved_path/model_name.pth&quot;)#对应第二种模型保存方法，只加载模型参数，结构需要在上面先定义或导入model.load_state_dict(torch.load(&quot;model_saved_path/model_name.pth&quot;))</code></pre><p>另外一种可以参考：<br><a href="https://jishuin.proginn.com/p/763bfbd63554" target="_blank" rel="noopener">pytorch代码模板</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>组合数</title>
      <link href="/posts/9ac1aea8.html"/>
      <url>/posts/9ac1aea8.html</url>
      
        <content type="html"><![CDATA[<h1 id="组合数一列等式"><a href="#组合数一列等式" class="headerlink" title="组合数一列等式"></a>组合数一列等式</h1><p>求解：</p><script type="math/tex; mode=display">\sum_{k=1}^{n-1}{k\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}m^{n-k}</script><p>联想到二次项恒等式：</p><script type="math/tex; mode=display">\sum_{k=0}^n{\left( \begin{array}{c}    n\\    k\\\end{array} \right)}x^km^{n-k}=\left( x+m \right) ^n</script><script type="math/tex; mode=display">\therefore \left( x+m \right) ^{n-1}=\sum_{k=1}^{n-1}{\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}x^{k-1}m^{n-k}</script><script type="math/tex; mode=display">\therefore x\left( x+m \right) ^{n-1}=\sum_{k=1}^{n-1}{\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}x^km^{n-k}</script><script type="math/tex; mode=display">\therefore \frac{\partial \left[ x\left( x+m \right) ^{n-1} \right]}{\partial x}=\frac{\partial \left[ \sum_{k=1}^{n-1}{\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}x^km^{n-k} \right]}{\partial x}</script><script type="math/tex; mode=display">\therefore \left( x+m \right) ^{n-1}+x\left( n-1 \right) \left( x+m \right) ^{n-2}=\sum_{k=1}^{n-1}{k\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}x^{k-1}m^{n-k}</script><script type="math/tex; mode=display">\therefore \sum_{k=1}^{n-1}{k\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}m^{n-k}=\left( m+1 \right) ^{n-1}+\left( n-1 \right) \left( m+1 \right) ^{n-2}</script><script type="math/tex; mode=display">\therefore \sum_{k=1}^{n-1}{k\left( \begin{array}{c}    n-1\\    k-1\\\end{array} \right)}9^{n-k}=\left( n+9 \right) \cdot 10^{n-2}</script>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 恒等式 </tag>
            
            <tag> 组合数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正交与二次型</title>
      <link href="/posts/e8779f12.html"/>
      <url>/posts/e8779f12.html</url>
      
        <content type="html"><![CDATA[<h1 id="正交与二次型"><a href="#正交与二次型" class="headerlink" title="正交与二次型"></a>正交与二次型</h1><p>正交矩阵其实就是空间的旋转，先说结论：<br>椭圆的长短轴分别沿着矩阵$\boldsymbol{A}$的两个特征向量的方向，而两个与之对应的特征值分别是半长轴和半短轴的长度的平方的倒数。</p><p>拿我们最熟悉的椭圆方程举例：</p><script type="math/tex; mode=display">ax^2+2bxy+cy^2=1</script><p>化为矩阵形式：</p><script type="math/tex; mode=display">\left[\begin{array}{l}x \\y\end{array}\right]^{T}\left[\begin{array}{ll}a & b \\b & c\end{array}\right]\left[\begin{array}{l}x \\y\end{array}\right]=x^{T} A x=1</script><p>这个方程可能是双曲线也可能是椭圆或抛物线，如果是椭圆，则$\boldsymbol{A}$是正定矩阵，举例：</p><script type="math/tex; mode=display">5x^2+8xy+5y^2=1</script><p>可以转化为：</p><script type="math/tex; mode=display">\left[\begin{array}{l}x \\y\end{array}\right]^{T}\left[\begin{array}{ll}5 & 4 \\4 & 5\end{array}\right]\left[\begin{array}{l}x \\y\end{array}\right]=x^{T} A x=1</script><p>$\boldsymbol{A}$的特征值为：</p><script type="math/tex; mode=display">\lambda_1=1, \lambda_2=9</script><p>$\boldsymbol{A}$的归一化特征向量为：</p><script type="math/tex; mode=display">\mu_{1}=\left[\begin{array}{c}1 / \sqrt{2} \\-1 / \sqrt{2}\end{array}\right] \mu_{2}=\left[\begin{array}{c}1 / \sqrt{2} \\1 / \sqrt{2}\end{array}\right]</script><p>于是可以将$\boldsymbol{A}$正交分解：</p><script type="math/tex; mode=display">A=Q \Lambda Q^{-1}=Q \Lambda Q^{T}=\left[\begin{array}{cc}1 / \sqrt{2} & 1 / \sqrt{2} \\-1 / \sqrt{2} & 1 / \sqrt{2}\end{array}\right]\left[\begin{array}{cc}1 & 0 \\0 & 9\end{array}\right]\left[\begin{array}{cc}1 / \sqrt{2} & -1 / \sqrt{2} \\1 / \sqrt{2} & 1 / \sqrt{2}\end{array}\right]</script><p>因此：</p><script type="math/tex; mode=display">\begin{aligned}P(f) &=x^{T} A x=x^{T} Q \Lambda Q^{T} x \\&=\left(Q^{T} x\right)^{T} \Lambda\left(Q^{T} x\right) \\&=1\left(\frac{x-y}{\sqrt{2}}\right)^{2}+9\left(\frac{x+y}{\sqrt{2}}\right)^{2}\\&=m^2+9n^2\end{aligned}</script><p>且：</p><script type="math/tex; mode=display">\begin{cases}    m=\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y\\    n=\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y\\\end{cases}</script><p>相当于$(x,y)$逆时针旋转了45°得到$(m,n)$,这样就得到了旋转后的方程，以及旋转方向。</p><ul><li>注意：二维的旋转矩阵是:<script type="math/tex; mode=display">\begin{cases}  m=\cos \theta \cdot x-\sin \theta \cdot y\\  n=\sin \theta \cdot x+\cos \theta \cdot y\\\end{cases}</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 线性代数 </tag>
            
            <tag> 正交 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD奇异分解</title>
      <link href="/posts/563bbb46.html"/>
      <url>/posts/563bbb46.html</url>
      
        <content type="html"><![CDATA[<h1 id="SVD奇异分解"><a href="#SVD奇异分解" class="headerlink" title="SVD奇异分解"></a>SVD奇异分解</h1><p>SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵A的SVD为：</p><script type="math/tex; mode=display">A=U\varSigma V^T</script><p>其中$U$是一个$m\times m$的矩阵，$\varSigma$是一个$m\times n$除了主对角线上的元素以外全为0，主对角线上的每个元素都称为<strong>奇异值</strong>，$V$是一个$n\times n$的矩阵。$U$和$V$都是酉矩阵，即满足</p><script type="math/tex; mode=display">U^TU=I, \quad V^T V =I</script><p><img src="https://i.loli.net/2021/08/25/YXHyEqIoxW9tkLu.png" alt="图解.png"></p><p>SVD分解步骤：</p><ul><li><strong>step1:求$V$</strong></li></ul><p>求$A^TA$(大小为$n\times n$)的特征值和特征向量：</p><script type="math/tex; mode=display">(A^TA)v_i=\lambda_i v_i</script><p>所有特征向量组成矩阵$V$<br>，$V$中的每个特征向量叫做$A$的右奇异向量。</p><ul><li><strong>step2:求$U$</strong></li></ul><p>求$AA^T$(大小为$m\times m$)的特征值和特征向量：</p><script type="math/tex; mode=display">(AA^T)u_i=\lambda_i u_i</script><p>所有特征向量组成矩阵$U$<br>，$U$中的每个特征向量叫做$A$的左奇异向量。</p><ul><li><strong>step3：求$\varSigma$</strong></li></ul><script type="math/tex; mode=display">A=U \Sigma V^{T} \Rightarrow A V=U \Sigma V^{T} V \Rightarrow A V=U \Sigma \Rightarrow A v_{i}=\sigma_{i} u_{i} \Rightarrow \sigma_{i}=A v_{i} / u_{i}</script><p>进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：</p><script type="math/tex; mode=display">A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma U^{T} \Rightarrow A^{T} A=V \Sigma U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}</script><p>由此可以看出</p><script type="math/tex; mode=display">\sigma_{i}=\sqrt{\lambda_{i}}</script><p>可从此计算奇异值</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><script type="math/tex; mode=display">\mathbf{A}=\left(\begin{array}{ll}0 & 1 \\1 & 1 \\1 & 0\end{array}\right)</script><p>首先求出$A^TA$和$AA^T$：</p><script type="math/tex; mode=display">\begin{gathered}\mathbf{A}^{\mathbf{T}} \mathbf{A}=\left(\begin{array}{lll}0 & 1 & 1 \\1 & 1 & 0\end{array}\right)\left(\begin{array}{ll}0 & 1 \\1 & 1 \\1 & 0\end{array}\right)=\left(\begin{array}{ll}2 & 1 \\1 & 2\end{array}\right) \\\mathbf{A} \mathbf{A}^{\mathbf{T}}=\left(\begin{array}{ll}0 & 1 \\1 & 1 \\1 & 0\end{array}\right)\left(\begin{array}{lll}0 & 1 & 1 \\1 & 1 & 0\end{array}\right)=\left(\begin{array}{lll}1 & 1 & 0 \\1 & 2 & 1 \\0 & 1 & 1\end{array}\right)\end{gathered}</script><p>进而求出$A^TA$的特征值和特征向量：</p><script type="math/tex; mode=display">\lambda_{1}=3 ; v_{1}=\left(\begin{array}{c}1 / \sqrt{2} \\1 / \sqrt{2}\end{array}\right) ; \lambda_{2}=1 ; v_{2}=\left(\begin{array}{c}-1 / \sqrt{2} \\1 / \sqrt{2}\end{array}\right)</script><p>接着求出$AA^T$的特征值和特征向量：</p><script type="math/tex; mode=display">\lambda_{1}=3 ; u_{1}=\left(\begin{array}{c}1 / \sqrt{6} \\2 / \sqrt{6} \\1 / \sqrt{6}\end{array}\right) ; \lambda_{2}=1 ; u_{2}=\left(\begin{array}{c}1 / \sqrt{2} \\0 \\-1 / \sqrt{2}\end{array}\right) ; \lambda_{3}=0 ; u_{3}=\left(\begin{array}{c}1 / \sqrt{3} \\-1 / \sqrt{3} \\1 / \sqrt{3}\end{array}\right)</script><p>利用$Av_i=\sigma_i u_i , i=1,2$求奇异值：</p><script type="math/tex; mode=display">\begin{aligned}&\left(\begin{array}{ll}0 & 1 \\1 & 1 \\1 & 0\end{array}\right)\left(\begin{array}{c}1 / \sqrt{2} \\1 / \sqrt{2}\end{array}\right)=\sigma_{1}\left(\begin{array}{c}1 / \sqrt{6} \\2 / \sqrt{6} \\1 / \sqrt{6}\end{array}\right) \Rightarrow \sigma_{1}=\sqrt{3} \\&\left(\begin{array}{ll}0 & 1 \\1 & 1 \\1 & 0\end{array}\right)\left(\begin{array}{c}-1 / \sqrt{2} \\1 / \sqrt{2}\end{array}\right)=\sigma_{2}\left(\begin{array}{c}1 / \sqrt{2} \\0 \\-1 / \sqrt{2}\end{array}\right) \Rightarrow \sigma_{2}=1\end{aligned}</script><p>也可以用$\sigma_i=\sqrt{\lambda_i}$直接求出奇异值为$\sqrt{3}$和1，最终得到A的奇异值分解为：</p><script type="math/tex; mode=display">A=U \Sigma V^{T}=\left(\begin{array}{ccc}1 / \sqrt{6} & 1 / \sqrt{2} & 1 / \sqrt{3} \\2 / \sqrt{6} & 0 & -1 / \sqrt{3} \\1 / \sqrt{6} & -1 / \sqrt{2} & 1 / \sqrt{3}\end{array}\right)\left(\begin{array}{cc}\sqrt{3} & 0 \\0 & 1 \\0 & 0\end{array}\right)\left(\begin{array}{cc}1 / \sqrt{2} & 1 / \sqrt{2} \\-1 / \sqrt{2} & 1 / \sqrt{2}\end{array}\right)</script><p>python和matlab都有内置的函数。</p>]]></content>
      
      
      <categories>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学习 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 奇异分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT代替密码密码验证</title>
      <link href="/posts/3c3e1a2a.html"/>
      <url>/posts/3c3e1a2a.html</url>
      
        <content type="html"><![CDATA[<h1 id="PAT代替密码验证"><a href="#PAT代替密码验证" class="headerlink" title="PAT代替密码验证"></a>PAT代替密码验证</h1><p>2021年8月13日git终止密码验证提交程序，强制使用PAT验证方式，这是一件好事，减少密码泄露。</p><pre><code>fatal: unable to access &#39;https://github.com/wsp666/wsp666.github.io/&#39;: The requested URL returned error: 403FATAL Something&#39;s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.htmlError: Spawn failed    at ChildProcess.&lt;anonymous&gt; (D:\Hexo\blog\node_modules\hexo-util\lib\spawn.js:51:21)    at ChildProcess.emit (events.js:210:5)    at ChildProcess.cp.emit (D:\Hexo\blog\node_modules\cross-spawn\lib\enoent.js:34:29)    at Process.ChildProcess._handle.onexit (internal/child_process.js:272:12)</code></pre><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>通过”github -&gt; account -&gt; settings -&gt; Developer settings -&gt; Personal access tokens”处，点击<code>Generate new token</code>。因为只是需要<code>git push</code>之类的操作，所以勾选<code>repo</code>选项，即可。随后token生成成功，然后再在本地<code>git bash</code>中进行<code>git push</code>，账号还是原来的github账号，密码改为填写刚刚生成的token即可。之后就不会再重复输入密码了。</p><p><img src="https://i.loli.net/2021/08/14/QFcM4T6uP8DyeVj.png" alt="image.png"></p><p>保存好代码后直接替换之前的密码即可。可在凭据管理器中查看相关信息。</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch训练神经网络</title>
      <link href="/posts/1272873f.html"/>
      <url>/posts/1272873f.html</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch训练神经网络"><a href="#PyTorch训练神经网络" class="headerlink" title="PyTorch训练神经网络"></a>PyTorch训练神经网络</h1><p>可以使用<code>torch.nn</code>包来构建神经网络。</p><p><code>nn</code>包则依赖于<code>autograd</code>包来定义模型并对它们求导。一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法，该方法返回<code>output</code>。</p><p>如图这个神经网络可以对数字进行分类：<br><img src="https://i.loli.net/2021/08/13/2GZXPvR6bwWI9i5.png" alt="image.png"><br>这是一个简单的前馈神经网络 (feed-forward network）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。</p><p>一个神经网络的典型训练过程如下：</p><ul><li>定义包含一些可学习参数(或者叫权重）的神经网络</li><li>在输入数据集上迭代</li><li>通过网络处理输入</li><li>计算 loss (输出和正确答案的距离）</li><li>将梯度反向传播给网络的参数</li><li>更新网络的权重，一般使用一个简单的规则：<script type="math/tex">weight = weight - learning_rate * gradient。</script><h2 id="定义神经网络"><a href="#定义神经网络" class="headerlink" title="定义神经网络"></a>定义神经网络</h2><pre><code class="lang-python">import torchimport torch.nn as nnimport torch.nn.functional as F</code></pre>导包后定义神经网络：</li></ul><pre><code class="lang-python">class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        # 输入图像channel：1；输出channel：6；5x5卷积核        self.conv1 = nn.Conv2d(1, 6, 5)        self.conv2 = nn.Conv2d(6, 16, 5)        # an affine operation: y = Wx + b        self.fc1 = nn.Linear(16 * 5 * 5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)    def forward(self, x):        # 2x2 Max pooling        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))        # 如果是方阵,则可以只使用一个数字进行定义        x = F.max_pool2d(F.relu(self.conv2(x)), 2)        x = x.view(-1, self.num_flat_features(x))        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        return x    def num_flat_features(self, x):        size = x.size()[1:]  # 除去批处理维度的其他所有维度        num_features = 1        for s in size:            num_features *= s        return num_features</code></pre><p>测试：</p><pre><code class="lang-python">net = Net()print(net)</code></pre><p>输出：</p><pre><code class="lang-python">Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>个模型的可学习参数可以通过<code>net.parameters()</code>返回:</p><pre><code class="lang-python">params = list(net.parameters())print(len(params))print(params[0].size())  # conv1&#39;s .weight</code></pre><p>输出：</p><pre><code class="lang-python">10torch.Size([6, 1, 5, 5])</code></pre><p>将一个随机的 32x32作为输入。这个网络 (LeNet）的期待输入是 32x32 的张量。如果使用 MNIST 数据集来训练这个网络，要把图片大小重新调整到 32x32。</p><pre><code class="lang-python">input = torch.randn(1, 1, 32, 32)out = net(input)print(out)</code></pre><p>输出为：</p><pre><code class="lang-python">tensor([[ 0.0399, -0.0856,  0.0668,  0.0915,  0.0453, -0.0680, -0.1024,  0.0493,         -0.1043, -0.1267]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>清零所有参数的梯度缓存，然后进行随机梯度的反向传播：</p><pre><code class="lang-python">net.zero_grad()out.backward(torch.randn(1, 10))</code></pre><p><code>torch.nn</code>只支持小批量处理 (mini-batches）。整个 <code>torch.nn</code> 包只支持小批量样本的输入，不支持单个样本的输入。比如，<code>nn.Conv2d</code> 接受一个4维的张量，即<code>nSamples x nChannels x Height x Width</code> 如果是一个单独的样本，只需要使用<code>input.unsqueeze(0)</code>来添加一个“假的”批大小维度。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>一个损失函数接受一对<code>(output, target)</code>作为输入，计算一个值来估计网络的输出和目标值相差多少。</p><p><code>nn</code>包中有很多不同的损失函数。<code>nn.MSELoss</code>是比较简单的一种，它计算输出和目标的均方误差。例如：</p><pre><code class="lang-python">output = net(input)target = torch.randn(10)  # 本例子中使用模拟数据target = target.view(1, -1)  # 使目标值与数据值尺寸一致criterion = nn.MSELoss()loss = criterion(output, target)print(loss)</code></pre><p>输出：</p><pre><code class="lang-python">tensor(1.0263, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果使用<code>loss</code>的<code>.grad_fn</code>属性跟踪反向传播过程，会看到计算图如下：</p><pre><code class="lang-python">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear      -&gt; MSELoss      -&gt; loss</code></pre><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们只需要调用<code>loss.backward()</code>来反向传播误差。我们需要清零现有的梯度，否则梯度将会与已有的梯度累加。</p><p>现在，我们将调用<code>loss.backward()</code>，并查看 <code>conv1</code>层的偏置在反向传播前后的梯度。</p><pre><code class="lang-python">net.zero_grad()     # 清零所有参数(parameter）的梯度缓存print(&#39;conv1.bias.grad before backward&#39;)print(net.conv1.bias.grad)loss.backward()print(&#39;conv1.bias.grad after backward&#39;)print(net.conv1.bias.grad)</code></pre><p>输出：</p><pre><code class="lang-python">conv1.bias.grad before backwardtensor([0., 0., 0., 0., 0., 0.])conv1.bias.grad after backwardtensor([ 0.0084,  0.0019, -0.0179, -0.0212,  0.0067, -0.0096])</code></pre><h2 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h2><p>最简单的更新规则是随机梯度下降法 (SGD）:</p><p><code>weight = weight - learning_rate * gradient</code></p><p><code>torch.optim</code>中实现了所有的这些方法。使用它很简单：</p><pre><code class="lang-python">import torch.optim as optim# 创建优化器(optimizer）optimizer = optim.SGD(net.parameters(), lr=0.01)# 在训练的迭代中：optimizer.zero_grad()   # 清零梯度缓存output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()    # 更新参数</code></pre><h2 id="训练一个图片分类器"><a href="#训练一个图片分类器" class="headerlink" title="训练一个图片分类器"></a>训练一个图片分类器</h2><p>以训练一个图片分类器为实例，按照上述步骤进行：</p><h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><pre><code class="lang-python">import torchimport torchvisionimport torchvision.transforms as transformstransform = transforms.Compose(    [transforms.ToTensor(),     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;,           &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)</code></pre><h3 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h3><pre><code class="lang-python">import matplotlib.pyplot as pltimport numpy as np# 输出图像的函数def imshow(img):    img = img / 2 + 0.5     # unnormalize    npimg = img.numpy()    plt.imshow(np.transpose(npimg, (1, 2, 0)))    plt.show()# 随机获取训练图片dataiter = iter(trainloader)images, labels = dataiter.next()# 显示图片imshow(torchvision.utils.make_grid(images))# 打印图片标签print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))</code></pre><h3 id="定义一个卷积神经网络"><a href="#定义一个卷积神经网络" class="headerlink" title="定义一个卷积神经网络"></a>定义一个卷积神经网络</h3><p>将最初定义的神经网络拿过来，并将其修改成输入为3通道图像(替代原来定义的单通道图像）。</p><pre><code class="lang-python">import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = nn.Conv2d(3, 6, 5)        self.pool = nn.MaxPool2d(2, 2)        self.conv2 = nn.Conv2d(6, 16, 5)        self.fc1 = nn.Linear(16 * 5 * 5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)    def forward(self, x):        x = self.pool(F.relu(self.conv1(x)))        x = self.pool(F.relu(self.conv2(x)))        x = x.view(-1, 16 * 5 * 5)        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        return xnet = Net()</code></pre><h3 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h3><p>使用多分类的交叉熵损失函数和随机梯度下降优化器：</p><pre><code class="lang-python">import torch.optim as optimcriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</code></pre><h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><p>遍历数据迭代器，并将输入“喂”给网络和优化函数。</p><pre><code class="lang-python">for epoch in range(2):  # loop over the dataset multiple times    running_loss = 0.0    for i, data in enumerate(trainloader, 0):        # get the inputs        inputs, labels = data        # zero the parameter gradients        optimizer.zero_grad()        # forward + backward + optimize        outputs = net(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        # print statistics        running_loss += loss.item()        if i % 2000 == 1999:    # print every 2000 mini-batches            print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000))            running_loss = 0.0print(&#39;Finished Training&#39;)</code></pre><p>保存训练好的模型：</p><pre><code class="lang-python">PATH = &#39;./cifar_net.pth&#39;torch.save(net.state_dict(), PATH)</code></pre><h3 id="使用测试数据测试网络"><a href="#使用测试数据测试网络" class="headerlink" title="使用测试数据测试网络"></a>使用测试数据测试网络</h3><p>将通过预测神经网络输出的标签来检查这个问题，并和正确样本进行对比。如果预测是正确的，将样本添加到正确预测的列表中。</p><pre><code class="lang-python">dataiter = iter(testloader)images, labels = dataiter.next()# 输出图片imshow(torchvision.utils.make_grid(images))print(&#39;GroundTruth: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))</code></pre><p>加载保存的模型：</p><pre><code class="lang-python">net = Net()net.load_state_dict(torch.load(PATH))</code></pre><p>神经网络认为上面的例子是:</p><pre><code class="lang-python">outputs = net(images)</code></pre><p>输出是10个类别的量值。一个类的值越高，网络就越认为这个图像属于这个特定的类。让我们得到最高量值的下标/索引；</p><pre><code class="lang-python">_, predicted = torch.max(outputs, 1)print(&#39;Predicted: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j]] for j in range(4)))</code></pre><p>结果对比：</p><pre><code>GroundTruth:    cat  ship  ship planePredicted:    dog  ship  ship plane</code></pre><pre><code class="lang-python">correct = 0total = 0with torch.no_grad():    for data in testloader:        images, labels = data        outputs = net(images)        _, predicted = torch.max(outputs.data, 1)        total += labels.size(0)        correct += (predicted == labels).sum().item()print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (    100 * correct / total))</code></pre><p>观测整个数据集上的表现：</p><pre><code class="lang-python">correct = 0total = 0with torch.no_grad():    for data in testloader:        images, labels = data        outputs = net(images)        _, predicted = torch.max(outputs.data, 1)        total += labels.size(0)        correct += (predicted == labels).sum().item()print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (    100 * correct / total))</code></pre><p>输出结果：</p><pre><code>Accuracy of the network on the 10000 test images: 55 %</code></pre><p>这比随机选取(即从10个类中随机选择一个类，正确率是10%）要好很多。看来网络确实学到了一些东西。接着看看具体是哪些表现的差的类呢？</p><pre><code class="lang-python">class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))with torch.no_grad():    for data in testloader:        images, labels = data        outputs = net(images)        _, predicted = torch.max(outputs, 1)        c = (predicted == labels).squeeze()        for i in range(4):            label = labels[i]            class_correct[label] += c[i].item()            class_total[label] += 1for i in range(10):    print(&#39;Accuracy of %5s : %2d %%&#39; % (        classes[i], 100 * class_correct[i] / class_total[i]))</code></pre><h3 id="GPU跑pytorch"><a href="#GPU跑pytorch" class="headerlink" title="GPU跑pytorch"></a>GPU跑pytorch</h3><p>用GPU跑pytorch程序就3点：</p><ol><li>申明用GPU</li><li>把你的model放到GPU上</li><li>把数据和标签放到GPU上<h4 id="申明"><a href="#申明" class="headerlink" title="申明"></a>申明</h4><pre><code class="lang-python">device=torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)print(device)</code></pre><h4 id="将模型放到GPU上"><a href="#将模型放到GPU上" class="headerlink" title="将模型放到GPU上"></a>将模型放到GPU上</h4>在创建完网络 或者引用网络之后，我们需要实体化我们的网络。直接在后面加一句话就可以<pre><code class="lang-python">net= Net ()net.to(device)</code></pre><h4 id="把数据放到GPU上"><a href="#把数据放到GPU上" class="headerlink" title="把数据放到GPU上"></a>把数据放到GPU上</h4><pre><code class="lang-python">inputs, labels = data         inputs, labels = inputs.to(device), labels.to(device)</code></pre>最后可以用<code>nvidia-smi</code>查看是否利用gpu训练。</li></ol>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch深度学习基础</title>
      <link href="/posts/5ff9131a.html"/>
      <url>/posts/5ff9131a.html</url>
      
        <content type="html"><![CDATA[<h1 id="pytorch深度学习基础"><a href="#pytorch深度学习基础" class="headerlink" title="pytorch深度学习基础"></a>pytorch深度学习基础</h1><h2 id="Tensor对象及其运算"><a href="#Tensor对象及其运算" class="headerlink" title="Tensor对象及其运算"></a>Tensor对象及其运算</h2><p>Tensor对象是一个维度任意的矩阵，但是一个Tensor中所有元素的数据类型必须一致。torch包含的数据类型和普遍编程语言的数据类型类似，包含浮点型，有符号整型和无符号整形，这些类型既可以定义在CPU上，也可以定义在GPU上。在使用Tensor数据类型时，可以通过dtype属性指定它的数据类型，device指定它的设备（CPU或者GPU）。</p><pre><code class="lang-python">import torchimport numpy as npa = torch.tensor([[1, 2], [3, 4]])b = torch.tensor([[1, 2], [3, 4]])print(a*b)print(torch.mm(a, b))</code></pre><p>显示：</p><pre><code class="lang-python">tensor([[ 1,  4],        [ 9, 16]])tensor([[ 7, 10],        [15, 22]])</code></pre><p><code>torch.clamp</code>起的是分段函数的作用，可用于去掉矩阵中过小或者过大的元素。</p><p>PyTorch提供了大量的对<code>Tensor</code>进行操作的函数或方法，这些函数内部使用指针实现对矩阵的形状变换，拼接，拆分等操作，使得我们无须关心<code>Tensor</code>在内存的物理结构或者管理指针就可以方便且快速的执行这些操作。<code>Tensor.nelement()</code>，<code>Tensor.ndimension()</code>，<code>ndimension.size()</code>可分别用来查看矩阵元素的个数，轴的个数以及维度，属性<code>Tensor.shape</code>也可以用来查看Tensor的维度。</p><h2 id="Tensor的变换、拼接和拆分"><a href="#Tensor的变换、拼接和拆分" class="headerlink" title="Tensor的变换、拼接和拆分"></a>Tensor的变换、拼接和拆分</h2><p>在PyTorch中，<code>Tensor.reshape</code>和<code>Tensor.view</code>都能被用来更改<code>Tensor</code>的维度。它们的区别在于，<code>Tensor.view</code>要求<code>Tensor</code>的物理存储必须是连续的，否则将报错，而<code>Tensor.reshape</code>则没有这种要求。但是，<code>Tensor.view</code>返回的一定是一个索引，更改返回值，则原始值同样被更改，<code>Tensor.reshape</code>返回的是引用还是拷贝是不确定的。它们的相同之处都接收要输出的维度作为参数，切输出的矩阵元素个数不能改变，可以在维度中输入<code>-1</code>，PyTorch会自动推断它的数值。</p><p>在PyTorch中，Tensor.reshape和Tensor.view都能被用来更改Tensor的维度。它们的区别在于，Tensor.view要求Tensor的物理存储必须是连续的，否则将报错，而Tensor.reshape则没有这种要求。但是，Tensor.view返回的一定是一个索引，更改返回值，则原始值同样被更改，Tensor.reshape返回的是引用还是拷贝是不确定的。它们的相同之处都接收要输出的维度作为参数，切输出的矩阵元素个数不能改变，可以在维度中输入-1，PyTorch会自动推断它的数值。</p><pre><code class="lang-python">b =torch.squeeze(a)print(b.shape)</code></pre><p>返回值：</p><pre><code class="lang-python">torch.Size([2, 3, 4, 5])</code></pre><p>二维转置用<code>torch.t</code>，多维度可以用<code>permute()</code>函数实现：</p><pre><code class="lang-python">a = torch.rand([1,  224, 224, 3])print(a)print(a.shape)b = a.permute(0,3,1,2)print(b.shape)</code></pre><p>返回值为：</p><pre><code class="lang-python">tensor([[[[3.6860e-01, 2.8806e-01, 2.1226e-01],          [8.5628e-04, 8.3488e-01, 5.4180e-01],          [5.0457e-01, 7.1150e-01, 1.8034e-02],          ...,          [1.0439e-01, 9.6792e-01, 5.4312e-02],          [5.9024e-01, 9.3023e-01, 7.6815e-01],          [2.8887e-01, 9.4723e-01, 6.4809e-01]],         [[6.3776e-03, 4.4275e-01, 9.9699e-01],          [1.6847e-01, 4.9107e-01, 9.8282e-01],          [5.7448e-01, 9.2207e-01, 5.1265e-01],          ...,          [6.8215e-02, 2.9971e-01, 2.2088e-01],          [1.5893e-02, 8.3497e-01, 4.0665e-01],          [3.9007e-01, 3.7976e-01, 3.4293e-01]],         [[3.9511e-01, 5.3699e-01, 8.7523e-01],          [2.0365e-02, 7.8796e-01, 3.4477e-01],          [1.0294e-01, 4.3773e-01, 1.5031e-01],          ...,          [1.4725e-01, 4.1975e-01, 8.9547e-01],          [2.3337e-01, 7.7988e-01, 7.1522e-01],          [2.7721e-01, 1.6050e-01, 1.0747e-02]],         ...,         [[4.0493e-01, 6.1730e-01, 9.4033e-01],          [3.5808e-01, 3.7298e-01, 9.8320e-01],          [3.7041e-02, 4.1414e-01, 6.6998e-01],          ...,          [9.5621e-01, 4.2584e-01, 5.6640e-01],          [3.5720e-02, 2.0758e-01, 6.1817e-01],          [6.9376e-01, 1.1626e-01, 2.9182e-01]],         [[5.5343e-01, 5.2249e-01, 6.6126e-02],          [5.8498e-01, 2.5871e-02, 2.3405e-01],          [6.4199e-02, 3.0509e-01, 5.7775e-01],          ...,          [6.1127e-01, 4.3303e-01, 3.7878e-01],          [1.3533e-02, 5.4714e-01, 9.1301e-01],          [3.5130e-01, 7.8249e-01, 9.5742e-01]],         [[8.2594e-01, 1.8202e-01, 9.5836e-01],          [4.6360e-01, 8.8640e-01, 8.6581e-01],          [6.4844e-01, 8.8921e-01, 5.9467e-01],          ...,          [3.8022e-01, 8.0356e-01, 3.2146e-01],          [6.1877e-01, 2.7097e-01, 3.7226e-02],          [1.4843e-01, 6.5034e-01, 7.2513e-02]]]])torch.Size([1, 224, 224, 3])torch.Size([1, 3, 224, 224])</code></pre><p>PyTorch提供了<code>torch.cat</code>和<code>torch.stack</code>用于拼接矩阵，不同的是,<code>torch.cat</code>在已有的轴dim上拼接矩阵，给定轴的维度可以不同，而其他轴的维度必须相同。<code>torch.stack</code>在新的轴上拼接，它要求被拼接的矩阵所有维度都相同。</p><pre><code class="lang-python">a =torch.randn(2,3)b =torch.randn(3,3)#默认维度为dim=0c=torch.cat((a,b))d=torch.cat((b,b,b),dim=1)print(c.shape)print(d.shape)</code></pre><p>返回：</p><pre><code class="lang-python">torch.Size([5, 3])torch.Size([3, 9])</code></pre><pre><code class="lang-python">c=torch.stack((b,b), dim=1)d=torch.stack((b,b), dim=0)print(c.shape)print(d.shape)</code></pre><p>返回：</p><pre><code class="lang-python">torch.Size([3, 2, 3])torch.Size([2, 3, 3])</code></pre><p>除了拼接矩阵，PyTorch还提供了<code>torch.split</code>和<code>torch.chunk</code>用于拆分矩阵。它们的不同之处在于，<code>torch.split</code>传入的是拆分后每个矩阵的大小，可以传入<code>list</code>，也可以传入整数，而<code>torch.chunk</code>传入的是拆分的矩阵个数。</p><p>Reduction运算的特点是它往往对一个<code>Tensor</code>内的元素做归约操作，比如<code>torch.max</code>找极大值，<code>torch.cumsum</code>计算累加，它还提供了dim参数来指定沿矩阵的哪个维度执行操作。</p><p>将<code>Tensor</code>的<code>requires_grad</code>属性设置为<code>True</code>时，PyTorch的<code>torch.autograd</code>会自动的追踪它的计算轨迹，当需要计算微分的时候，只需要对最终计算结果的<code>Tensor</code>调用<code>backward</code>方法,中间所有计算结点的微分就会被保存在<code>grad</code>属性中了。</p><pre><code class="lang-python">x = torch.arange(9).view(3,3)print(x.requires_grad)print(&#39;*******************************************&#39;)x = torch.rand(3, 3, requires_grad = True)print(x)print(&#39;*******************************************&#39;)w = torch.ones(3, 3, requires_grad=True)y = torch.sum(torch.mm(w,x))print(y)print(&#39;*******************************************&#39;)y.backward()print(y.grad)print(x.grad)print(w.grad)</code></pre><p>返回：</p><pre><code class="lang-python">False*******************************************tensor([[0.6608, 0.0725, 0.9230],        [0.8255, 0.2638, 0.0431],        [0.6806, 0.4232, 0.4164]], requires_grad=True)*******************************************tensor(12.9265, grad_fn=&lt;SumBackward0&gt;)*******************************************Nonetensor([[3., 3., 3.],        [3., 3., 3.],        [3., 3., 3.]])tensor([[1.6564, 1.1323, 1.5202],        [1.6564, 1.1323, 1.5202],        [1.6564, 1.1323, 1.5202]])Process finished with exit code 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习基础知识</title>
      <link href="/posts/54da7039.html"/>
      <url>/posts/54da7039.html</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习基础知识"><a href="#机器学习基础知识" class="headerlink" title="机器学习基础知识"></a>机器学习基础知识</h1><h2 id="模型评估与参数选择"><a href="#模型评估与参数选择" class="headerlink" title="模型评估与参数选择"></a>模型评估与参数选择</h2><p>若对于给定的输入$x$，若某个模型的输出$\hat{y}=f(x)$偏离真实目标值$y$，那么就说明模型存在误差；$\hat{y}$偏离$y$的程度可以用关于$\hat{y}$和$y$某个函数$L(y,\hat{y})$来表示，作为误差的度量标准：这样的函数$L(y,\hat{y})$称为损失函数。</p><p>在某种损失函数度量下，训练集上的平均误差被称为训练误差，测试集上的误差称为泛化误差。由于我们训练得到一个模型最终的目的是为了在未知的数据上得到尽可能准确的结果，因此泛化误差是衡量一个模型泛化能力的重要标准。</p><p>训练集可能存在以下问题：</p><ol><li>训练集样本太少，缺乏代表性；</li><li>训练集中本身存在错误的样本，即噪声。如果片面地追求训练误差的最小化，就会导致模型参数复杂度增加，使得模型过拟合。</li></ol><p>为了选择效果最佳的模型，防止过拟合的问题，通常可以采取的方法有：</p><ul><li>使用验证集调参</li><li>对损失函数进行正则化</li></ul><p>参数的选择（即调参）必须在一个独立于训练集和测试集的数据集上进行，这样的用于模型调参的数据集被称为开发集或验证集。</p><p>很多时候我们能得到的数据量非常有限。这个时候我们可以不显式地使用验证集，而是重复使用训练集和测试集，这种方法称为交叉验证。常用的交叉验证方法有：</p><ul><li>简单交叉验证。</li><li>K-重交叉验证。</li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>为了避免过拟合，需要选择参数复杂度最小的模型。这是因为如果有两个效果相同的模型，而它们的参数复杂度不相同，那么冗余的复杂度一定是由于过拟合导致的。为了选择复杂度较小的模型，一种策略是在优化目标中加入正则化项，以惩罚冗余的复杂度：</p><script type="math/tex; mode=display">\min_{\theta} L\left( y,\hat{y};\theta \right) +\lambda J\left( \theta \right)</script><p>其中$\theta$为模型参数，$L\left( y,\hat{y};\theta \right) $ 为原来的损失函数，$J\left( \theta \right) $ 是正则化项，$\lambda$ 用于调整正则化项的权重。正则化项通常为$\theta$的某阶向量范数。</p><h2 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h2><p>模型与最优化算法的选择，很大程度上取决于能得到什么样的数据。如果数据集中样本点只包含了模型的输入$x$，那么就需要采用非监督学习的算法；如果这些样本点以$(x,y)$这样的输入-输出二元组的形式出现，那么就可以采用监督学习的算法。</p><p>监督学习算法主要适用于两大类问题：回归和分类。这两类问题的区别在于：回归问题的输出是连续值，而分类问题的输出是离散值。</p><p>度量分类问题的指标通常为准确率（$Accuracy$）：对于测试集中$D$个样本，有k个被正确分类，$D-k$个被错误分类，则准确率为：</p><script type="math/tex; mode=display">Accuracy=k/D</script><p>然而在一些特殊的分类问题中，属于各类的样本的并不是均一分布，甚至其出现概率相差很多个数量级，这种分类问题称为不平衡类问题。在不平衡类问题中，准确率并没有多大意义，我们需要一些别的指标。</p><p>通常在不平衡类问题中，我们使用F-度量来作为评价模型的指标。以二元不平衡分类问题为例，这种分类问题往往是异常检测，模型的好坏往往取决于能否很好地检出异常，同时尽可能不误报异常。定义占样本少数的类为正类（Positive class），占样本多数的为负类（Negative class），那么预测只可能出现4种状况：<br>将正类样本预测为正类（True Positive, TP）<br>将负类样本预测为正类（False Positive, FP）<br>将正类样本预测为负类（False Negative, FN）<br>将负类样本预测为负类（True Negative, TN）</p><p>定义召回率:</p><script type="math/tex; mode=display">R=\frac{|TP|}{|TP|+|FN|}</script><p>召回率度量了在所有的正类样本中，模型正确检出的比率，因此也称为查全率；</p><p>定义精确率（precision）：</p><script type="math/tex; mode=display">R=\frac{|TP|}{|TP|+|FP|}</script><p>精确率度量了在所有被模型预测为正类的样本中，正确预测的比率，因此也称查准率。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 监督学习 </tag>
            
            <tag> 非监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信号与系统学习笔记</title>
      <link href="/posts/c8ba4314.html"/>
      <url>/posts/c8ba4314.html</url>
      
        <content type="html"><![CDATA[<h1 id="信号与系统"><a href="#信号与系统" class="headerlink" title="信号与系统"></a>信号与系统</h1><p>这本书是大二寒假自学《信号与系统》整理的笔记，由于当时基础并不是很扎实，内容也省略了一些，采用LaTeX排版，<code>tizk</code>绘图，封面是著名考研竞赛数学中科大向老师开源的仿蒲和平封皮，LaTeX模板是采用<code>elegantbook</code>，内容主要是参考武汉理工刘泉著《信号与系统》，少部分是奥本海默上的。</p><p>大一期间运营公众号：whut数学（不图任何利润）很感谢好友with me（当时的网名）对我的支持和帮助，因此pdf中有公众号的水印<br><br></p><div class="row">    <embed src="https://yzuf49.coding-pages.com/file/SASystem.pdf" width="100%" height="550" type="application/pdf"></div><p><br></p>]]></content>
      
      
      <categories>
          
          <category> 信号与系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
            <tag> 信号与系统 </tag>
            
            <tag> PDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差错控制编码</title>
      <link href="/posts/b39abbbf.html"/>
      <url>/posts/b39abbbf.html</url>
      
        <content type="html"><![CDATA[<h1 id="差错控制编码"><a href="#差错控制编码" class="headerlink" title="差错控制编码"></a>差错控制编码</h1><p>差错控制编码属于信道编码：</p><ul><li>目的：克服信道噪声及其他干扰引起的误码，提高传输的可靠性。</li><li>基本原理：信道编码器在信息码元序列中按照一定的关系加入一些冗余码元(也即监督码元)，信道译码器利用这种关系<strong>发现</strong>或<strong>纠正</strong>可能存在的错码。</li></ul><blockquote><p>差错控制方式：</p><ul><li>检错重发(ARQ)</li><li>前向纠错(FEC)</li><li>检错删除</li><li>反馈校验</li></ul><p>编码类型：</p><ul><li>线性码：监督吗和信息码关系由一组线性方程确定。可分为分组码和非分组码。</li><li>分组码：结构如下图所示。它是把信息序列以k个码元分为-组,通过编码器把每个信息组(k个信息码元)按定规则产生r个监督(校验)码元，从而构成每组长度为n=k+r的具有纠检功能的编码集合。因此，分组码中的每一码组的监督元仅与本组中的信息元有关。<br><img src="https://i.loli.net/2021/08/10/GkOs68FIUhAxm2p.png" alt="分组码"></li><li>卷积码:监督码元不仅和当前的一段信 息码元有关，而且还同前 面若干个信息段码元也有约束关系。卷积码是非分组码的一一种。</li></ul></blockquote><p><strong>码长、码重、码距</strong>的概念</p><p><strong>最小码距$d_0$与纠检错能力：</strong>$d_0$决定了编码的纠检错能力，对于分组码来说：</p><ul><li>检测$e$个错码，要求：<script type="math/tex; mode=display">d_0 \ge e+1</script></li><li>纠正$t$个错码，要求：<script type="math/tex; mode=display">d_0 \ge 2t+1</script></li><li>纠$t$个同时可检$e$个错码。要求：<script type="math/tex; mode=display">d_0 \ge e+t+1 \text{且}e>t</script></li></ul><blockquote><ul><li>编码效率：$R_c = k/n$</li><li>编码增益：在保持误码率恒定条件下，采用纠错编码所节省的信噪比称为编码增益。<h2 id="线性分组码"><a href="#线性分组码" class="headerlink" title="线性分组码"></a>线性分组码</h2>下面以举例为主：</li></ul></blockquote><p>监督方程：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}1 \cdot a_{6}+1 \cdot a_{5}+1 \cdot a_{4}+0 \cdot a_{3}+1 \cdot a_{2}+0 \cdot a_{1}+0 \cdot a_{0}=0 \\1 \cdot a_{6}+1 \cdot a_{5}+0 \cdot a_{4}+1 \cdot a_{3}+0 \cdot a_{2}+1 \cdot a_{1}+0 \cdot a_{0}=0 \\1 \cdot a_{6}+0 \cdot a_{5}+1 \cdot a_{4}+1 \cdot a_{3}+0 \cdot a_{2}+0 \cdot a_{1}+1 \cdot a_{0}=0\end{array}\right.</script><p>矩阵形式：</p><script type="math/tex; mode=display">\left[\begin{array}{l}1110100 \\1101010 \\1011001\end{array}\right]\left[\begin{array}{l}a_{6} \\a_{5} \\a_{4} \\a_{3} \\a_{2} \\a_{1} \\a_{0}\end{array}\right]=\left[\begin{array}{l}0 \\0 \\0\end{array}\right]</script><p>记作：</p><script type="math/tex; mode=display">\mathbf{H}\cdot \mathbf{A}^T = \mathbf{0}^T</script><p>$\mathbf{H}$称为监督矩阵，形状为$(r, n)$且可以初等变换化简为典型监督矩阵：</p><script type="math/tex; mode=display">\boldsymbol{H}=\left[ \boldsymbol{P}_{r\times k}\,\,\boldsymbol{I}_r \right]</script><p>生成监督位方程：</p><script type="math/tex; mode=display">\left[\begin{array}{l}d_{2} \\a_{1} \\a_{0}\end{array}\right]=\left[\begin{array}{l}1110 \\1101 \\1011\end{array}\right]\left[\begin{array}{l}a_{6} \\a_{5} \\a_{4} \\a_{3}\end{array}\right]</script><script type="math/tex; mode=display">\boldsymbol{G}=\left[ \boldsymbol{I}_k\,\,\boldsymbol{Q}_{k\times r} \right]</script><p>通过生成矩阵，我们将信息位码字转化为整个码字：</p><script type="math/tex; mode=display">\boldsymbol{A}=\left[ a_6\,\,a_5\,\,a_4\,\,a_3 \right] \cdot \boldsymbol{G}</script><p>并且$\boldsymbol{Q}_{k\times r}$和$\boldsymbol{P}_{r\times k}$互为转置。</p><h3 id="汉明码"><a href="#汉明码" class="headerlink" title="汉明码"></a>汉明码</h3><p>对线性分组码当$n=2^r-1$时就是汉明码。<br>汉明码是能纠1位错码的高效线性分组码。</p><h3 id="校正子和译码"><a href="#校正子和译码" class="headerlink" title="校正子和译码"></a>校正子和译码</h3><p>设接收码元组为$\boldsymbol{B}$，定义校正子$\boldsymbol{S}$：</p><script type="math/tex; mode=display">\boldsymbol{B}\cdot \boldsymbol{H}^T=\boldsymbol{S}</script><p>若$\boldsymbol{S}=0$则无错或检测不出错误。<br>设错误图样为$\boldsymbol{E}$，则</p><script type="math/tex; mode=display">\boldsymbol{A}=\boldsymbol{B}-\boldsymbol{E}=\boldsymbol{B}+\boldsymbol{E}</script><p>因此易知：</p><script type="math/tex; mode=display">\boldsymbol{S}=\boldsymbol{E}\cdot \boldsymbol{H}^T</script><p>根据上述三个式子便可根据接收码组$\boldsymbol{B}$和监督矩阵$\boldsymbol{H}$中纠错得到发送码组$\boldsymbol{A}$</p><p>线性分组码性质：</p><ul><li>具有封闭性，即任意两个许用码组之和(逐位模2加)仍为一需用码组</li><li>它的最小码距$d_0$等于非全零码组的最小重量<h2 id="循环码"><a href="#循环码" class="headerlink" title="循环码"></a>循环码</h2>循环码属于线性分组码，循环码还具有循环性，循环码中任一码组经过循环移位后仍为一个许用码组。</li></ul><h3 id="码多项式-A-x"><a href="#码多项式-A-x" class="headerlink" title="码多项式$A(x)$"></a>码多项式$A(x)$</h3><p>例如码组(1100101)可表示为：</p><script type="math/tex; mode=display">A(x)=x^6+x^5+x^2+1</script><h3 id="生成多项式-g-x"><a href="#生成多项式-g-x" class="headerlink" title="生成多项式$g(x)$"></a>生成多项式$g(x)$</h3><blockquote><p>存在性：</p><ul><li>$(n,k)$循环码中，有且仅有一个次数为$(n-k)$的多项式：<br>$<br>g(x)=1 \cdot x^{n-k}+a_{n-k-1} x^{n-k-1}+\cdots+a_{1} x+1$<br>$g(x)$为生成多项式，$g(x)$决定了循环码的纠错能力。</li></ul><p>性质：</p><ul><li>$g(x)$是$x^n+1$的一个因式</li><li>所有码多项式$A(x)$都可以被$g(x)$整除，而且任意一个次数不大于$(k-1)$的多项式乘$g(x)$都是码多项式<h3 id="生成矩阵-G"><a href="#生成矩阵-G" class="headerlink" title="生成矩阵$G$"></a>生成矩阵$G$</h3><script type="math/tex; mode=display">\boldsymbol{G}(x)=\left[\begin{array}{c}x^{k-1} g(x) \\x^{k}-2^{2} g(x) \\\vdots \\x g(x) \\g(x)\end{array}\right]</script><h3 id="循环码的编码"><a href="#循环码的编码" class="headerlink" title="循环码的编码"></a>循环码的编码</h3><script type="math/tex; mode=display">A(x)=x^{n-k} m(x)+r(x)</script>$r(x)$作为余式，代表监督码元。$m(x)$为信息码多项式，$x^{n-k}$目的是预留给监督位位置。以$g(x)=x^4+x^2+x+1$为例：<br><img src="https://i.loli.net/2021/08/10/v6aHWuZFjNmL1rC.png" alt="编码电路"><h3 id="循环码的译码"><a href="#循环码的译码" class="headerlink" title="循环码的译码"></a>循环码的译码</h3></li><li>检错：<br>对接收码组$B(x)$进行：<script type="math/tex; mode=display">B(x)/g(x)</script>若能除尽，则表示无错，若除不尽则发生错误</li><li>纠错：<ul><li>用接收码组$B(x)$除以生成多项式$g(x)$得到余式，也就是校正子多项式$S(x)$</li><li>由$S(x)$查表或通过计算得到错误图样$E(x)$，确定错码位置</li><li>$A(x)=B(x)-E(x)$<h2 id="卷积码-非分组码"><a href="#卷积码-非分组码" class="headerlink" title="卷积码(非分组码)"></a>卷积码(非分组码)</h2>分组码是把$k$个信息比特的序列编成$n$个比特的码组，每个码组的$n-k$个校验位与本码组的$k$个信息位有关，而与其他码组无关。为了达到一定的纠错能力和编码效率，分组码的长度一般都比较大。编译码时必须把整个信息码组存储起来，由此产生的译码延时随n的增加而增加。<br><img src="https://i.loli.net/2021/08/10/Uu5l2K6tvGeBIdo.png" alt="卷积码结构图"></li></ul></li></ul></blockquote><p>卷积码是一个有限记忆系统，如上图所示，它也将信息序列分割成长度$k$的一个个分组，然后将$k$个信息比特编成$n$个比特，但$k$和$n$通常很小，特别适合以串行形式进行传输，时延小。与分组码不同的是在某一分组编码时，不仅参看本时刻的分组而且参看以前的$N-1$个分组，编码过程中互相关联的码元个数为$nN$。$N$称为约束长度。常把卷积码写成$(n，k，N-1)$卷积码。正因为卷积码在编码过程中，充分利用了各级之间的相关性，无论是从理论上还是实际上均已证明其性能要优于分组码。卷积编码过程如下图：</p><p><img src="https://i.loli.net/2021/08/10/Uu5l2K6tvGeBIdo.png" alt="卷积编码过程"></p><p>卷积码的译码方式常采用基于似然法的维特比译码法：<br><img src="https://i.loli.net/2021/08/10/WjJQTky9d7IelnM.png" alt="维特比译码过程"><br>常采用软判决和硬判决，详情请看我的课设论文：<br><br></p><div class="row">    <embed src="https://yzuf49.coding-pages.com/file/thesis.pdf" width="100%" height="550" type="application/pdf"></div><p><br></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 差错控制编码 </tag>
            
            <tag> 线性码 </tag>
            
            <tag> 分组码 </tag>
            
            <tag> 卷积码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数字信号的最佳接收</title>
      <link href="/posts/d0f7ce70.html"/>
      <url>/posts/d0f7ce70.html</url>
      
        <content type="html"><![CDATA[<h1 id="数字信号的最佳接收"><a href="#数字信号的最佳接收" class="headerlink" title="数字信号的最佳接收"></a>数字信号的最佳接收</h1><h2 id="最佳接收准则"><a href="#最佳接收准则" class="headerlink" title="最佳接收准则"></a>最佳接收准则</h2><h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>接收机输入：</p><script type="math/tex; mode=display">r\left( t \right) \begin{cases}    s_0\left( t \right) +n\left( t \right)\\    s_1\left( t \right) +n\left( t \right)\\\end{cases}</script><p>对应似然函数：</p><script type="math/tex; mode=display">\begin{cases}    f_0\left( r \right) =\frac{1}{\left( \sqrt{2\pi}\sigma _n \right) ^k}\exp \left\{ \frac{-1}{n_0}\int_0^T{\left[ r\left( t \right) -s_0\left( t \right) \right] ^2dt} \right\}\\    f_1\left( r \right) =\frac{1}{\left( \sqrt{2\pi}\sigma _n \right) ^k}\exp \left\{ \frac{-1}{n_0}\int_0^T{\left[ r\left( t \right) -s_1\left( t \right) \right] ^2dt} \right\}\\\end{cases}</script><p>若$f_0(t) &gt; f_1(t)$则判为$s_0(t)$</p><h3 id="似然比准则"><a href="#似然比准则" class="headerlink" title="似然比准则"></a>似然比准则</h3><script type="math/tex; mode=display">\begin{cases}    \frac{f_0\left( r_0 \right)}{f_1\left( r_0 \right)}>\frac{p\left( 1 \right)}{p\left( 0 \right)}\,\,   \text{判“0”}\\    \frac{f_0\left( r_0 \right)}{f_1\left( r_0 \right)}<\frac{p\left( 1 \right)}{p\left( 0 \right)}\,\,   \text{判“1”}\\\end{cases}</script><h3 id="确知信号的最佳接收"><a href="#确知信号的最佳接收" class="headerlink" title="确知信号的最佳接收"></a>确知信号的最佳接收</h3><p><img src="https://i.loli.net/2021/08/10/47ukUPYjvJn3lQ9.png" alt="二进制最佳接收机原理方框图"></p><h3 id="实际接收机和最佳接收机的性能比较"><a href="#实际接收机和最佳接收机的性能比较" class="headerlink" title="实际接收机和最佳接收机的性能比较"></a>实际接收机和最佳接收机的性能比较</h3><p><img src="https://i.loli.net/2021/08/10/wjx6MfPe3d8ElGY.png" alt="实际接收机和最佳接收机的性能比较"></p><h3 id="匹配滤波器"><a href="#匹配滤波器" class="headerlink" title="匹配滤波器"></a>匹配滤波器</h3><p>定义：令输出信噪比最大的线性滤波器。<br>最小化输出信噪比：</p><script type="math/tex; mode=display">arg\min \frac{|s_{oy}\left( t_0 \right) |^2}{N_0}=\frac{|\int_{-\infty}^{\infty}{H\left( f \right) S\left( f \right) e^{j2\pi ft_0}df}|^2}{\frac{n_0}{2}\int_{-\infty}^{\infty}{|H\left( f \right) |^2df}}</script><p>满足：</p><script type="math/tex; mode=display">\begin{cases}    \text{接收信号：}r\left( t \right) =s\left( t \right) +n\left( t \right)\\    \text{输出信号：}y\left( t \right) =s_0\left( t \right) +n_0\left( t \right)\\    s_0\left( t \right) =\int_{-\infty}^{\infty}{H\left( f \right) S\left( f \right) e^{j2\pi ft}df}\\\end{cases}</script><p>由柯西不等式易知极值条件：</p><script type="math/tex; mode=display">H\left( f \right) =kS^*\left( f \right) e^{-j2\pi ft_0}</script><p>匹配滤波器(最佳接收滤波器传输特性)等于信号码元频谱的复共轭。转化为时域条件为：</p><script type="math/tex; mode=display">h(t)=ks(t_0 - t)</script><p>通常选择在码元末尾抽样，即选择$t_0=T_B$<br><img src="https://i.loli.net/2021/08/10/ycbeMHW7Cp8wzEr.png" alt="匹配滤波器接收框图"></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 最佳接收准则 </tag>
            
            <tag> 匹配滤波器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模拟信号的数字传输</title>
      <link href="/posts/d0f7ce70.html"/>
      <url>/posts/d0f7ce70.html</url>
      
        <content type="html"><![CDATA[<h1 id="模拟信号的数字传输"><a href="#模拟信号的数字传输" class="headerlink" title="模拟信号的数字传输"></a>模拟信号的数字传输</h1><p>模拟信号数字化传输的系统框图：<br><img src="https://i.loli.net/2021/08/09/a91onWrwlSHhLIF.png" alt="模拟信号数字化传输的系统"></p><ul><li>A/D转换：<ul><li>抽样：将取值和时间都连续的模拟信号变换为时间离散，取值仍连续的抽样信号（PAM），该信号仍为模拟信号</li><li>量化：将时间离散，取值连续的PAM信号变为时间和取值均离散的量化信号，该信号为多电平数字信号，称之为数字PAM信号</li><li>编码：将时间和取值均离散的量化信号变换为二进制数字信号(PCM)</li></ul></li><li>Ｄ/A转换：<ul><li>译码：将数字通信系统输出的PCM信号转化成量化信号</li><li>低通滤波：将量化信号恢复成模拟信号<h2 id="抽样定理"><a href="#抽样定理" class="headerlink" title="抽样定理"></a>抽样定理</h2>抽样定理为模拟信号数字化和时分复用奠定了理论基础。</li></ul></li><li>低通采样定理：频带限制在，$(0,f_H)$赫兹内的时间连续信号$m(t)$<br>，若以$f_s \ge 2f_H$的速率对 $m(t)$等间$T_s=1/f_s=1/2f_H$抽 样，则$m(t)$将被所得抽样函数完全确定。<ul><li>如果不满足上述条件，将产生混叠失真</li><li>典型话音信号的最高频率通常限制在3400Hz，其抽样频率为8000Hz</li></ul></li><li>带通采样定理：频带限制在$(f_L, f_H)$赫兹内的带通时间连续信号$m(t)$，带宽为$B=f_H - f_L $ ，则此带通信号所需的最小采样频率为:<script type="math/tex; mode=display">f_s = 2B(1+\frac{k}{n})</script>其中$n$和$k$分别是$f_H /B$的整数和小数部分</li></ul><p>对于窄带信号而言，最小采样频率为：</p><script type="math/tex; mode=display">f_s\approx 2B</script><h2 id="脉冲振幅调制-PAM"><a href="#脉冲振幅调制-PAM" class="headerlink" title="脉冲振幅调制(PAM)"></a>脉冲振幅调制(PAM)</h2><p>模拟脉冲调制是把模拟基带信号$m(t)$“寄托”在周期性脉冲序列(载波)的某个参量(幅度、宽度和位置)上的过程,相应有脉冲幅度调制(PAM)、脉宽调制(PDM)和脉位调制(PPM)。</p><p>PAM是模拟信号数字化过程中的必经之路(中间步骤)，是脉冲编码调制(PCM)的基础。</p><h2 id="脉冲编码调制-PCM"><a href="#脉冲编码调制-PCM" class="headerlink" title="脉冲编码调制(PCM)"></a>脉冲编码调制(PCM)</h2><p>PCM要在PAM的基础上经过量化和编码：<br><img src="https://i.loli.net/2021/08/09/NHivRPbLGDtszYJ.png" alt="PCM系统原理图"></p><h3 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h3><p>量化分为<strong>均匀量化</strong>和<strong>非均匀量化</strong></p><ul><li>均匀量化缺点：均匀量化噪声功率与信号样值大小无关，仅与量化间隔有关，从而小信号均匀信号量噪比小</li><li>非均匀量化：<ul><li>特点：量化间隔随信号抽样值的增大而增大，随信号抽样值的减小而减小。</li><li>目的：提高小信号的信号量噪比，扩大输入信号的动态范围。</li><li>实现：先将信号抽样值压缩，再均匀量化<br>压缩标准：</li></ul></li><li>A律压缩</li><li>$\mu$律压缩<br><img src="https://i.loli.net/2021/08/10/E8hUvmrTeSWf3xo.png" alt="A律压缩13折线"></li></ul><h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><p>PCM编码常用二进制码：</p><ul><li>自然二进制</li><li>折叠二进制</li><li>格雷二进制<br>常采用折叠二进制<br><img src="https://i.loli.net/2021/08/10/tDQpYjOxNZPGB9R.png" alt="13折线折叠二进制码规则"></li><li>极性码：正数“1”，负数“0”</li><li>段落码：<img src="https://i.loli.net/2021/08/10/DWcy9f4mb2xp1GM.png" alt="段落码编码"></li><li>段内码：<img src="https://i.loli.net/2021/08/10/qMo4imFCXacywWp.png" alt="段内码编码"><h3 id="译码即为编码逆过程"><a href="#译码即为编码逆过程" class="headerlink" title="译码即为编码逆过程"></a>译码即为编码逆过程</h3>译码即为编码逆过程</li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 抽样定理 </tag>
            
            <tag> 脉冲编码调制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新型数字通带调制技术</title>
      <link href="/posts/7a319ab8.html"/>
      <url>/posts/7a319ab8.html</url>
      
        <content type="html"><![CDATA[<h1 id="新型数字通带调制技术"><a href="#新型数字通带调制技术" class="headerlink" title="新型数字通带调制技术"></a>新型数字通带调制技术</h1><h2 id="正交振幅调制QAM"><a href="#正交振幅调制QAM" class="headerlink" title="正交振幅调制QAM"></a>正交振幅调制QAM</h2><p>QAM是一种多进制振幅和相位联合键控体制。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}e_k\left( t \right) &=A_k \cos \left( w_ct+\theta _k \right) ,  \\&=X_k\cos w_ct+Y_k\sin w_ct\end{split}\nonumber\end{equation}</script><p>其中$kT_B&lt;t\le \left( k+1 \right) T_B $ ，<br>常见的16QAM有两种：<br><img src="https://i.loli.net/2021/08/08/YubGLj9kJBNgrKh.png" alt="16QAM星座图"></p><h3 id="调制"><a href="#调制" class="headerlink" title="调制"></a>调制</h3><p><img src="https://i.loli.net/2021/08/08/wCHhj4ApomGk3sq.png" alt="16QAM调制"></p><h3 id="解调"><a href="#解调" class="headerlink" title="解调"></a>解调</h3><p><img src="https://i.loli.net/2021/08/08/ZRwfmbsyLdMnYXC.png" alt="16QAM解调"></p><h2 id="最小频移键控MSK"><a href="#最小频移键控MSK" class="headerlink" title="最小频移键控MSK"></a>最小频移键控MSK</h2><p>MSK是对FSK的改进，它是一种包络恒定相位稳定，调制指数$h$最小(0.5)的正交FSK。</p><script type="math/tex; mode=display">\begin{equation}\begin{split}e_{\mathrm{MSK}}\left( t \right) &=\cos \left[ w_ct+\theta _k\left( t \right) \right] \\&=\cos \left[ w_ct+\frac{a_k\pi}{2T_B}t+\varphi _k \right] \end{split}\nonumber\end{equation}</script><p>其中$ kT_B\le t\le \left( k+1 \right) T_B$,    $w_c$为中心角频率，$\frac{a_k\pi}{2T_B}$为相对$w_c$的频偏，$a_k$取值-1和+1，<br>因此频率间隔为：</p><script type="math/tex; mode=display">\Delta f=\frac{1}{2T_B}=\frac{R_B}{2}</script><p>调制：正交调制法：<br><img src="https://i.loli.net/2021/08/09/dIrf9LXaO1Nc5xb.png" alt="正交调制法"></p><h2 id="高斯最小频移键控"><a href="#高斯最小频移键控" class="headerlink" title="高斯最小频移键控"></a>高斯最小频移键控</h2><p>GMSK方法是在MSK调制器之前用一个高斯型低通滤波器对输入基带矩形信号脉冲进行处理，目的是为了获得比MSK更好的频谱特性：功率谱密度更加集中，旁瓣进一步降低。</p><h2 id="OFDM正交频分复用"><a href="#OFDM正交频分复用" class="headerlink" title="OFDM正交频分复用"></a>OFDM正交频分复用</h2><ul><li>单载波调制:是将需要传输的数据流调制到单个载波上进行传送，前面介绍的各种数字调制方式都属于单载波体制。这种体制在高速数据传输的宽带业务中，易因信道特性的不理想而造成信号失真和码间串扰(ISD)。为了解决这些问题，除了采用均衡器外，途径之一就是 采用多载波传输技术。</li><li>多载波调制:将信道分成许多(N个)子信道,把高速数据信号串/并变换为N路速率较低的子数据流，分别调制到各子载波上进行并行传输。由于每个子信道的码元速率将降低为原来的1/N,使每路数据信号的码元持续时间增长为原来的N倍,大于信道的最大多径时延(或者说，使每个子信道上的信号带宽减小为原来的1/N,远小于信道的相关带宽) ,所以各个子信道特性接近理想信道特性，从而可以有效地克服码间干扰和多径效应的影响。<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3>OFDM的基本原理是将发送的数据流分散到多个子载波上，使各子载波的信号速率大为降低，从而提高抗多径和抗衰落的能力。为了提高频谱利用率,OFDM方式中各路子载波的已调信号频谱有1/2重叠，但保持相互正交。<script type="math/tex; mode=display">e\left( t \right) =\sum_{k=0}^{N-1}{x_k\left( t \right)}=\sum_{k=0}^{N-1}{B_k\cos \left( 2\pi f_kt+\varphi _k \right)}=\sum_{k=0}^{N-1}{\boldsymbol{B}_ke^{j\left( 2\pi f_kt+\varphi _k \right)}}</script>为了使$N$路子信道信号在接收时能够完全分离，要求它们满足正交条件：<script type="math/tex; mode=display">\int_0^{T_{\mathrm{B}}}{\cos}\left( 2\pi f_kt+\varphi _k \right) \cos \left( 2\pi f_it+\varphi _i \right) \mathrm{d}t=0</script>等价满足：<script type="math/tex; mode=display">\begin{cases}  \left( f_k+f_i \right) T_B=m\in \mathbb{Z} ^+\\  \left( f_k-f_i \right) T_B=n\in \mathbb{Z}\\\end{cases}</script>也就是：<script type="math/tex; mode=display">\begin{cases}  f_k=\frac{m}{2T_B}\left( m\in \mathbb{Z} ^+ \right)\\  \Delta f=f_k-f_i=\frac{n}{T_B}\left( n\in \mathbb{Z} \right)\\\end{cases}</script>可以得出：<script type="math/tex; mode=display">\Delta f_{\min}=\frac{1}{T_B}</script><img src="https://i.loli.net/2021/08/09/O3hmWcg7THEU1Ml.png" alt="OFDM调制原理"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> QAM </tag>
            
            <tag> FSK </tag>
            
            <tag> OFDM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数字带通传输系统</title>
      <link href="/posts/d0f7ce70.html"/>
      <url>/posts/d0f7ce70.html</url>
      
        <content type="html"><![CDATA[<h1 id="数字带通传输系统"><a href="#数字带通传输系统" class="headerlink" title="数字带通传输系统"></a>数字带通传输系统</h1><h2 id="二进制数字调制"><a href="#二进制数字调制" class="headerlink" title="二进制数字调制"></a>二进制数字调制</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>调制方法：</p><ul><li>模拟调制法：利用模拟调制的方法去实现数字式调制，</li><li>数字键控法：利用数字信号的离散取值特点，通过开关键控载波，从而实现数字调制。<br>二(多)进制调制：指载波的幅度、频率或相位受调制后有两（M)种状态。<h2 id="二进制振幅键控-2ASK"><a href="#二进制振幅键控-2ASK" class="headerlink" title="二进制振幅键控(2ASK)"></a>二进制振幅键控(2ASK)</h2>时域信号表达式：<script type="math/tex; mode=display">ASK:\begin{cases}  e_{2\mathrm{ASK}}\left( t \right) =s\left( t \right) \cos w_ct\\  s\left( t \right) =\sum_n{a_ng\left( t-nT_B \right)}\\  a_n=\begin{cases}  +1  \quad \quad  \text{概率为}P\\  0   \quad \quad    \text{概率为}1-P\\\end{cases}\\\end{cases}</script>解调方法：</li><li>非相干解调(包络检波法）)</li><li>相干解调(同步检测法)<br><img src="https://i.loli.net/2021/08/07/3QR6FXm7VaJTyLY.png" alt="2ASK解调"></li></ul><h2 id="二进制频率键控-2FSK"><a href="#二进制频率键控-2FSK" class="headerlink" title="二进制频率键控(2FSK)"></a>二进制频率键控(2FSK)</h2><script type="math/tex; mode=display">FSK\text{：}\begin{cases}    e_{2\mathrm{FSK}}\left( t \right) =\begin{cases}    A\cos w_1t \quad \quad  \text{发送}“1”\\    A\cos w_2t \quad \quad  \text{发送}“0”\\\end{cases}\\    \,\,         =s_1\left( t \right) \cos w_1t+s_2\left( t \right) \cos w_2t\,\,\\    \begin{cases}    s_1\left( t \right) =\sum_n{a_ng\left( t-nT_B \right)}\\    s_2\left( t \right) =\sum_n{\bar{a}_ng\left( t-nT_B \right)}\\\end{cases}\\    a_n=\begin{cases}    +1  \quad \quad \text{概率为}P\\    0    \quad \quad  \text{概率为}1-P\\\end{cases}\\\end{cases}</script><p><img src="https://i.loli.net/2021/08/08/1UgjfLhqYOlcbdW.png" alt="2FSK相干解调和非相干解调"></p><h2 id="二进制相位键控-2PSK-2DPSK"><a href="#二进制相位键控-2PSK-2DPSK" class="headerlink" title="二进制相位键控(2PSK,2DPSK)"></a>二进制相位键控(2PSK,2DPSK)</h2><script type="math/tex; mode=display">PSK\text{：}\begin{cases}    e_{2\mathrm{PSK}}\left( t \right) =\begin{cases}    A\cos w_ct \quad \quad  \text{发送}“1”\\    -A\cos w_ct \quad \quad  \text{发送}“-1”\\\end{cases}\\    \,\,         =s\left( t \right) \cos w_ct\\    s\left( t \right) =\sum_n{a_ng\left( t-nT_B \right)}\\    a_n=\begin{cases}    +1  \quad \quad  \text{概率为}P\\    -1  \quad \quad     \text{概率为}1-P\\\end{cases}\\\end{cases}</script><p>其中$s(t)$为双极性非归零码。<br>相干解调：<br><img src="https://i.loli.net/2021/08/08/UgR97WGJDm8YLnB.png" alt="2PSK"></p><p>缺点：“倒$\pi$现象。克服方法：2DPSK<br><img src="https://i.loli.net/2021/08/08/csN1DHhTI8nrxmi.png" alt="2DPSK"><br><img src="https://i.loli.net/2021/08/08/NbPZkfuriw4FYXy.png" alt="2DPSK调制"></p><script type="math/tex; mode=display">\begin{cases}    a_n=b_n\oplus b_{n-1}\\    b_n=a_n\oplus b_{n-1}\\\end{cases}</script><p>解调：</p><ul><li>相干解调(码反变换)<br><img src="https://i.loli.net/2021/08/08/USZ8ETz51PJvWGX.png" alt="码反变换"></li><li>差分相干解调(相位比较法)<br><img src="https://i.loli.net/2021/08/08/DbJxuUzgekO8ZoY.png" alt="相位比较法"></li></ul><p><img src="https://i.loli.net/2021/08/07/w6HzyY3KeWbukTr.png" alt="二进制频带传输系统性能比较"><br><img src="https://i.loli.net/2021/08/07/4CfSjAKwxGPNR1a.png" alt="二进制频带传输系统性能比较"></p><h2 id="多进制数字调制"><a href="#多进制数字调制" class="headerlink" title="多进制数字调制"></a>多进制数字调制</h2><p>多进制调制的目的：为了提高频带利用率，使一个码元携带多个信息<br><img src="https://i.loli.net/2021/08/07/AtXUKHaFuL4SGo2.png" alt="多进制数字调制"></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 数字键控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数字基带传输系统</title>
      <link href="/posts/df7aca25.html"/>
      <url>/posts/df7aca25.html</url>
      
        <content type="html"><![CDATA[<h1 id="数字基带传输系统"><a href="#数字基带传输系统" class="headerlink" title="数字基带传输系统"></a>数字基带传输系统</h1><h2 id="数字基带信号及其频谱特性"><a href="#数字基带信号及其频谱特性" class="headerlink" title="数字基带信号及其频谱特性"></a>数字基带信号及其频谱特性</h2><p>设计归零信号的意义：由于其相邻脉冲之间存在零电位的间隔，使得接收端很容易识别出每个码元的起止时刻。</p><p>设置双极性的意义：提高抗干扰能力。</p><p>差分波形传输信息可以消除设备初始状态的影响。</p><h3 id="基带信号的频谱特性"><a href="#基带信号的频谱特性" class="headerlink" title="基带信号的频谱特性"></a>基带信号的频谱特性</h3><ul><li>数字基带信号是一个随机脉冲序列，没有确定的频谱函数，所以只能用功率谱来描述它的频谱特性。<br>下面推导数字基带信号的功率谱密度：<br>设数字基带信号为$s(t)$<script type="math/tex; mode=display">s\left( t \right) =\sum_{n=-\infty}^{\infty}{s_n\left( t \right)}</script>其中：<script type="math/tex; mode=display">s_n\left( t \right) =\begin{cases}  g_1\left( t-nT_B \right) \,\, \text{以概率}P\text{出现}\\  g_2\left( t-nT_B \right) \,\, \text{以概率}\left( 1-P \right) \text{出现}\\\end{cases}</script>将$s(t)$分为稳态波$v(t)$和交态波$u(t)$</li></ul><script type="math/tex; mode=display">v(t)=\sum_{n=-\infty}^{\infty}{\left[ Pg_1\left( t-nT_{\mathrm{B}} \right) +(1-P)g_2\left( t-nT_{\mathrm{B}} \right) \right]}=\sum_{n=-\infty}^{\infty}{v_n}(t)</script><h4 id="P-v-f-的计算"><a href="#P-v-f-的计算" class="headerlink" title="$P_{v}(f)$的计算"></a>$P_{v}(f)$的计算</h4><p>可知$v(t)$以$T_B$为周期的周期函数，而交变波为：</p><script type="math/tex; mode=display">u(t)=s(t)-v(t)</script><p>同时可以写为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}u_n\left( t \right)& =s_n\left( t \right) -v_n\left( t \right) \\&=a_n\left[ g_1\left( t-nT_B \right) -g_2\left( t-nT_B \right) \right] \end{split}\nonumber\end{equation}</script><p>其中</p><script type="math/tex; mode=display">a_n=\begin{cases}    1-P\,\, \text{以概率}P\\    -P\,\,    \text{以概率}\left( 1-P \right)\\\end{cases}</script><script type="math/tex; mode=display">v\left( t \right) =\sum_{m=-\infty}^{\infty}{C_me^{j2\pi mf_Bt}}\,\,      \text{其中}                \left( f_B=\frac{1}{T_B} \right)</script><p>其中傅里叶系数$C_m$</p><script type="math/tex; mode=display">C_m=f_B\left[ PG_1\left( mf_B+\left( 1-P \right) G_2\left( mf_B \right) \right) \right]</script><p>由此可知功率谱密度为：</p><script type="math/tex; mode=display">P_{v}(f)=\sum_{m=-\infty}^{\infty}\left|f_{\mathrm{B}}\left[P G_{1}\left(m f_{\mathrm{B}}\right)+(1-P) G_{2}\left(m f_{\mathrm{B}}\right)\right]\right|^{2} \delta\left(f-m f_{\mathrm{B}}\right)</script><p>上式表明稳态波$v(T)$的功率谱$P_{v}(f)$是冲击强度取决于$|C_m|^2$的离散线谱。</p><h4 id="P-u-f-的计算"><a href="#P-u-f-的计算" class="headerlink" title="$P_{u}(f)$的计算"></a>$P_{u}(f)$的计算</h4><p>通过式：</p><script type="math/tex; mode=display">\begin{cases}    P_u\left( f \right) =\lim_{T\rightarrow \infty} \frac{\mathbb{E} \left[ |U_T\left( f \right) |^2 \right]}{T}\\    T=\left( 2N+1 \right) T_B\\    U_T\left( f \right) =\mathscr{F} \left[ u_T\left( t \right) \right]\\    u_T\left( t \right) =\sum_{n=-N}^N{a_n}\left[ g_1\left( t-nT_{\mathrm{B}} \right) -g_2\left( t-nT_{\mathrm{B}} \right) \right]\\\end{cases}</script><p>求得：</p><script type="math/tex; mode=display">P_u\left( f \right) =f_BP\left( 1-P \right) |G_1\left( f \right) -G_2\left( f \right) |^2</script><p>又由$s(t)=v(t)+u(t)$可得：</p><script type="math/tex; mode=display">\begin{aligned}P_{\mathrm{s}}(f)=& P_{u}(f)+P_{v}(f)=f_{\mathrm{B}} P(1-P)\left|G_{1}(f)-G_{2}(f)\right|^{2}+\\& \sum_{m=-\infty}^{\infty}\left|f_{\mathrm{B}}\left[P G_{1}\left(m f_{\mathrm{B}}\right)+(1-P) G_{2}\left(m f_{\mathrm{B}}\right)\right]\right|^{2} \delta\left(f-m f_{\mathrm{B}}\right)\end{aligned}</script><p>写成单边：</p><script type="math/tex; mode=display">\begin{aligned}P_{\mathrm{s}}(f)=& 2 f_{\mathrm{B}} P(1-P)\left|G_{1}(f)-G_{2}(f)\right|^{2}+f_{\mathrm{B}}^{2}\left|P G_{1}(0)+(1-P) G_{2}(0)\right|^{2} \delta(f)+\\& 2 f_{\mathrm{B}}^{2} \sum_{m=1}^{\infty}\left|P G_{1}\left(m f_{\mathrm{B}}\right)+(1-P) G_{2}\left(m f_{\mathrm{B}}\right)\right|^{2} \delta\left(f-m f_{\mathrm{B}}\right) \quad f \geqslant 0\end{aligned}</script><p>由此可以得出结论：</p><ul><li>$P_{\mathrm{s}}(f)$可能包含离散谱和连续谱</li><li>连续谱一定存在，离散谱不一定存在</li><li>对于双极性信号，-1和+1波形出现概率相等时没有离散谱<h3 id="常用码型"><a href="#常用码型" class="headerlink" title="常用码型"></a>常用码型</h3><h4 id="AMI码"><a href="#AMI码" class="headerlink" title="AMI码"></a>AMI码</h4><img src="https://i.loli.net/2021/08/07/9ZkXcI6fgB7HoWU.png" alt="AMI"></li><li>优点：没有直流分量</li><li>缺点：提取定时信号困难<h4 id="HDB3"><a href="#HDB3" class="headerlink" title="HDB3"></a>HDB3</h4><img src="https://i.loli.net/2021/08/07/GtmF8oHhsRLPSIe.png" alt="HDB3"><h2 id="无码间干扰"><a href="#无码间干扰" class="headerlink" title="无码间干扰"></a>无码间干扰</h2>码间串扰产生的原因是由于系统总传输特性的不理想，使前面码元波形的拖尾蔓延到当前码元的抽样时刻上，从而对当前码元的判决造成干扰；</li></ul><h4 id="无码间干扰条件"><a href="#无码间干扰条件" class="headerlink" title="无码间干扰条件"></a>无码间干扰条件</h4><ul><li><p>时域：</p><script type="math/tex; mode=display">h\left( kT_s \right) =\begin{cases}  C\,\,  ,k=0\\  0   , k\ne 0\\\end{cases}</script></li><li><p>频域： </p><script type="math/tex; mode=display">H_{eq}\left( w \right) =\sum_i{H\left( w+i\frac{2\pi}{T_s} \right) =C,  |w|\le \frac{\pi}{T_s}}</script><h2 id="部分响应系统"><a href="#部分响应系统" class="headerlink" title="部分响应系统"></a>部分响应系统</h2><p><img src="https://i.loli.net/2021/08/07/gquBIjy76Y2TLQ5.png" alt="部分响应系统"></p></li></ul><p><img src="https://i.loli.net/2021/08/07/dFcEuqQPORaB6Uo.png" alt="部分响应系统"><br>当输入数据为Ｌ进制时，第Ｉ和第ＩＶ相关编码电平数为２Ｌ－１，因此，抗噪声性能变差，但是以上两类的抽样值电平数比其它类别的少，这使得其判决时噪声容限较大，误码率较低。</p><h2 id="眼图及其模型"><a href="#眼图及其模型" class="headerlink" title="眼图及其模型"></a>眼图及其模型</h2><p><img src="https://i.loli.net/2021/08/07/XZYQjH1MqpIG894.png" alt="眼图模型"></p><h2 id="均衡技术"><a href="#均衡技术" class="headerlink" title="均衡技术"></a>均衡技术</h2><p>在基带系统中插入一种可调（或不可调）滤波器可以校正或补偿系统特性，减小码间串扰的影响，这种起补偿作用的滤波器成为均衡器。</p><ul><li>频域均衡：从校正系统的频域出发，使包括均衡器在内的基带系统的总特性满足无失真传输条件</li><li>时域均衡：是利用均衡器产生的时间波形取直接矫正已畸变的波形，使包括均衡器在内的整个系统的冲击响应满足无码间串扰条件。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 数字基带传输 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模拟通信系统</title>
      <link href="/posts/c6ba78c6.html"/>
      <url>/posts/c6ba78c6.html</url>
      
        <content type="html"><![CDATA[<h1 id="模拟通信系统"><a href="#模拟通信系统" class="headerlink" title="模拟通信系统"></a>模拟通信系统</h1><hr><h2 id="调制定义"><a href="#调制定义" class="headerlink" title="调制定义"></a>调制定义</h2><p>用调制信号去控制载波参数，<br>把信号转换成适合在信道中<br>传输的形式的过程。</p><h2 id="调制目的"><a href="#调制目的" class="headerlink" title="调制目的"></a>调制目的</h2><ol><li>把调制信号转换成适合在信道中传输的已调信号。</li><li>实现信道的多路复用，提高信道利用率。</li><li>改善系统抗噪声性能。<h2 id="线性调制"><a href="#线性调制" class="headerlink" title="线性调制"></a>线性调制</h2>定义：调信号频谱只是调制信号频谱的简单搬移，频谱形状保持不变。（如ＡＭ、ＤＳＢ、ＳＳＢ、ＶＳＢ、ＭＡＳＫ、ＭＰＳＫ、ＭＦＳＫ、ＱＡＭ、ＭＳＫ、ＧＭＳＫ等）<h3 id="滤波法"><a href="#滤波法" class="headerlink" title="滤波法"></a>滤波法</h3><img src="https://i.loli.net/2021/08/07/2CfFI9Z1G7LWtqT.png" alt="滤波法"></li></ol><p>适当选择$h(t)$便可得到各种幅度调制信号，如$H(w)=1$即为全通网络。</p><h3 id="移相法"><a href="#移相法" class="headerlink" title="移相法"></a>移相法</h3><p><img src="https://i.loli.net/2021/08/07/A8ayQ6RjktPs1qD.png" alt="移相法"></p><script type="math/tex; mode=display">\begin{equation}\begin{split}s_{m}(t)&=s_{I}(t) \cos \omega_{c} t+s_{Q}(t) \sin \omega_{c} t \\s_{I}(t)&=h_{I}(t) * m(t)  \\s_{Q}(t)&= h_{Q}(t) * m(t) \\h_{I}(t) &=h(t) \cos \omega_{c} t \\h_{Q}(t) &=h(t) \sin \omega_{c} t\end{split}\nonumber\end{equation}</script><p>$I$表示同向分量，$Q$表示正交分量</p><h2 id="非线性调制"><a href="#非线性调制" class="headerlink" title="非线性调制"></a>非线性调制</h2><p>已调信号频谱不再是只是调制信号频谱的简单搬移，而是频谱的非线性变换(如FM、PM)</p><h2 id="AM系统"><a href="#AM系统" class="headerlink" title="ＡＭ系统"></a>ＡＭ系统</h2><p>ＡＭ系统的调制解调原理及抗噪声性能：<br><img src="https://i.loli.net/2021/08/07/XuAGjZxW2MkBRyp.png" alt="ＡＭ系统"></p><p><img src="https://i.loli.net/2021/08/07/8KHSmAMLrfGaDFd.png" alt="ＡＭ系统"></p><p><img src="https://i.loli.net/2021/08/07/hwfN7xuijEQ35cC.png" alt="ＡＭ系统"></p><h3 id="相干解调"><a href="#相干解调" class="headerlink" title="相干解调"></a>相干解调</h3><p><img src="https://i.loli.net/2021/08/07/s92MloBVaOfA1DC.png" alt="ＡＭ系统"></p><p><img src="https://i.loli.net/2021/08/07/s2tYBqAIHbjrWek.png" alt="ＡＭ系统"></p><p><img src="https://i.loli.net/2021/08/07/MBQm1lzPoH5Zv2n.png" alt="ＡＭ系统"></p><h3 id="包络解调"><a href="#包络解调" class="headerlink" title="包络解调"></a>包络解调</h3><p><img src="https://i.loli.net/2021/08/07/uTbxifpR81hBnwc.png" alt="包络检波"></p><p><img src="https://i.loli.net/2021/08/07/Wti5BxzjcTQK69J.png" alt="包络检波"></p><h3 id="DSB和SSB系统抗噪声性能比较"><a href="#DSB和SSB系统抗噪声性能比较" class="headerlink" title="DSB和SSB系统抗噪声性能比较"></a>DSB和SSB系统抗噪声性能比较</h3><p><img src="https://i.loli.net/2021/08/07/4CcYaB2lG7bpAUW.png" alt=""></p><h2 id="FM系统"><a href="#FM系统" class="headerlink" title="ＦＭ系统"></a>ＦＭ系统</h2><p><img src="https://i.loli.net/2021/08/07/cn3QNkAiYUCuqSL.png" alt="ＦＭ系统"></p><h3 id="预加重与去加重"><a href="#预加重与去加重" class="headerlink" title="预加重与去加重"></a>预加重与去加重</h3><p>由宽带调频系统鉴频器输出噪声谱密度呈抛物线形状可知，解调器输出噪声随调制信号频率升高而增强，而调制信号的幅度随频率升高而减小，因此，解调器恢复的信号在高频部分信噪比变差。为提高恢复信号在高频部分的信噪比，常采用“预加重”和“去加重”技术，在保持恢复信号高频部分功率不变的条件下，减小噪声的平均功率，从而改善高频部分的输出信噪比。<br><img src="https://i.loli.net/2021/08/07/cpAJ5atwMYBnsXV.png" alt="系统框图"></p><h3 id="模拟调制系统性能比较"><a href="#模拟调制系统性能比较" class="headerlink" title="模拟调制系统性能比较"></a>模拟调制系统性能比较</h3><p><img src="https://i.loli.net/2021/08/07/R1SvztKQbHkTsh2.png" alt="模拟调制系统性能比较"></p><h3 id="FDM及其带宽的计算"><a href="#FDM及其带宽的计算" class="headerlink" title="FDM及其带宽的计算"></a>FDM及其带宽的计算</h3><ul><li>优点：是信道复用率高，容许复用的路数多，分路也很方便。因此，它成为目前模拟通信中最主要的一种复用方式。</li><li>缺点：设备生产比较复杂，会因滤波器件特性不够理想和信道内存在非线性而产生路间干扰。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 模拟调制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信道</title>
      <link href="/posts/b6fa9949.html"/>
      <url>/posts/b6fa9949.html</url>
      
        <content type="html"><![CDATA[<h1 id="信道"><a href="#信道" class="headerlink" title="信道"></a>信道</h1><h2 id="恒参信道"><a href="#恒参信道" class="headerlink" title="恒参信道"></a>恒参信道</h2><p><img src="https://i.loli.net/2021/08/07/xQH8q6fiNO5Glr3.png" alt="恒参信道"></p><h2 id="随参信道"><a href="#随参信道" class="headerlink" title="随参信道"></a>随参信道</h2><p>随参信道又称为<strong>衰落信道</strong>，$k(t)$与$t$有关，随机快变信号的衰耗随时间随机变化；信号传输的时延随时间随机变化；多径传播。</p><p>对所传输信号的影响：</p><ul><li>瑞利衰落</li><li>频率弥散</li><li>频率选择性衰落<blockquote><p>慢衰落：加快发射功率，采用自动增益控制<br>快衰落：分级接收技术，扩频技术，OFDM</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 信道 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随机过程</title>
      <link href="/posts/9f4e6fbf.html"/>
      <url>/posts/9f4e6fbf.html</url>
      
        <content type="html"><![CDATA[<h1 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h1><h2 id="随机过程的数字特征"><a href="#随机过程的数字特征" class="headerlink" title="随机过程的数字特征"></a>随机过程的数字特征</h2><p><img src="https://i.loli.net/2021/08/06/VFGBtCpDxwUv8ME.png" alt="随机过程的数字特征"></p><h2 id="平稳随机过程"><a href="#平稳随机过程" class="headerlink" title="平稳随机过程"></a>平稳随机过程</h2><h3 id="严平稳和广义平稳随机过程"><a href="#严平稳和广义平稳随机过程" class="headerlink" title="严平稳和广义平稳随机过程"></a>严平稳和广义平稳随机过程</h3><p><img src="https://i.loli.net/2021/08/06/HmdqeMQaKhZU61b.png" alt="严平稳和广义平稳随机过程"></p><h3 id="各态历经性"><a href="#各态历经性" class="headerlink" title="各态历经性"></a>各态历经性</h3><p>任取平稳随机过程$\xi(t)$的任一样本函数$x(t)$，其时间均值和时间自相关满足:</p><script type="math/tex; mode=display">\begin{equation}\begin{gathered}\bar{a}=\overline{x(t)}=\lim _{T \rightarrow \infty} \frac{1}{T} \int_{-T / 2}^{T / 2} x(t) d t=a \\\overline{R(\tau)}=\overline{x(t) x(t+\tau)}=\lim _{T \rightarrow \infty} \frac{1}{T} \int_{-T / 2}^{T / 2} x(t) x(t+\tau) d t=R(\tau)\end{gathered}\end{equation}</script><p><strong>意义：可用任意一次实现的“样本平均”来取代随机过程的“统计平均”，可用任意一次实现的功率谱密度来取代随机过程的功率谱密度，简化测量和计算问题；具有各态历经性的随机过程一定是平稳随机过程，反之不一定成立</strong></p><h3 id="平稳随机过程自相关函数的性质"><a href="#平稳随机过程自相关函数的性质" class="headerlink" title="平稳随机过程自相关函数的性质"></a>平稳随机过程自相关函数的性质</h3><p><img src="https://i.loli.net/2021/08/07/zerMulRJoC2AXZ1.png" alt="平稳随机过程自相关函数的性质"></p><h3 id="维纳———辛钦定理"><a href="#维纳———辛钦定理" class="headerlink" title="维纳———辛钦定理"></a>维纳———辛钦定理</h3><p>平稳随机过程的自相关函数和功率谱密度互为傅里叶变换对。</p><h3 id="高斯随机过程"><a href="#高斯随机过程" class="headerlink" title="高斯随机过程"></a>高斯随机过程</h3><blockquote><p>定义：任意ｎ维概率密度都服从正态分布的随机过程。</p><p>重要性质：高斯过程若广义平稳，则必狭义平稳；高斯过程中的随机变量之间若不相关，则它们统计独立；若干个高斯过程之和仍是高斯过程；高斯过程经线性变换后，仍是高斯过程.</p></blockquote><p>均值为０，方差为σ２的平稳高斯窄带随机过程的定义及性质:<br><img src="https://i.loli.net/2021/08/07/9gQSEMidtB3RVoN.png" alt="平稳高斯窄带随机过程"></p><h3 id="高斯白噪声和带限白噪声"><a href="#高斯白噪声和带限白噪声" class="headerlink" title="高斯白噪声和带限白噪声"></a>高斯白噪声和带限白噪声</h3><p><img src="https://i.loli.net/2021/08/07/TUdgAip7IHJWtmz.png" alt="高斯白噪声"></p><p><img src="https://i.loli.net/2021/08/07/b6ymHoej1SUipRC.png" alt="带限白噪声"></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 随机过程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用傅里叶变换及频域性质</title>
      <link href="/posts/f43f2f79.html"/>
      <url>/posts/f43f2f79.html</url>
      
        <content type="html"><![CDATA[<h2 id="常用傅里叶变换对"><a href="#常用傅里叶变换对" class="headerlink" title="常用傅里叶变换对"></a>常用傅里叶变换对</h2><h3 id="门函数"><a href="#门函数" class="headerlink" title="门函数"></a>门函数</h3><p><img src="https://i.bmp.ovh/imgs/2021/08/d00dc50e5486952e.png" alt=""></p><h3 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h3><p><img src="https://i.bmp.ovh/imgs/2021/08/513278322edac61f.png" alt=""></p><h3 id="周期函数"><a href="#周期函数" class="headerlink" title="周期函数"></a>周期函数</h3><p><img src="https://files.mdnice.com/user/16955/ebac65ef-3567-4c90-9e9e-834e8c810c54.png" alt=""></p><p><img src="https://files.mdnice.com/user/16955/6659e9c5-94b5-496d-8eda-e8db9c55bb61.png" alt=""><br><img src="https://i.bmp.ovh/imgs/2021/08/af9d77e1b1ad31b7.png" alt=""><br><img src="https://i.bmp.ovh/imgs/2021/08/c8cac4c8c6d08019.png" alt=""><br><img src="https://i.bmp.ovh/imgs/2021/08/9a215b8792cc5b04.png" alt=""></p><h3 id="能量谱密度和功率谱密度"><a href="#能量谱密度和功率谱密度" class="headerlink" title="能量谱密度和功率谱密度"></a>能量谱密度和功率谱密度</h3><p>帕氏瓦尔能量谱和功率谱守恒定理：<br><img src="https://i.bmp.ovh/imgs/2021/08/361f2b9343ad45d4.png" alt=""><br>能量谱密度和功率谱密度：<br><img src="https://i.bmp.ovh/imgs/2021/08/179612757970073d.png" alt=""></p><h3 id="自相关函数和互相关函数"><a href="#自相关函数和互相关函数" class="headerlink" title="自相关函数和互相关函数"></a>自相关函数和互相关函数</h3><p>自相关函数和互相关函数：<br><img src="https://i.bmp.ovh/imgs/2021/08/e321bc1f9346274a.png" alt=""><br>$R(0)$总功率，$R(\infty)$交流功率</p><h3 id="相关函数与功率谱密度"><a href="#相关函数与功率谱密度" class="headerlink" title="相关函数与功率谱密度"></a>相关函数与功率谱密度</h3><p><img src="https://i.loli.net/2021/08/06/N6DXq2KIh7P4HZM.png" alt="相关函数与功率谱密度"></p><h3 id="双边谱与单边谱的关系"><a href="#双边谱与单边谱的关系" class="headerlink" title="双边谱与单边谱的关系"></a>双边谱与单边谱的关系</h3><p><img src="https://i.loli.net/2021/08/06/2AjMbhOwsN6YrKX.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 傅里叶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通信系统简介</title>
      <link href="/posts/aead685f.html"/>
      <url>/posts/aead685f.html</url>
      
        <content type="html"><![CDATA[<h1 id="通信系统的组成"><a href="#通信系统的组成" class="headerlink" title="通信系统的组成"></a>通信系统的组成</h1><ul><li><p>信源：把待传输的消息转换成原始电信号（基带信号），即完成非电到电的。</p></li><li><p>发送设备：将信源和信道匹配起来，即将信源产生的基带信号变换成适合在信道中传输的。</p></li><li><p>信道：信号传输的通道，同时对所传输的信号产生损耗、时延和干。</p></li><li><p>噪声源：是信道中的所有噪声以及分散在通信系统中其它各处噪声的集。</p></li><li><p>接收设备：从受到衰减和干扰的接收信号中正确恢复出原始电信号。</p></li><li><p>信宿：将复原的原始电信号转换成相应的消息</p><h2 id="模拟通信系统模型"><a href="#模拟通信系统模型" class="headerlink" title="模拟通信系统模型"></a>模拟通信系统模型</h2><p><img src="https://i.bmp.ovh/imgs/2021/08/edd5578d750ba16e.png" alt=""></p><h2 id="数字通信系统模型"><a href="#数字通信系统模型" class="headerlink" title="数字通信系统模型"></a>数字通信系统模型</h2><p><img src="https://i.bmp.ovh/imgs/2021/08/fa019f9b5b6b576d.png" alt=""></p><h3 id="各部分作用"><a href="#各部分作用" class="headerlink" title="各部分作用"></a>各部分作用</h3></li><li>信源编码与译码。源编码两个基本功能：<ul><li>完成模／数转换；</li><li>将数字信号进行压缩处理，提高信息传输的有效性。信源译码是信源编码的逆过程。</li></ul></li><li>信道编码与译码。信道编码对输入的代码加入监督位进行差错控制编码。信道译码发现或纠正接收码元中的错误，提高可靠性。</li><li>加密与解密。加密提高了所传信息的安全。解密恢复加密前的信息。</li><li>数字调制与解调。数字调制形成适合在信道中传输的带通信号。数字解调是数字调制的逆过程。</li><li>同步。使收发两端的信号在时间上保持步调一致。<h3 id="数字通信系统的优缺点"><a href="#数字通信系统的优缺点" class="headerlink" title="数字通信系统的优缺点"></a>数字通信系统的优缺点</h3>优点：</li><li>（１）抗干扰能力强，且噪声不积累</li><li>（２）传输差错可控</li><li>（３）便于处理、变换、存储，将来自不同信源的信号综合到一起传输</li><li>（４）易于集成，使通信设备小型化</li><li>（５）易于加密处理，且保密性好。<br>缺点：</li><li>（１）需要较大的传输带宽</li><li>（２）对同步要求高，系统设备复杂。<br>通信的任务是快速、准确地传递信息，从研究消息传输的角度来说，<strong>有效性和可靠性</strong>是评价通信<br>系统优劣的主要性能指标。</li><li>有效性：传输一定信息量时所占用的信道资源（带宽或时间间隔即时隙的大小）。</li><li>可靠性：接收信息的准确程度。<h4 id="模拟通信系统的主要性能指标"><a href="#模拟通信系统的主要性能指标" class="headerlink" title="模拟通信系统的主要性能指标"></a>模拟通信系统的主要性能指标</h4></li><li>有效性：传输带宽。传输同样的消息所需的带宽越小，有效性就越好。</li><li>可靠性：输出信噪比。<h4 id="数字通信系统的主要性能指标"><a href="#数字通信系统的主要性能指标" class="headerlink" title="数字通信系统的主要性能指标"></a>数字通信系统的主要性能指标</h4></li><li>有效性———传输速率和频带利用率(单位带宽内的传输速率)</li><li>可靠性———误码率和误信率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 通信系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows版本更新nodejs</title>
      <link href="/posts/8ae57060.html"/>
      <url>/posts/8ae57060.html</url>
      
        <content type="html"><![CDATA[<h2 id="Windows版本更新nodejs"><a href="#Windows版本更新nodejs" class="headerlink" title="Windows版本更新nodejs"></a>Windows版本更新nodejs</h2><p>Windows版本更新nodejs有坑，不能像mac系统直接使用<code>n latest</code>即可完成最新版更新，需要手动进行添加</p><h3 id="手动下载"><a href="#手动下载" class="headerlink" title="手动下载"></a>手动下载</h3><p>找到<a href="http://nodejs.cn/download/" target="_blank" rel="noopener">官网</a>，根据需要下载相应版本。</p><h3 id="查看当前node安装位置"><a href="#查看当前node安装位置" class="headerlink" title="查看当前node安装位置"></a>查看当前node安装位置</h3><p> 查看当前node版本</p><pre><code>node -v</code></pre><p>查看当前node安装位置</p><pre><code>where node</code></pre><h3 id="将下载下来的node安装到相同路径即可"><a href="#将下载下来的node安装到相同路径即可" class="headerlink" title="将下载下来的node安装到相同路径即可"></a>将下载下来的node安装到相同路径即可</h3><h3 id="更好的方法是安装nvmw"><a href="#更好的方法是安装nvmw" class="headerlink" title="更好的方法是安装nvmw"></a>更好的方法是安装nvmw</h3><pre><code>npm install -g nvmw</code></pre><p>使用nvmw很简单。帮助说明：</p><pre><code>nvmw -h</code></pre><p>版本查看</p><pre><code>nvmw -v</code></pre><p>安装相应版本的nodejs：</p><pre><code>nvmw install v16.5.0</code></pre><p>切换版本</p><pre><code>nvmw use &lt;version&gt;</code></pre><p>最后查看nodejs的版本：</p><pre><code>node -v</code></pre><p>由于nvm也可用于Windows，因此可以采用nvm进行版本管理。其使用方法类似于nvmw。<br>首先采用<code>nvm list</code>查看当前可用(已安装)的nodejs版本，安装相应版本：</p><pre><code>nvm install 13.0.1</code></pre><p>再使用相应版本如：</p><pre><code>nvm ues 13.0.1</code></pre>]]></content>
      
      
      <categories>
          
          <category> nodejs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nodejs </tag>
            
            <tag> nvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学期望</title>
      <link href="/posts/b9d78651.html"/>
      <url>/posts/b9d78651.html</url>
      
        <content type="html"><![CDATA[<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><p>之前被问一个概率论的题目，这里分享一下我的解法：</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>问题一：</p><script type="math/tex; mode=display">\left( 1 \right) x_1+x_2+x_3+x_4\le 1, 0\le x_i\le 1</script><p>求：$\mathbb{E} \left( x_1x_2x_3x_4 \right) $</p><p>问题二：</p><script type="math/tex; mode=display">\left( 2 \right) x_1+x_2+x_3+x_4\le 1\text{，}0\le x_i\le 1</script><p>且$x_1+x_2=\frac{1}{2},x_3+x_4=\frac{1}{2}$<br>求：$\mathbb{E} \left( x_1x_2x_3x_4 \right) $</p><h3 id="误区"><a href="#误区" class="headerlink" title="误区"></a>误区</h3><p>首先一个误区是就是考虑$x_i$是均匀分布，我们先假设$x_i$为均匀分布，则有：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\mathbb{E}&  \left(  x_1+x_2+x_3+x_4 \right) =4\mathbb{E} \left( x_1 \right)\\&=4\int_0^1{x_1dx_1}=2\end{split}\nonumber\end{equation}</script><p>我们又知道：</p><script type="math/tex; mode=display">\mathbb{E} \left( x_1+x_2+x_3+x_4 \right) \le \mathbb{E} \left( 1 \right) =1</script><p>因此产生矛盾，即$x_i$是均匀分布不成立。如下正解</p><h3 id="问题一解答"><a href="#问题一解答" class="headerlink" title="问题一解答"></a>问题一解答</h3><p>设$x_1+x_2+x_3+x_4$是均匀分布的，那么：</p><script type="math/tex; mode=display">\begin{equation}\iiiint_{ x_1+ x_2 + x_3 + x_4 \le 1}{dx_1 dx_2 dx_3 dx_4} =1\end{equation}</script><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{\partial^2 f}{\partial{x^2}} &= \frac{\partial(\Delta_x f(i,j))}{\partial x} = \frac{\partial(f(i+1,j)-f(i,j))}{\partial x} \\&= \frac{\partial f(i+1,j)}{\partial x} - \frac{\partial f(i,j)}{\partial x} \\&= f(i+2,j) -2f(f+1,j) + f(i,j)\end{split}\nonumber\end{equation}</script><p>则有：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}f\left( x_1,x_2,x_3,x_4 \right) &=\frac{1}{\iiiint_{x_1+x_2+x_3+x_4\le 1}{dx_1dx_2dx_3dx_4}}\\&=\frac{1}{\int_0^1{dx_1}\int_0^{1-x_1}{dx_2}\int_0^{1-x_1-x_2}{dx_3}\int_0^{1-x_1-x_2-x_3}{dx_4}}\\&=\frac{1}{\int_0^1{dx_1}\int_0^{1-x_1}{dx_2}\int_0^{1-x_1-x_2}{\left( 1-x_1-x_2-x_3 \right) dx_3}}\\&=\frac{1}{\int_0^1{dx_1}\int_0^{1-x_1}{\frac{\left( 1-x_1-x_2 \right) ^2}{2}dx_2}}\\&=\frac{1}{\int_0^1{dx_1}\int_0^{1-x_1}{\frac{x_{2}^{2}}{2}dx_2}}\\&=\frac{1}{\int_0^1{\frac{\left( 1-x_1 \right)^3}{6}dx_1}}\\&=\frac{1}{\int_0^1{\frac{x_1^3}{6}dx_1}}\\&=24\end{split}\nonumber\end{equation}</script><p>因此得到其概率密度为：</p><script type="math/tex; mode=display">f\left( x_1,x_2,x_3,x_4 \right) =\begin{cases}    24  \quad  0\le x_1+x_2+x_3+x_4\le 1\text{，}0\le x_i\le 1\text{，}\\    0       \quad  \quad \quad \quad \quad            \mathrm{else}\\\end{cases}</script><p>因此我们可以求出期望：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\mathbb{E} \left( x_1x_2x_3x_4 \right) &=\iiiint_{\varOmega}{24x_1x_2x_3x_4dx_1dx_2dx_3dx_4}\\&=24\int_0^1{x_1dx_1}\int_0^{1-x_1}{x_2dx_2}\int_0^{1-x_1-x_2}{x_3dx_3}\int_0^{1-x_1-x_2-x_3}{x_4dx_4}\\&=24\int_0^1{x_1dx_1}\int_0^{1-x_1}{x_2dx_2}\int_0^{1-x_1-x_2}{x_3\frac{\left( 1-x_1-x_2-x_3 \right) ^2}{2}dx_3}\\&=24\int_0^1{x_1dx_1}\int_0^{1-x_1}{x_2dx_2}\int_0^{1-x_1-x_2}{x_3\frac{\left( 1-x_1-x_2-x_3 \right) \left( 1-x_1-x_2 \right)}{4}dx_3}\\&=24\int_0^1{x_1dx_1}\int_0^{1-x_1}{x_2\left[ \frac{\left( 1-x_1-x_2 \right) ^4}{24} \right] dx_2}\\&=\int_0^1{x_1\frac{\left( 1-x_1 \right) ^6}{30}dx_1}\\&=\frac{1}{56\times 30}=0.000595238\end{split}\nonumber\end{equation}</script><h3 id="问题二解答"><a href="#问题二解答" class="headerlink" title="问题二解答"></a>问题二解答</h3><p>问题二也即：</p><script type="math/tex; mode=display">0\le x_i\le \frac{1}{2}\text{，且}x_1+x_2=\frac{1}{2},x_3+x_4=\frac{1}{2}</script><p>$\text{求}\mathbb{E} \left( x_1x_2x_3x_4 \right)$</p><p>也即：</p><script type="math/tex; mode=display">0\le x_i\le \frac{1}{2}\text{，且}x_1+x_2=\frac{1}{2}</script><p>求$\left[ \mathbb{E} \left( x_1x_2 \right) \right] ^2$<br>由于</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\mathbb{E} \left( x_1x_2 \right) &=\mathbb{E} \left( x_1\left( \frac{1}{2}-x_1 \right) \right)\\&=\frac{1}{2}\mathbb{E} \left( x_1 \right) -\mathbb{E} \left( x_{1}^{2} \right) \\&=\frac{1}{2}\times \frac{1}{4}-\int_0^{\frac{1}{2}}{2x^2dx}\\&=\frac{1}{24}\end{split}\nonumber\end{equation}</script><p>因此：</p><script type="math/tex; mode=display">\left[ \mathbb{E} \left( x_1x_2 \right) \right] ^2=\left( \frac{1}{24} \right) ^2=0.00173611</script><h3 id="计算机验证"><a href="#计算机验证" class="headerlink" title="计算机验证"></a>计算机验证</h3><pre><code class="lang-matlab">clear;close all;num = 100000;s1=0;N=0;for i=1:num    x1 = rand;x2 = rand; x3 = rand; x4 = rand;    if x1+x2+x3+x4 &lt;= 1        z1=x1*x2*x3*x4;N=N+1;        scatter(N,z1,8,&#39;r&#39;)        hold on;        s1=s1+z1;    endends1_mean=s1/Nclear s2;s2=0;for i=1:N    x1 = 1/2*rand;  x3 = 1/2*rand;    z2=x1*(1/2-x1)*x3*(1/2-x3);    scatter(i,z2,8,&#39;g&#39;)    s2=s2+z2;ends2_mean=s2/N</code></pre><p><img src="https://i.loli.net/2021/07/08/qrnYFzeXfDcjlbh.png" alt="image.png"><br>数值模拟的平均为：<br><img src="https://i.loli.net/2021/07/08/8idyjQ4mDRfxLqY.png" alt="image.png"><br>验证成功。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 概率论 </tag>
            
            <tag> 数学期望 </tag>
            
            <tag> 计算机仿真 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>炉温曲线PDE</title>
      <link href="/posts/7430df62.html"/>
      <url>/posts/7430df62.html</url>
      
        <content type="html"><![CDATA[<h1 id="详解2020数学建模国赛A题炉温曲线"><a href="#详解2020数学建模国赛A题炉温曲线" class="headerlink" title="详解2020数学建模国赛A题炉温曲线"></a>详解2020数学建模国赛A题炉温曲线</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在集成电路板等电子产品生产中，需要将安装有各种电子元件的印刷电路板放置在回焊炉中，通过加热，将电子元件自动焊接到电路板上。在这个生产过程中，让回焊炉的各部分保持工艺要求的温度，对产品质量至关重要。目前，这方面的许多工作是通过实验测试来进行控制和调整的。本题旨在通过机理模型来进行分析研究。</p><p>回焊炉内部设置若干个小温区，它们从功能上可分成4个大温区：预热区、恒温区、回流区、冷却区（如图1所示）。电路板两侧搭在传送带上匀速进入炉内进行加热焊接。<br><img src="https://img-blog.csdnimg.cn/20210706090353769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FiY3dzcA==,size_16,color_FFFFFF,t_70" alt="图1"></p><p>某回焊炉内有11个小温区及炉前区域和炉后区域（如图1），每个小温区长度为30.5 cm，相邻小温区之间有5 cm的间隙，炉前区域和炉后区域长度均为25 cm。</p><p>回焊炉启动后，炉内空气温度会在短时间内达到稳定，此后，回焊炉方可进行焊接工作。炉前区域、炉后区域以及小温区之间的间隙不做特殊的温度控制，其温度与相邻温区的温度有关，各温区边界附近的温度也可能受到相邻温区温度的影响。另外，生产车间的温度保持在25ºC。</p><p>在设定各温区的温度和传送带的过炉速度后，可以通过温度传感器测试某些位置上焊接区域中心的温度，称之为炉温曲线（即焊接区域中心温度曲线）。附件是某次实验中炉温曲线的数据，各温区设定的温度分别为175ºC（小温区1-5）、195ºC（小温区6）、235ºC（小温区7）、255ºC（小温区8-9）及25ºC（小温区10~11）；传送带的过炉速度为70 cm/min；焊接区域的厚度为0.15 mm。温度传感器在焊接区域中心的温度达到30ºC时开始工作，电路板进入回焊炉开始计时。</p><p>实际生产时可以通过调节各温区的设定温度和传送带的过炉速度来控制产品质量。在上述实验设定温度的基础上，各小温区设定温度可以进行±10ºC范围内的调整。调整时要求小温区1-5中的温度保持一致，小温区8-9中的温度保持一致，小温区10-11中的温度保持25ºC。传送带的过炉速度调节范围为65~100 cm/min。</p><p>在回焊炉电路板焊接生产中，炉温曲线应满足一定的要求，称为制程界限（见表1）。详情看官网题目数据。</p><p>请你们团队回答下列问题：</p><p><strong>问题1</strong>  请对焊接区域的温度变化规律建立数学模型。假设传送带过炉速度为78 cm/min，各温区温度的设定值分别为173ºC（小温区1-5）、198ºC（小温区6）、230ºC（小温区7）和257ºC（小温区8~9），请给出焊接区域中心的温度变化情况，列出小温区3、6、7中点及小温区8结束处焊接区域中心的温度，画出相应的炉温曲线，并将每隔0.5 s焊接区域中心的温度存放在提供的result.csv中。</p><p><strong>问题2</strong>  假设各温区温度的设定值分别为182ºC（小温区1-5）、203ºC（小温区6）、237ºC（小温区7）、254ºC（小温区8~9），请确定允许的最大传送带过炉速度。</p><p><strong>问题3</strong>  在焊接过程中，焊接区域中心的温度超过217ºC的时间不宜过长，峰值温度也不宜过高。理想的炉温曲线应使超过217ºC到峰值温度所覆盖的面积（图2中阴影部分）最小。请确定在此要求下的最优炉温曲线，以及各温区的设定温度和传送带的过炉速度，并给出相应的面积。</p><p><strong>问题4</strong>  在焊接过程中，除满足制程界限外，还希望以峰值温度为中心线的两侧超过217ºC的炉温曲线应尽量对称（参见图2）。请结合问题3，进一步给出最优炉温曲线，以及各温区设定的温度及传送带过炉速度，并给出相应的指标值。<br><img src="https://img-blog.csdnimg.cn/20210706090311361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FiY3dzcA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>拿到题目首先分析得知此类型属于热传导题目（即PDE类型)。给定的数据有：传送带速度、已知回流焊设定温度后的一次测试数据，回流焊结构(各个温区的长度)。其实拿到题目前就要将这些变量关系弄明白，是设定好传送带速度、各个温区的温度后，印刷版的受热情况也就确定、炉温曲线就确定了，因此，核心就是找到变量对应的关系：<br>传送带速度+各个温区的温度———$^f$————&gt;炉温曲线<br>而其中$f$是由于回焊炉内的热传导参数确定的模型(这个参数我们也是不知道的)</p><p>这样子题目就清晰多啦！<br>第一问其实说白了就是根据最小二乘的思想进行拟合，在先验数据测试数据下优化回焊炉内的热传导参数，来确定模型，也就是确定$f$，从而可以在问题一中给定的传送带速度+各个温区的设定温度下，去计算整个受热状态，这样小温区3、6、7中点及小温区8结束处焊接区域中心的温度，相应的炉温曲线都出来啦！！</p><h2 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a>数学模型</h2><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><p><strong>最小化数据误差:</strong></p><script type="math/tex; mode=display">(h_{cm},D,\lambda)=\mathop{\arg}\limits_{h_{c},D,\lambda}\min\sum_{i=1}^{N}[T(\frac{d}{2},t_{i};h_{c},D,\lambda)-T^{\ast}(t_{i})]^{2},m=1,2,3.</script><p><strong>热力学控制方程：</strong></p><script type="math/tex; mode=display">\frac{\partial u(y,t)}{\partial t}|_{y\in(0,d)}=D\frac{\partial^{2}u(y,t)}{\partial y^{2}}|_{y\in(0,d)}.</script><p>这个方程是热传导方程的核心，它属于抛物型PDE，利用傅里叶热传导定律和能量守恒推出来的热传导方程。<br><strong>第三类边界条件</strong></p><script type="math/tex; mode=display">\begin{cases}-\lambda \frac{\partial u\left( y,t \right)}{\partial y}|_{y=d}=h_{cm}\left( u\left( d,t \right) -T_{qi}. \right)\\-\lambda \frac{\partial u\left( y,t \right)}{\partial y}|_{y=0}=h_{cm}\left(T_{qi} -u\left( 0,t \right) \right).\\\end{cases}</script><p>利用热对流密度的定义推出来的，作为边界条件。<br><strong>初值条件</strong></p><script type="math/tex; mode=display">u(y,0)|_{y\in(0,d)}=T_{ori}.</script><p>初始温度设定好，也就是印刷版刚进入回流焊区域时的温度。</p><p>有了PDE方程和初边值条件，就可以求解啦！！！如有限差分、有限元、有限体积等等，一般我们采用有限差分法，相对简单：</p><h3 id="有限差分"><a href="#有限差分" class="headerlink" title="有限差分"></a>有限差分</h3><p>先对热传导方程进行离散化，主要是有限差分格式的选取，方法比较多，自圆其说即可，这里提供自适应的差分格式：</p><script type="math/tex; mode=display">\frac{u_{j}^{n}-u_{j}^{n-1}}{\tau}-a\left[ \theta \frac{u_{j+1}^{n}-2u_{j}^{n}+u_{j-1}^{n}}{h^2}+\left( 1-\theta \right) \frac{u_{j+1}^{n-1}-2u_{j}^{n-1}+u_{j-1}^{n-1}}{h^2} \right] =0.</script><p>引入$\theta$后，也就是将前后差分结合。</p><p>对其进行泰勒展开得到截断误差即为：</p><script type="math/tex; mode=display">\Delta E=a\left( \frac{1}{2}-\theta \right) \tau \left[ \frac{\partial ^3u}{\partial y^2\partial t} \right] _{j}^{n}+O\left( \tau ^2+h^2 \right).</script><p>令$\theta=\frac{1}{2}$则得到更高精度。</p><h3 id="追赶法"><a href="#追赶法" class="headerlink" title="追赶法"></a>追赶法</h3><p>对离散化的线性方程组进行LU分解，根据系数矩阵特点，采用追赶法求解，这里讲一讲追赶法，追赶法是刚好适用于离散化后的线性方程组的形式，其分为两部分————“追”和“赶”，对于线性方程组：</p><script type="math/tex; mode=display">\left(\begin{array}{ccccccc}b_{1} & c_{1} & & & & & \\a_{2} & b_{2} & c_{2} & & & & \\& \ddots & \ddots & \ddots & & & \\& & a_{i} & b_{i} & c_{i} & & \\& & & \ddots & \ddots & \ddots & \\& & & & a_{n-1} & b_{n-1} & c_{n-1} \\& & & & & a_{n} & b_{n}\end{array}\right)\left(\begin{array}{c}x_{1} \\x_{2} \\\vdots \\x_{i} \\\vdots \\x_{n}\end{array}\right)=\left(\begin{array}{c}d_{1} \\d_{2} \\\vdots \\d_{i} \\\vdots \\d_{n}\end{array}\right)</script><p>其中系数矩阵$A$满足对角占优的三对角线矩阵则可用，即满足：<br>(1) $\left|b_{1}\right|&gt;\left|c_{1}\right|&gt;0 ;$<br>(2) $\left|b_{i}\right| \geqslant\left|a_{i}\right|+\left|c_{i}\right|, a_{i}, c_{i} \neq 0, i=2,3, \cdots, n-1$<br>(3) $\left|b_{n}\right|&gt;\left|a_{n}\right|&gt;0$<br>通过高斯消元后得到：</p><script type="math/tex; mode=display">\left(\begin{array}{cccccc}1 & u_{1} & & & \\& 1 & u_{2} & & \\& & \ddots & \ddots & \\& & & 1 & u_{n-1} \\& & & & 1\end{array}\right)\left(\begin{array}{c}x_{1} \\x_{2} \\\vdots \\x_{n-1} \\x_{n}\end{array}\right)=\left(\begin{array}{c}q_{1} \\q_{2} \\\vdots \\q_{n-1} \\q_{n}\end{array}\right)</script><p>其中系数为：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}u_{1}=c_{1} / b_{1} \\q_{1}=d_{1} / b_{1} \\u_{i}=c_{i} /\left(b_{i}-u_{i-1} a_{i}\right), \quad i=2,3, \cdots, n-1 \\q_{i}=\left(d_{i}-q_{i-1} a_{i}\right) /\left(b_{i}-u_{i-1} a_{i}\right), \quad i=2,3, \cdots, n\end{array}\right.</script><p>此时就可以回带啦！先算$x_n$再算$x_{n-1}$一步步的解出$x$。回带方程为：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}x_{n}=q_{n} \\x_{i}=q_{i}-u_{i} x_{i+1}, \quad i=n-1, n-2, \cdots, 2,1\end{array}\right.</script><p><img src="https://img-blog.csdnimg.cn/20210706191919160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FiY3dzcA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最终得到温度的分布图：<br><img src="https://img-blog.csdnimg.cn/20210706203424162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FiY3dzcA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><p>解决了问题一，那问题二就so easy啦！因为问题二只需要优化一个参数，也就是速度的最大值，此时加上制程界限的约束，放入问题一已经得到的热力学模型即可，这里贴一下其中的一种方案：<br><img src="https://img-blog.csdnimg.cn/20210706203712579.png" alt="在这里插入图片描述"></p><h3 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h3><p>问题三其实就是通过调整各温区的设定温度和传送带的过炉速度，在满足制程界限的条件下，使阴影部分面积最小。这里有两个问题，怎么去根据离散数据求面积，第二，如何去优化这个最小。方法很大，对于面积问题，我们可以用数值积分的方式，可以采用辛普森等等；对于优化算法，可以用启发式的智能算法，或者将问题转化为凸优化等等，这是建模都会的。</p><h3 id="问题四"><a href="#问题四" class="headerlink" title="问题四"></a>问题四</h3><p>为了衡量对称性可以从两个方面去衡量，第一是从数据的冗余角度去衡量，也即：</p><script type="math/tex; mode=display">\min\varUpsilon=\sqrt{\frac{\sum_{i=1}^n{\varDelta d^2}}{n}}</script><p>第二就是一般人都会想的面积之差：</p><script type="math/tex; mode=display">\min S_{dark}=S_{n}-T_{k}\times(t_{O}-t_{3})</script><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>问题的关键就是去求解热传导方程，以及去优化整个模型的$f$，这是本题的重点。</p>]]></content>
      
      
      <categories>
          
          <category> 数学建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 偏微分方程 </tag>
            
            <tag> 数值分析 </tag>
            
            <tag> 数学建模 </tag>
            
            <tag> 国赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像增强</title>
      <link href="/posts/cfbddab6.html"/>
      <url>/posts/cfbddab6.html</url>
      
        <content type="html"><![CDATA[<h2 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h2><p>增强图像中的有用信息，它可以是一个失真的过程，其目的是要改善图像的视觉效果，因此，图像增强可以不顾增强后的图像是否失真，只要看得舒服就行。</p><p>图像增强技术分为：<code>空域处理</code>和<code>频域处理</code>，其中空域处理有常见的<code>灰度变换法</code>、<code>直方图法</code>、<code>空域滤波</code>。空域处理常见的有：<code>低通、高通、带阻、同态</code>等</p><h3 id="空域技术"><a href="#空域技术" class="headerlink" title="空域技术"></a>空域技术</h3><h4 id="灰度变换增强"><a href="#灰度变换增强" class="headerlink" title="灰度变换增强"></a>灰度变换增强</h4><p>首先读取图像获取灰度直方图：</p><pre><code class="lang-matlab">clear; close all;I=imread(&#39;pout.tif&#39;);row=size(I,1);column=size(I,2);N=zeros(1, 256);for i=1:row    for j=1:column        k=I(i, j);        N(k+1)=N(k+1)+1;    endendfigure;subplot(121);imshow(I);subplot(122);bar(N);axis tight;</code></pre><p><img src="https://ae01.alicdn.com/kf/U357d274229384e6d93ee357b531ec824m.jpg" alt=""><br>右图可以知道该图像灰度值主要分布在80~150,将80~150处的灰度值均匀分布在0~255中即：</p><pre><code class="lang-matlab">clear; close all;I=imread(&#39;pout.tif&#39;);row=size(I,1);column=size(I,2);N=zeros(1, 256);for i=1:row    for j=1:column        k=I(i, j);        N(k+1)=N(k+1)+1;    endendfigure;subplot(121);imshow(I);subplot(122);bar(N);axis tight;</code></pre><p>得到最终的灰度图以及灰度直方图：<br><img src="https://ae01.alicdn.com/kf/U2982a9dd240943a880d7d06350ab9fabp.jpg" alt=""><br>当然可以使用内置的<code>imadjust()</code>和<code>stretchlim()</code></p><ul><li><code>imadjust(I,[low_in,high_in],[low_out,hight_out],gamma)</code></li><li><code>stretchlim(I)</code>获取最佳区间,即<code>[low_in,high_in]</code><br><img src="https://ae01.alicdn.com/kf/Ud99f7c3b41114fa3a00a4ceca99e8122S.jpg" alt=""><h3 id="直方增强"><a href="#直方增强" class="headerlink" title="直方增强"></a>直方增强</h3>直方图调整法包括直方图均衡化和直方图规定化<h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4>直方图均衡化是将灰度值分布动态范围偏小的图像（如灰度值集中在直方图右部，此时图像过于明亮）扩大其动态范围，改变后的图像的灰度级数有可能降低：<pre><code class="lang-matlab">clear all; close all;I=imread(&#39;tire.tif&#39;);J=histeq(I);figure;subplot(121);imshow(uint8(I));subplot(122);imshow(uint8(J));figure;subplot(121);imhist(I, 64);subplot(122);imhist(J, 64);</code></pre><img src="https://ae01.alicdn.com/kf/U83a3eef292ef4633b8748d1de3b32e60y.jpg" alt=""><br><img src="https://ae01.alicdn.com/kf/U464e8fc2a7b14731834406a28b33d78dE.jpg" alt=""><h4 id="直方图规定化"><a href="#直方图规定化" class="headerlink" title="直方图规定化"></a>直方图规定化</h4>直方图均衡化能自动增强图像的整体对比度，但是往往结果难以受到控制。实际中常常需要增强某个特定灰度值范围内的对比度或使图像灰度值的分布满足特定需求。这个时候使用直方图规定化会有较好的结果。<pre><code class="lang-matlab">clear all; close all;I=imread(&#39;tire.tif&#39;);hgram=ones(1, 256);J=histeq(I, hgram);figure;subplot(121);imshow(uint8(J));subplot(122);imhist(J);</code></pre><img src="https://ae01.alicdn.com/kf/U67ee040fd8164af5b7ad58ece86b71288.jpg" alt=""><h3 id="空域滤波"><a href="#空域滤波" class="headerlink" title="空域滤波"></a>空域滤波</h3>空域滤波是对图像中每个像素为中心的领域进行一系列的运算，然后将得到的结果代替原来的像素值。其中分为线性<code>空域滤波</code>和<code>非线性空域滤波</code>，</li><li><p><strong>线性滤波</strong></p><p>空域滤波常采用<code>imfilter()</code>函数和<code>conv2()</code>函数。<code>imfilter</code>默认方式为相关滤波<br><a href="https://blog.csdn.net/u012746763/article/details/47017581?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control" target="_blank" rel="noopener">参考博客</a></p></li><li><strong>非线性滤波</strong></li><li>中值滤波<code>medfilt2()</code>，中值滤波可以很好的去除椒盐噪声，平滑效果优于均值滤波，在抑制噪声的同时还可以保持图像的边缘清晰。</li><li>排序滤波<code>ordfilt2()</code>，可选取排序后的某个值作为输出。</li><li>自适应滤波<code>wiener()</code>，可根据图像的局部方差来调整滤波器输出：局部方差大则平滑效果弱；局部方差小则平滑效果强。<h4 id="拉普拉斯算子锐化滤波"><a href="#拉普拉斯算子锐化滤波" class="headerlink" title="拉普拉斯算子锐化滤波"></a>拉普拉斯算子锐化滤波</h4>拉普拉斯算子是最简单的各向同性微分算子，具有旋转不变性。一个二维图像函数 的拉普拉斯变换是各向同性的二阶导数，定义为：<script type="math/tex; mode=display">\nabla ^2f\left( x,y \right) =\frac{\partial ^2f}{\partial x^2}+\frac{\partial ^2f}{\partial y^2}</script>为了更适合于数字图像处理，将该方程表示为离散形式： <script type="math/tex; mode=display">\nabla ^2f\left( x,y \right)</script><script type="math/tex; mode=display">=f\left( x-1,y \right) +f\left( x+1,y \right) +</script><script type="math/tex; mode=display">f\left( x,y-1 \right) +f\left( x,y+1 \right) -4f\left( x,y \right)</script>更多内容如参考<a href="https://www.cnblogs.com/xfzhang/archive/2011/01/19/1939020.html" target="_blank" rel="noopener">参考博客</a></li></ul><p><img src="https://ae01.alicdn.com/kf/U5c6ce845e4f946a59ad1c2ed7beaa6cea.jpg" alt=""></p><h3 id="频域滤波"><a href="#频域滤波" class="headerlink" title="频域滤波"></a>频域滤波</h3><p>设$D(u,v)$为频率点$(u,v)$到频域中心的距离,$D_0$为截止频率,则对于低通的滤波有：</p><blockquote><p>理想低通滤波：</p><script type="math/tex; mode=display">H\left( u,v \right) =\begin{cases}    1,         D\left( u,v \right) \leqslant D_0\\    0,         D\left( u,v \right) >D_0\\\end{cases}</script><p>巴特沃斯低通滤波：</p><script type="math/tex; mode=display">H\left( u,v \right) =\frac{1}{1+\left[ \frac{D\left( u,v \right)}{D_0} \right] ^{2n}}</script><p>高斯低通滤波：</p><script type="math/tex; mode=display">H\left( u,v \right) =e^{-D^2\left( u,v \right) /2D_{0}^{2}}</script><p><img src="https://ae01.alicdn.com/kf/Uf4b19d1add2046f78818c98d10c2297aW.jpg" alt=""></p><h3 id="同态滤波"><a href="#同态滤波" class="headerlink" title="同态滤波"></a>同态滤波</h3><p>同态变换一般是指将非线性组合信号通过某种变换，使其变成线性组合信号，从而可以更方便的运用线性操作对信号进行处理。</p></blockquote><p>所谓非线性组合信号，举例来说，比如 $z(t) = x(t) y(t)$，两个信号相乘得到组合信号，由于时域相乘等价于频率域卷积，所以无法在频率域将其分开。但是我们应用一个log算子，对两边取对数，则有： $ln(z(t)) = ln(x(t)) + ln(y(t))$，这样一来，就变成了线性组合的信号，$ln(x(t))$ 和 $ln(y(t))$ 时域相加，所以频域也是相加的关系，如果它们的频谱位置不同，就可以傅里叶变换后较好的分开，以便进行后续的分别的操作，比如应用高、低通滤波或者其他手工设计的滤波器等，然后再将结果傅里叶反变换，得到处理过的 $\hat{ ln(z(t)) }$，在取幂，就可以得到最终的处理结果。</p><pre><code class="lang-matlab">clear; close all;I=imread(&#39;pout.tif&#39;);J=log(im2double(I)+1);    K=fft2(J);n=5;D0=0.1*pi;rh=0.7;rl=0.4;[row, column]=size(J);for i=1:row    for j=1:column        D1(i,j)=sqrt(i^2+j^2);        H(i,j)=rl+(rh/(1+(D0/D1(i,j))^(2*n)));    endendL=K.*H;M=ifft2(L);N=exp(M)-1;figure;subplot(121);imshow(I);subplot(122);imshow(real(N));</code></pre><p><img src="https://ae01.alicdn.com/kf/U431bf47349f14ce69a464d3dcc8da8f4d.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KNN</title>
      <link href="/posts/7c10b52f.html"/>
      <url>/posts/7c10b52f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学习 </tag>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>感知机</title>
      <link href="/posts/18592060.html"/>
      <url>/posts/18592060.html</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习笔记"><a href="#统计学习笔记" class="headerlink" title="统计学习笔记"></a>统计学习笔记</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机适用于<strong>二分类</strong>是对<strong>线性的向量集合</strong>进行分类，是一种<strong>判别模型</strong>，将<strong>极小化误分点到超平面距离</strong>，<strong>把误分点到超平面距离</strong>作为学习的损失函数，利用<strong>梯度下降算法</strong>进行寻优。<br>其算法流程图如下：<br><img src="https://ae01.alicdn.com/kf/Ue8e01b8c695149eab61d96d2518755de1.jpg" alt=""></p><h3 id="matlab实现"><a href="#matlab实现" class="headerlink" title="matlab实现"></a>matlab实现</h3><p>其matlab程序如下：</p><pre><code class="lang-matlab">clear;clc;%% 载入数据data = [1 2 1 ; 2 3 1 ;3 5 1 ; 4 7 1 ; 4 2 -1; 5 1 -1; 9 5 -1; 8 6 -1];    % 测试数据%% 绘制数据点n=size(data,2)-1;% 输入变量的维度sample = data(:,1:n); % 每一行代表一个数据点label = data(:,end);% 样本判别结果draw(sample, label)% 绘制散点图（由于例子给定二维因此可以可视化）hold on%% 初始化w,b,alphaw = [-1,-2];b = 2;alpha = 1;  % 学习因子%%  更新 w,bwhile 1    [idx_misclass, counter] = class(sample, label, w, b);        if (counter~=0)        R = unidrnd(counter);% 随机选择        % 更新w，b        w = w + alpha * sample(idx_misclass(R),:) * label(idx_misclass(R));        b = b + alpha * label(idx_misclass(R));       else        break    endend%% 绘制线性分类器x1 = 1:0.01:10;x2 = (-b-w(1).*x1)./w(2);plot(x1, x2)%% 做分类处理，找到误判及其个数function [idx_misclass, counter] = class(sample, label, w, b)    counter = 0;    idx_misclass = [];    for i=1:length(label)        if (label(i)*(w*sample(i,:)&#39;+b)&lt;=0)            idx_misclass = [idx_misclass i];            counter = counter + 1;                end       endendfunction draw(sample, label)% 用于区分点的类型    idx_pos = find(label==1);    idx_neg = find(label~=1);    plot(sample(idx_pos, 1), sample(idx_pos, 2),&#39;ro&#39;,&#39;LineWidth&#39;,1.5)    hold on    plot(sample(idx_neg, 1), sample(idx_neg, 2),&#39;b*&#39;,&#39;LineWidth&#39;,1.5)%     axis([0 10 0 10])    grid onend</code></pre><p>测试结果如图：<br><img src="https://ae01.alicdn.com/kf/Ua87ffa27578f4922a3cd65f28002ad7dA.jpg" alt=""></p><h3 id="R语言实现"><a href="#R语言实现" class="headerlink" title="R语言实现"></a>R语言实现</h3><pre><code class="lang-R">percept = function(data = data,eta = eta ){  x = data[,-dim(data)[2]] ## 样本输入数据点  y = data[,dim(data)[2]]  ## 样本输出数据  w = c(0,0)               ## 初始化w，b  b = 0  len = length(y)  i = 1    while(i &lt;= len){    if(y[i] * (x[i,] %*% w + b) &lt;= 0){      ## 更新 w and b      w = w + eta * y[i] * x[i,]   ## 在存在误判的点处进行梯度下降      b = b + eta * y[i]      i = 1 ## 保证遍历每个样本点    }    else{      i = i + 1     }  }  return(list(w=w,b=b))}## 主函数，eta学习率## 载入数据data = matrix(c(3,3,1,4,3,1,1,1,-1),nr=3,byrow=T)perceptron = percept(data = data,eta = 1)## 可视化，针对二维dat1 = data[,1:2]dat1 = as.data.frame(dat1)names(dat1) = c(&quot;x1&quot;,&quot;x2&quot;)## 分别计算系数a = perceptron[[&quot;w&quot;]][1]b = perceptron[[&quot;w&quot;]][2]c = perceptron[[&quot;b&quot;]]plot(x2~x1,data = dat1,col = ifelse(a*x1 +b*x2 + c&lt;= 0,&quot;red&quot;,&quot;blue&quot;),pch = 17,bty = &quot;l&quot;)if(b){  abline(-c/b,-a/b,lwd=2,lty=2,col=&quot;red&quot;)}</code></pre><p><img src="https://ae01.alicdn.com/kf/U3e29abce02f44e1c858f885828ac72f0n.jpg" alt=""></p><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><pre><code class="lang-python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学习 </tag>
            
            <tag> 感知机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matlab矢量绘图</title>
      <link href="/posts/9b44567c.html"/>
      <url>/posts/9b44567c.html</url>
      
        <content type="html"><![CDATA[<h1 id="matlab矢量图"><a href="#matlab矢量图" class="headerlink" title="matlab矢量图"></a>matlab矢量图</h1><h2 id="流线图streamline"><a href="#流线图streamline" class="headerlink" title="流线图streamline"></a>流线图streamline</h2><p><code>streamline(x,y,u,v,startx,starty)</code>其中startx,starty分别为流线的坐标</p><pre><code class="lang-matlab">[x,y]=meshgrid(0:0.1:1,0:0.1:1);u=sin(x);v=-y;figurequiver(x,y,u,v,1);startx=0.1:0.1:1;starty=ones(size(startx));streamline(x,y,u,v,startx,starty)</code></pre><p><img src="https://ae01.alicdn.com/kf/Ue1b10ec452d94ecfafcbe52443c31617W.jpg" alt=""></p><h2 id="矢量函数的可视化"><a href="#矢量函数的可视化" class="headerlink" title="矢量函数的可视化"></a>矢量函数的可视化</h2><pre><code class="lang-matlab">%% 二维[x,y]=meshgrid(0:.2:2);u=cos(x).*y;    v=sin(x).*y;quiver(x,y,u,v)</code></pre><p><img src="https://ae01.alicdn.com/kf/Ua41cede298f54548b91409cf92884f5aR.jpg" alt=""></p><pre><code class="lang-matlab">%% 三维[x,y]=meshgrid(0:.2:2);u=cos(x).*y;    v=sin(x).*y;quiver(x,y,u,v)</code></pre><p><img src="https://ae01.alicdn.com/kf/U7ac8e2cd8d9c4f4ca6ff8c5978139fd3h.jpg" alt=""></p><h2 id="梯度gradient"><a href="#梯度gradient" class="headerlink" title="梯度gradient"></a>梯度gradient</h2><p>对于直角坐标系</p><script type="math/tex; mode=display">\nabla =\frac{\partial}{\partial x}\boldsymbol{e}_x+\frac{\partial}{\partial y}\boldsymbol{e}_y+\frac{\partial}{\partial z}\boldsymbol{e}_z</script><p>对于柱坐标系</p><script type="math/tex; mode=display">\nabla =\frac{\partial}{\partial \rho}\boldsymbol{e}_{\rho}+\frac{1}{\rho}\frac{\partial}{\partial \varphi}\boldsymbol{e}_{\varphi}+\frac{\partial}{\partial z}\boldsymbol{e}_z</script><p>对于球坐标系</p><script type="math/tex; mode=display">\nabla =\frac{\partial}{\partial r}\boldsymbol{e}_r+\frac{1}{r}\frac{\partial}{\partial \theta}\boldsymbol{e}_{\theta}+\frac{1}{r\sin \theta}\frac{\partial}{\partial \varphi}\boldsymbol{e}_{\varphi}</script><p>matlab中用法，<code>[Fx,Fy,Fz...Fn]=gradient(F,hx,hy,hz...hn)</code></p><p>下面给个例子：</p><pre><code class="lang-matlab">[x,y]=meshgrid(-2:.1:2);z=x.*exp(-x.^2-y.^2);[px,py]=gradient(z,.2,.2);figurecontour(x,y,z);hold onquiver(x,y,px,py);hold off</code></pre><p><img src="https://ae01.alicdn.com/kf/U9571364d333e46b093dbd5a89ccaf777Z.jpg" alt=""></p><pre><code class="lang-matlab">[x,y]=meshgrid(-2:.1:2);z=x.*exp(-x.^2-y.^2);[px,py]=gradient(z,.2,.2);figurecontour(x,y,z);hold onquiver(x,y,px,py);hold off</code></pre><p><img src="https://ae01.alicdn.com/kf/Ua05dc6b0bc5a4564bf1bfc92bc920b0dD.jpg" alt=""></p><h2 id="散度divergence"><a href="#散度divergence" class="headerlink" title="散度divergence"></a>散度divergence</h2><p>散度是单位体积的通量(即通量体密度)<br>其定义为：</p><script type="math/tex; mode=display">div\boldsymbol{F}=\nabla \cdot \boldsymbol{F}=\underset{\varDelta V\rightarrow 0}{\lim}\frac{\iint{\boldsymbol{F}\cdot d\boldsymbol{S}}}{\varDelta V}</script><p>直角坐标：</p><script type="math/tex; mode=display">\nabla \cdot \boldsymbol{F}=\frac{\partial F_x}{\partial x}+\frac{\partial F_y}{\partial y}+\frac{\partial F_z}{\partial z}</script><p>柱坐标：</p><script type="math/tex; mode=display">\nabla \cdot \boldsymbol{F}=\frac{1}{\rho}\left( \frac{\partial}{\partial \rho}\left( \rho F_{\rho} \right) +\frac{\partial F_{\varphi}}{\partial \varphi}+\rho \frac{\partial F_z}{\partial z} \right)</script><p>球坐标：</p><script type="math/tex; mode=display">\nabla \cdot \boldsymbol{F}=\frac{1}{r^2\sin \theta}\left[ \sin \theta \frac{\partial}{\partial r}\left( r^2F_r \right) +r\frac{\partial}{\partial \theta}\left( \sin \theta F_{\theta} \right) +r\frac{\partial F_{\varphi}}{\partial \varphi} \right]</script><p>高斯公式：</p><script type="math/tex; mode=display">\int_V{\nabla \cdot \boldsymbol{F}dV}=\iint{\boldsymbol{F}\cdot d\boldsymbol{S}}</script><p>散度的体积分等于通量<br>matlab中内置<code>divergence</code></p><ul><li><code>div=divergence(x,y,z,u,v,w)</code></li></ul><pre><code class="lang-matlab">syms x y z realF=[cos(x+2*y),sin(x-2*y)];g=divergence(F,[x y]);divF=matlabFunction(g);x=-2.5:0.1:2.5;[X,Y]=meshgrid(x);Fx=cos(X+2*Y);Fy=sin(X-2*Y);div_num=divF(X,Y);pcolor(X,Y,div_num);shading interp;colorbarhold onquiver(X,Y,Fx,Fy,&#39;k&#39;,&#39;linewidth&#39;,1);</code></pre><p><img src="https://ae01.alicdn.com/kf/U8aaa80c47b1b4c489973aa91efd8923cJ.jpg" alt=""></p><h2 id="旋度curl"><a href="#旋度curl" class="headerlink" title="旋度curl"></a>旋度curl</h2><p>矢量场旋度用来描述围绕中心旋转的程度，定义为：</p><script type="math/tex; mode=display">rot\boldsymbol{F}=\nabla \times \boldsymbol{F}=\underset{\varDelta S\rightarrow 0}{\lim}\frac{\oint{\boldsymbol{F}\cdot d\boldsymbol{l}}}{\varDelta S}|_{\max}\cdot \vec{n}</script><p>对于直角坐标系：</p><script type="math/tex; mode=display">\nabla \times \boldsymbol{F}=\left| \begin{matrix}    \boldsymbol{e}_x&        \boldsymbol{e}_y&        \boldsymbol{e}_z\\    \frac{\partial}{\partial x}&        \frac{\partial}{\partial y}&        \frac{\partial}{\partial z}\\    F_x&        F_y&        F_z\\\end{matrix} \right|</script><p>对于柱坐标：</p><script type="math/tex; mode=display">\nabla \times \boldsymbol{F}=\left| \begin{matrix}    \frac{\boldsymbol{e}_{\rho}}{\rho}&        \boldsymbol{e}_{\varphi}&        \frac{\boldsymbol{e}_z}{\rho}\\    \frac{\partial}{\partial \rho}&        \frac{\partial}{\partial \varphi}&        \frac{\partial}{\partial z}\\    F_{\rho}&        \rho F_{\varphi}&        F_z\\\end{matrix} \right|</script><p>对于球坐标：</p><script type="math/tex; mode=display">\nabla \times \boldsymbol{F}=\left| \begin{matrix}    \frac{\boldsymbol{e}_r}{r^2\sin \theta}&        \frac{\boldsymbol{e}_{\theta}}{r\sin \theta}&        \frac{\boldsymbol{e}_{\varphi}}{r}\\    \frac{\partial}{\partial r}&        \frac{\partial}{\partial \theta}&        \frac{\partial}{\partial \varphi}\\    F_r&        rF_{\theta}&        r\sin \theta F_{\varphi}\\\end{matrix} \right|</script><p>matlab中用<code>curl</code>函数</p><pre><code class="lang-matlab">syms x y z realF=[cos(x+2*y),sin(x-2*y)];G=curl([F,0],[x y z])curlF=matlabFunction(G(3));x=-2.5:0.1:2.5;[X,Y]=meshgrid(x);Fx=cos(X+2*Y);Fy=sin(X-2*Y);rot=curlF(X,Y);pcolor(X,Y,rot);shading interp;colorbarhold onquiver(X,Y,Fx,Fy,&#39;k&#39;,&#39;linewidth&#39;,1);</code></pre><p><img src="https://ae01.alicdn.com/kf/U6fb789e2067947aeba6a131b27989e44f.jpg" alt=""></p><h2 id="拉普拉斯算子"><a href="#拉普拉斯算子" class="headerlink" title="拉普拉斯算子"></a>拉普拉斯算子</h2><p>定义：</p><script type="math/tex; mode=display">\Delta =\nabla \cdot \nabla =\nabla ^2=\frac{\partial ^2}{\partial x^2}+\frac{\partial ^2}{\partial y^2}+\frac{\partial ^2}{\partial z^2}</script><p>矢量函数拉普拉斯变换：</p><script type="math/tex; mode=display">\Delta \boldsymbol{F}=\nabla \left( \nabla \cdot \boldsymbol{F} \right) -\nabla \times \left( \nabla \times \boldsymbol{F} \right)</script><ul><li>通过上式可以求出解析解：<pre><code class="lang-matlab">syms x y zV=[x^2*y,y^2*z,z^2*x];vars=[x y z];gradient(divergence(V,vars))-curl(curl(V,vars),vars)</code></pre></li><li>数值差分法<br><code>del2(U,hx,hy,hz...hn)</code><script type="math/tex; mode=display">L_{i,j}=\frac{1}{4}\left( U_{i,j+1}+U_{i+1,j}+U_{i-1,j}+U_{i,j-1} \right) -U_{i,j}</script>故<code>del2</code>求出的是拉普拉斯运算的1/4</li></ul><pre><code class="lang-matlab">[x,y]=meshgrid(-5:0.25:5);U=1/3.*(x.^4+y.^4);h=0.25;%指定步长L=4*del2(U,h);figuresurf(x,y,L);grid ontitle(&#39;$\Delta U(x,y)=\frac{x^4+y^4}{3}$&#39;,&#39;Interpreter&#39;,&#39;latex&#39;);xlabel(&#39;x&#39;)ylabel(&#39;y&#39;)zlabel(&#39;z&#39;)</code></pre><p><img src="https://ae01.alicdn.com/kf/Uab7c6909662c4bd2bc0ca1aece3d0b6a1.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> MATLAB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 矢量绘图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matlab标量绘图</title>
      <link href="/posts/12076d5d.html"/>
      <url>/posts/12076d5d.html</url>
      
        <content type="html"><![CDATA[<h1 id="matlab标量绘图"><a href="#matlab标量绘图" class="headerlink" title="matlab标量绘图"></a>matlab标量绘图</h1><h2 id="mesh-or-surf"><a href="#mesh-or-surf" class="headerlink" title="mesh or surf"></a>mesh or surf</h2><p>常见不多说，注意xy为网格坐标，mesh类有meshc，meshz；surf类有surfc，surfl</p><pre><code class="lang-matlab">x=linspace(-2,2,25);y=linspace(-2,2,25);[xx,yy]=meshgrid(x,y);zz=sqrt(xx.^2+yy.^2);subplot(221)mesh(xx,yy,zz);subplot(222)surf(xx,yy,zz);subplot(223)meshz(xx,yy,zz);subplot(224)surfc(xx,yy,zz);</code></pre><p>如图</p><p><img src="https://p.pstatp.com/origin/137f70001410a6f2efe14" alt=""></p><h2 id="contour"><a href="#contour" class="headerlink" title="contour"></a>contour</h2><p>contour(x,y,z)基本用法和mesh相同<br>contour3(x,y,z,n)其中n控制等高线的稀密</p><pre><code class="lang-matlab">[X,Y]=meshgrid([-3:0.3:3]);Z=X.*exp(-(X.^2+Y.^2));subplot(211)contour(X,Y,Z);title 二维等高subplot(212)contour3(X,Y,Z,30);title 三维等高</code></pre><p>等高图：<br><img src="https://ae01.alicdn.com/kf/U280f0beebc3f43059f8406208a3cb7ccT.jpg" alt=""></p><h2 id="pcolor伪彩色图"><a href="#pcolor伪彩色图" class="headerlink" title="pcolor伪彩色图"></a>pcolor伪彩色图</h2><pre><code>x=linspace(-2,2,25);y=linspace(-2,2,25);[xx,yy]=meshgrid(x,y);zz=sqrt(xx.^2+yy.^2);pcolor(xx,yy,zz);hold onplot([0:20],[0:20],&#39;r+&#39;);   % 绘制xx与yy相等的点colorbar;</code></pre><p><img src="https://img.rruu.net/image/5f89175768de2" alt=""></p><h2 id="isosurfface隐函数图或等值面"><a href="#isosurfface隐函数图或等值面" class="headerlink" title="isosurfface隐函数图或等值面"></a>isosurfface隐函数图或等值面</h2><pre><code>[X,Y,Z]=meshgrid(linspace(-10,10));V=X.^2+Y.^2-Z.^2;isosurface(X,Y,Z,V,1);%绘制隐函数图像axis equalcolormap([1 0 0]);brighten(0.5);%进行增亮camlight right;%设置光源位置lighting phong;%设置光照模式figure(2);fv=isosurface(X,Y,Z,V,1);p=patch(fv);%绘制等值面set(p,&#39;FaceColor&#39;,&#39;red&#39;,&#39;EdgeColor&#39;,&#39;none&#39;);axis equal</code></pre><p><img src="https://ae01.alicdn.com/kf/U5e17985b212e4593abb2e40787e93152a.jpg" alt=""><br><img src="https://ae01.alicdn.com/kf/U1b5e6caa482a4367a87cb6b12b944eeb7.jpg" alt=""></p><h2 id="peaks多锋函数"><a href="#peaks多锋函数" class="headerlink" title="peaks多锋函数"></a>peaks多锋函数</h2><pre><code class="lang-matlab">[x,y,z]=peaks;peaks;figure(2)meshz(x,y,z);figure(3)surfc(x,y,z);</code></pre><p><img src="https://ae01.alicdn.com/kf/U08fe815493814b7693dadc4b7b37b71f7.jpg" alt=""><br><img src="https://ae01.alicdn.com/kf/U4a17bce9ec084866a0af35ebd16736fbQ.jpg" alt=""><br><img src="https://ae01.alicdn.com/kf/Ucd2df1bb6b53484c84b2743c6ce33356d.jpg" alt=""></p><h2 id="slice切片函数"><a href="#slice切片函数" class="headerlink" title="slice切片函数"></a>slice切片函数</h2><p><code>slice(x,y,z,v,xi,yi,zi)</code></p><pre><code>[x,y,z]=meshgrid(-2:0.16:2);v=x.*y.*z.*exp(-(x.^2+y.^2+z.^2));xslice=-1.2:0.8:2;yslice=2;zslice=[-2,0];%定义切片位置slice(x,y,z,v,xslice,yslice,zslice)colormap hsv</code></pre><p><img src="https://ae01.alicdn.com/kf/U03d062f345ae4b109a1d6ec88af0b78bR.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> MATLAB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 标量绘图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>确知信号</title>
      <link href="/posts/d17ccd85.html"/>
      <url>/posts/d17ccd85.html</url>
      
        <content type="html"><![CDATA[<h1 id="确知信号"><a href="#确知信号" class="headerlink" title="确知信号"></a>确知信号</h1><h2 id="频率分析"><a href="#频率分析" class="headerlink" title="频率分析"></a>频率分析</h2><h3 id="周期信号的频谱"><a href="#周期信号的频谱" class="headerlink" title="周期信号的频谱"></a>周期信号的频谱</h3><p>设$s(t)$周期为$T_0$且满足Dirichlet条件，则可以展开为指数型的傅里叶级数：</p><script type="math/tex; mode=display">s(t)=\sum_{n=-\infty}^{\infty} C_n e^{jnw_0 t}</script><p>其中傅里叶系数为：</p><script type="math/tex; mode=display">C_n=C(nf_0)=\frac{1}{T_0}\int_{-\frac{T_0}{2}}^{\frac{T_0}{2}} s(t)e^{-jnw_0 t}dt</script><p>直流分量为信号一个周期的时间平均值：</p><script type="math/tex; mode=display">C_0=\frac{1}{T_0}\int_{-\frac{T_0}{2}}^{\frac{T_0}{2}} s(t)dt</script><p>以周期为2，脉宽为1，幅度为1的方波信号进行频域变换为：</p><p><strong>时域为：</strong><br><img src="https://imgkr2.cn-bj.ufileos.com/57f5569f-589d-4c20-b1d1-a20b31df8c1e.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=RmtXqv3MHDviAdoXRBoysq09fZI%253D&amp;Expires=1602728477" alt=""></p><p><strong>频域为：</strong></p><p><img src="https://imgkr2.cn-bj.ufileos.com/59c7b5f8-168e-47db-9b1f-9a21088d6a52.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=Ez5nm%252BYJUMb0zJbsGXujWo8Exs4%253D&amp;Expires=1602728608" alt=""></p><h3 id="非周期信号的频谱"><a href="#非周期信号的频谱" class="headerlink" title="非周期信号的频谱"></a>非周期信号的频谱</h3><p>非周期信号的频谱通过傅里叶变换求出概率密度函数</p><p>eg：<strong>时域信号：</strong></p><p><img src="https://imgkr2.cn-bj.ufileos.com/f6f171e0-8948-4be3-93ed-359196a707f6.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=IHZqp6FVnuJunw4dgaKyfZ9VL04%253D&amp;Expires=1602728677" alt=""></p><p><strong>频域谱图：</strong></p><p><img src="https://imgkr2.cn-bj.ufileos.com/7ac9c582-23c9-43f3-a731-63018b19a467.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=6Qx6F4HGExQJiMMfHQcMesh%252BXwk%253D&amp;Expires=1602728714" alt=""></p><h3 id="能量谱和功率谱"><a href="#能量谱和功率谱" class="headerlink" title="能量谱和功率谱"></a>能量谱和功率谱</h3><p>能量信号有能量谱，功率信号有功率谱</p><h4 id="能量谱密度"><a href="#能量谱密度" class="headerlink" title="能量谱密度"></a>能量谱密度</h4><p>设$s(t)$频谱密度为$S(t)$，则能量谱密度为：</p><script type="math/tex; mode=display">G(f)=|S(f)|^2   \qquad (j/Hz)</script><p><img src="https://imgkr2.cn-bj.ufileos.com/3603502f-f850-43d0-ba88-535f8d8da427.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=t%252FfFjsI%252Ffj3nii2qsz3EMwUV1MY%253D&amp;Expires=1602728755" alt=""></p><h4 id="能量信号的能量——Parseval"><a href="#能量信号的能量——Parseval" class="headerlink" title="能量信号的能量——Parseval"></a>能量信号的能量——Parseval</h4><script type="math/tex; mode=display">E=\underset{\text{时域}}{\underbrace{\int_{-\infty}^{+\infty}{s^2\left( t \right) dt}}}=\underset{\text{频域}}{\underbrace{\int_{-\infty}^{+\infty}{|S\left( f \right) |^2df}}}</script><h4 id="功率谱密度"><a href="#功率谱密度" class="headerlink" title="功率谱密度"></a>功率谱密度</h4><script type="math/tex; mode=display">P\left( f \right) =\lim_{T\rightarrow \infty} \frac{1}{T}|S_T\left( f \right) |^2\qquad \left( W/Hz \right)</script><h4 id="功率信号的功率——Parseval"><a href="#功率信号的功率——Parseval" class="headerlink" title="功率信号的功率——Parseval"></a>功率信号的功率——Parseval</h4><script type="math/tex; mode=display">P=\underset{\text{时域}}{\underbrace{\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}{s^2\left( t \right) dt}}}=\underset{\text{频域}}{\underbrace{\int_{-\infty}^{+\infty}{P\left( f \right) ^2df}}}</script><p>对于周期信号，其功率谱密度为</p><script type="math/tex; mode=display">P\left( f \right) =\sum_{n=-\infty}^{\infty}{|C_n|^2\delta \left( f-nf_0 \right)}</script><script type="math/tex; mode=display">P\left( w \right) =2\pi \sum_{n=-\infty}^{\infty}{|C_n|^2\delta \left( w-nw_0 \right)}</script><p>其<strong>Parseval</strong>恒等式为</p><script type="math/tex; mode=display">P=\underset{\text{时域}}{\underbrace{\frac{1}{T_0}\int_{-\frac{T_0}{2}}^{\frac{T_0}{2}}{s^2\left( t \right) dt}}}=\underset{\text{频域}}{\underbrace{\sum_{n=-\infty}^{\infty}{|C_n|^2}}}</script><h2 id="时域分析"><a href="#时域分析" class="headerlink" title="时域分析"></a>时域分析</h2><h3 id="相关函数"><a href="#相关函数" class="headerlink" title="相关函数"></a>相关函数</h3><ol><li>能量信号互相关函数：<script type="math/tex; mode=display">R_{12}\left( \tau \right) =\int_{-\infty}^{+\infty}{s_1\left( t \right) s_2\left( t+\tau \right) dt}</script></li><li>功率信号互相关函数：<script type="math/tex; mode=display">R_{12}\left( \tau \right) =\lim_{T\rightarrow \infty} \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}{s_1\left( t \right) s_2\left( t+\tau \right) dt}</script></li></ol><ul><li>自相关函数即令$s_1(t)=s_2(t)=s(t)$<h4 id="互相关函数的性质"><a href="#互相关函数的性质" class="headerlink" title="互相关函数的性质"></a>互相关函数的性质</h4></li><li>若对任意$\tau$有$R_{12}(\tau)=0$，则两个信号互不相关。</li><li>$R_{12}(\tau)=R_{21}(-\tau)$</li><li>$R_{12}(0)=R_{21}(0)$可用于表示$s_1(t)$和$s_2(t)$无时差时的相似性，可做为互相关系数。</li><li>归一化互相关系数：<ul><li>对于两个能量信号：<script type="math/tex; mode=display">\rho_{12}=\frac{R_{12}(0)}{\sqrt{E_{1} E_{2}}}=\frac{\int_{-\infty}^{\infty} s_{1}(t) s_{2}(t) \mathrm{d} t}{\sqrt{E_{1} E_{2}}}</script></li><li>对于两个功率信号：<script type="math/tex; mode=display">\rho_{12}=\frac{R_{12}(0)}{\sqrt{P_{1} P_{2}}}</script><h4 id="自相关函数的性质"><a href="#自相关函数的性质" class="headerlink" title="自相关函数的性质"></a>自相关函数的性质</h4></li></ul></li><li><script type="math/tex; mode=display">R(\tau)=R(-\tau)</script></li><li><script type="math/tex; mode=display">|R(\tau)| \leqslant R(0)</script></li><li>$R(0)$可表示为能量信号（功率信号）的能量或功率<h4 id="相关函数和谱密度关系（重点）"><a href="#相关函数和谱密度关系（重点）" class="headerlink" title="相关函数和谱密度关系（重点）"></a>相关函数和谱密度关系（重点）</h4></li><li>能量信号的自相关函数与能量谱密度是一对傅里叶变换：<script type="math/tex; mode=display">R\left( \tau \right) \Longleftrightarrow |S\left( f \right) |^2</script></li><li>功率信号的自相关函数与功率谱密度是一对傅里叶变换：<script type="math/tex; mode=display">R\left( \tau \right) \Longleftrightarrow P\left( f \right)</script></li></ul><p>下面绘制出$g_\tau (t),\tau=1$在[-10,10]的自相关函数与$g_\tau (t)$和$\sin (t)$的互相关函数：</p><p><img src="https://imgkr2.cn-bj.ufileos.com/5f1384ea-eb1d-4236-9224-574a4c3fe9de.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=NBU3tPViTtPfHkEcsiFrwqblVNI%253D&amp;Expires=1602728797" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 通信原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 确知信号 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通信原理信息论</title>
      <link href="/posts/f117c497.html"/>
      <url>/posts/f117c497.html</url>
      
        <content type="html"><![CDATA[<h1 id="通信原理信息论"><a href="#通信原理信息论" class="headerlink" title="通信原理信息论"></a>通信原理信息论</h1><h2 id="信息量-I-单位-bit"><a href="#信息量-I-单位-bit" class="headerlink" title="信息量$I$单位(bit)"></a>信息量$I$单位(bit)</h2><script type="math/tex; mode=display">I=-log_2 P(x)</script><h2 id="平均信息量-或熵-H-x-单位-b-符号"><a href="#平均信息量-或熵-H-x-单位-b-符号" class="headerlink" title="平均信息量(或熵)$H(x)$单位(b/符号)"></a>平均信息量(或熵)$H(x)$单位(b/符号)</h2><p>描述信源的不确定性：</p><script type="math/tex; mode=display">H(x)=-\sum_{i=1}^M P(x_i)log_2 P(x_i)</script><p>当每个符号等概率$P(x_i)=1/M$独立出现时，不确定性最大，此时熵为：</p><script type="math/tex; mode=display">H_{max}=log_2M</script><h2 id="总信息量"><a href="#总信息量" class="headerlink" title="总信息量"></a>总信息量</h2><script type="math/tex; mode=display">I=n\cdot H(x)</script><h2 id="有效性和可靠性"><a href="#有效性和可靠性" class="headerlink" title="有效性和可靠性"></a>有效性和可靠性</h2><ul><li>对于模拟信号：</li></ul><p>1、有效性：信号带宽</p><p>2、 可靠性：输出信噪比$\frac{S_o}{N_o}$</p><ul><li>对于数字信号：</li></ul><p>1、有效性：</p><ul><li>频带利用率(单位带宽内传输速率)<script type="math/tex; mode=display">\eta =\frac{R_B}{B} \qquad \qquad (Baud/Hz)</script><script type="math/tex; mode=display">\eta_{b} =\frac{R_b}{B} \qquad \qquad ((bit/s)/Hz)</script>其中$R_B$为传码率，仅仅取决于每个码元持续时间(码元宽度)$T_B$:<script type="math/tex; mode=display">R_B=\frac{1}{T_B} \qquad (Baud)</script>$R_b$为传信率，在符号等概率出现时有<script type="math/tex; mode=display">R_b =R_Blog_2M</script></li></ul><p>2、 可靠性：输出信噪比$\frac{S_o}{N_o}$</p><ul><li>误码率：<script type="math/tex; mode=display">P_e=\frac{错误接收的码元数}{传输的总码元数}=\frac{N_e}{N}</script></li><li>误信率：<script type="math/tex; mode=display">P_b=\frac{错误接收的比特数}{传输的总比特数}=\frac{I_e}{I}</script>关系：二进制时，有$P_b=P_e$,<br>$M$进制时，$P_b &lt; P_e$</li></ul>]]></content>
      
      
      <categories>
          
          <category> 信息论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 通信原理 </tag>
            
            <tag> 信息论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>偏微分方程数值方法</title>
      <link href="/posts/1f70d57e.html"/>
      <url>/posts/1f70d57e.html</url>
      
        <content type="html"><![CDATA[<h1 id="偏微分方程数值解"><a href="#偏微分方程数值解" class="headerlink" title="偏微分方程数值解"></a>偏微分方程数值解</h1><h2 id="工具箱使用"><a href="#工具箱使用" class="headerlink" title="工具箱使用"></a>工具箱使用</h2><p>步骤如下：</p><ol><li>画区域，<code>Draw</code>或者直接在菜单栏上找，（画圆时，需要按住<code>Ctrl</code>）</li><li>设置边界条件，<code>Boundary</code>进入边界模式，按住<code>shift</code>选择边界进行设置</li><li>设置方程：<code>PDE Specification</code></li><li>网格剖分</li><li>解方程：<code>Solve PDE</code></li><li>绘图：<code>Plot Parameters</code></li><li>输出数值解：<code>Export Solution</code></li></ol><h2 id="偏微分方程模型"><a href="#偏微分方程模型" class="headerlink" title="偏微分方程模型"></a>偏微分方程模型</h2><ol><li>椭圆方程<ul><li>散射问题</li><li>最小曲面问题（泛函极值问题）：利用变分极值转化为非线性偏微分方程</li><li>非线性椭圆型方程</li></ul></li><li>抛物型方程<ul><li>热传导方程</li></ul></li><li>双曲型方程<ul><li>波动方程<h2 id="有限差分"><a href="#有限差分" class="headerlink" title="有限差分"></a>有限差分</h2>核心：用差商代替微商</li></ul></li></ol><p><strong>对于椭圆型：</strong></p><ul><li>五点差分格式</li><li>九点差分格式</li><li>极坐标下的差分格式</li></ul><p><strong>对于抛物型：</strong></p><ol><li><p>常系数扩散方程</p><ul><li>1.1 差分格式</li><li>1.2 加权格式</li><li>1.3 三层格式</li><li>1.4 跳点格式</li></ul></li><li>初值问题<ul><li>Saul‘ev算法</li><li>分组显示方法</li></ul></li><li>对流扩散方程<ul><li>修正中心显格式</li><li>迎风差分格式</li><li>Samarskii格式</li><li>指数型差分格式</li><li>隐式格式</li><li>特征差分格式</li></ul></li><li>变系数方程<ul><li>Taylor展开法</li><li>Keller盒式格式</li></ul></li><li>多维问题<ul><li>交替方向隐式格式</li><li>局部一维格式</li><li>预测-校正格式</li><li>跳点格式</li></ul></li><li>非线性方程<ul><li>Richtmyer线性化方法</li><li>拟线性扩散方程的隐式格式</li><li>三层格式</li><li>预测-校正法<br><strong>对于双曲型：</strong></li></ul></li></ol><p>有限元法如下：</p><p><img src="https://imgkr2.cn-bj.ufileos.com/abb604ee-8a69-4146-a675-107ff61f1314.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=dzxISHcvgn1cWgTxJpMySKhIqmA%253D&amp;Expires=1596254814" alt=""></p><p>有限差分法如下：</p><p><img src="https://imgkr2.cn-bj.ufileos.com/9c568a22-9f72-40c2-a319-3e7c1927e746.png?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&amp;Signature=9LF02XAVWrCIjyXTxRX4aTYX0xg%253D&amp;Expires=1596254909" alt=""></p><pre><code class="lang-matlab">v0=100;%上边界hx=17;hy=10;%确定网格步长上下界v1=zeros(hy,hx);v1(hy,:)=ones(1,hx)*v0;%上边界v1(2:hy-1,2:hx-1)=ones(hy-2,hx-2);%中间的初始化v2=zeros(hy,hx);maxt=1;t=0;%初始化v2(hy,:)=v1(hy,:);%上边界while (maxt&gt;0.1)%由v1迭代，算出v2，迭代精度为0.1    for i=2:hy-1        for j=2:hx-1            v2(i,j)=(v1(i,j-1)+v1(i,j+1)+v1(i-1,j)+...                v1(i+1,j))/4;%五点格式差分法            t=v2(i,j)-v1(i,j);            maxt=0;            if(t&gt;maxt)                 maxt=t;             end        end    end    v1=v2;endsubplot(1,2,1),surf(v2);axis([0,17,0,10,0,100])subplot(122)contour(v2);%画等值线hold on;%保屏x=1:hx;y=1:hy;[xx,yy]=meshgrid(x,y);%形成栅格[Gx,Gy]=gradient(v2,0.6,0.6);%计算梯度quiver(xx,yy,Gx,Gy,&#39;r&#39;);%画矢量图hold off</code></pre><h2 id="椭圆型"><a href="#椭圆型" class="headerlink" title="椭圆型"></a>椭圆型</h2><h3 id="五点差分格式"><a href="#五点差分格式" class="headerlink" title="五点差分格式"></a>五点差分格式</h3><p><a href="https://www.cnblogs.com/xtu-hudongdong/p/6512071.html" target="_blank" rel="noopener">五点差分格式</a></p><p>未完待续。。。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 偏微分方程 </tag>
            
            <tag> 数值分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常微分方程数值解法</title>
      <link href="/posts/3745673b.html"/>
      <url>/posts/3745673b.html</url>
      
        <content type="html"><![CDATA[<h1 id="微分方程数值解"><a href="#微分方程数值解" class="headerlink" title="微分方程数值解"></a>微分方程数值解</h1><p>核心思想：用差分代替微分</p><h2 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h2><p>eg1:<script type="math/tex">\begin{equation}\begin{cases}    y'=y-\frac{2x}{y},   0<x<1 \\\\    y\left( 0 \right) =1 \\\end{cases}\end{equation}</script></p><h3 id="欧拉法"><a href="#欧拉法" class="headerlink" title="欧拉法"></a>欧拉法</h3><p>迭代方程：</p><script type="math/tex; mode=display">y_{n+1}=y_n+hf\left( x_n,y_n \right)</script><p>将例题1微分方程转化为迭代方程：</p><script type="math/tex; mode=display">y_{n+1}=y_n+h\left( y_n-\frac{2x_n}{y_n} \right)</script><pre><code class="lang-matlab">function yn1=myfun(xn,yn,h)yn1=yn+h*(yn-2*xn/yn);end</code></pre><p>直接一个循环就出来了：</p><pre><code class="lang-matlab">clc;clear;yn=1;%设置初值xn=0;%设置起点Xs=1;%设置终点h=0.1;%设置步长x(1)=xn;y(1)=yn;%用于存储变量与函数值i=1;while xn&lt;Xs             %当xn达到研究区间上限结束    yn=myfun(xn,yn,h);    xn=xn+h;    i=i+1;    x(i)=xn;y(i)=yn;endplot(x,y,x,sqrt(1+2*x));xlabel(&#39;自变量x&#39;)ylabel(&#39;函数值y&#39;)legend(&#39;欧拉法&#39;,&#39;实际值&#39;);figure;plot(x,y-sqrt(1+2*x))legend(&#39;误差值&#39;)</code></pre><p>结果如下：</p><p><img src="https://imgkr.cn-bj.ufileos.com/5cd76778-16b1-4ee9-9bcc-9b20df5e89c2.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/4babbcf7-180b-44b9-b016-349078ad9708.png" alt=""></p><h3 id="改进欧拉法"><a href="#改进欧拉法" class="headerlink" title="改进欧拉法"></a>改进欧拉法</h3><p>改进欧拉法实则首先用欧拉法进行预测估计：</p><script type="math/tex; mode=display">\bar{y}_{n+1}=y_n+hf\left( x_n,y_n \right)</script><p>再利用梯形公式估计：</p><script type="math/tex; mode=display">y_{n+1}=y_n+\frac{h}{2}\left[ f\left( x_n,y_n \right) +f\left( x_{n+1},\bar{y}_{n+1} \right) \right]</script><p>转化为平均化形式：</p><script type="math/tex; mode=display">\begin{cases}    y_p=y_n+hf\left( x_n,y_n \right)\\    y_c=y_n+hf\left( x_{n+1},y_p \right)\\    y_{n+1}=\frac{1}{2}\left( y_p+y_c \right)\\\end{cases}</script><p>相比于欧拉法只是迭代方程产生区别：</p><pre><code class="lang-matlab">function yn1=myfun(xn,yn,h)yp=yn+h*(yn-2*xn/yn);yc=yn+h*(yn-2*(xn+h)/yp);yn1=1/2*(yp+yc);end</code></pre><p><img src="https://imgkr.cn-bj.ufileos.com/6d886a07-aa30-4407-8a7d-960ef870f2a7.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/327b3ccd-9c93-4e9a-af27-143f3f64e129.png" alt=""><br>可以看出确实改善了求解精度</p><h3 id="显式R-K方法"><a href="#显式R-K方法" class="headerlink" title="显式R-K方法"></a>显式R-K方法</h3><p>R-K方法是对欧拉方法的推广，只是将：</p><script type="math/tex; mode=display">\varDelta y=y_{n+1}-y_n=\int_{x_n}^{x_{n+1}}{f\left( x,y\left( x \right) \right) dx}\approx h\sum_{i=1}^r{c_if\left( x_n+\lambda _ih,y\left( x_n+\lambda _ih \right) \right)}</script><p>积分的精度调高了，为了保证精度和减少计算，一般采用四阶龙格库塔法，通过待定系数可以求出四阶R-K方程模型：</p><script type="math/tex; mode=display">\begin{cases}    y_{n+1}=y_n+\frac{h}{6}\left( K_1+2K_2+2K_3+K_4 \right)\\    K_1=f\left( x_n,y_n \right)\\    K_2=f\left( x_n+\frac{h}{2},y_n+\frac{h}{2}K_1 \right)\\    K_3=f\left( x_n+\frac{h}{2},y_n+\frac{h}{2}K_2 \right)\\    K_4=f\left( x_n+h,y_n+hK_3 \right)\\\end{cases}</script><p><strong><em>注意：R-K法是基于泰勒展开的方法，要求解具有较好的光滑性质，如果解得光滑性差，使用四阶龙格库塔求得的结果效果反而不如改进的欧拉方法，建模时需要对不同问题进行判断使用</em></strong></p><p>结果：</p><p><img src="https://imgkr.cn-bj.ufileos.com/702ff152-0d62-4d03-8a24-4cf844841024.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/19239303-0579-45b1-a4a6-91c1b42eb9bd.png" alt=""></p><p>其截断误差是:</p><script type="math/tex; mode=display">o(h^5)</script><h3 id="刚性方程组"><a href="#刚性方程组" class="headerlink" title="刚性方程组"></a>刚性方程组</h3><p>eg2：</p><script type="math/tex; mode=display">\begin{cases}    u'=-1000.25u+999.75v+0.5\\    v'=999.75u-1000.25v+0.5\\    u\left( 0 \right) =1\\    v\left( 0 \right) =-1\\\end{cases}</script><p>利用龙格库塔求出结果：</p><p><img src="https://imgkr.cn-bj.ufileos.com/e10aa399-f659-4197-8d2d-e5fa53690154.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/ebadbaa7-d6da-460e-a4c4-958a870369f5.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/eba2635a-a5b0-49da-bbc6-713c24a0b37b.png" alt=""></p><p><img src="https://imgkr.cn-bj.ufileos.com/de66e439-f2b0-4b5c-9809-dd3d841ef6d1.png" alt=""></p><p>我针对特殊情况总结了一下流程：</p><blockquote><p>首先对方程组进行降阶处理转化为一阶n元微分方程组：</p><script type="math/tex; mode=display">\begin{cases}    y_{1}^{'}=f_1\left( x,y_1,y_2\cdots y_N \right)\\    y_{2}^{'}=f_2\left( x,y_1,y_2\cdots y_N \right)\\    \vdots\\    y_{N}^{'}=f_N\left( x,y_1,y_2\cdots y_N \right)\\    y_1\left( x_0 \right) =a_1\\    y_2\left( x_0 \right) =a_2\\    \vdots\\    y_N\left( x_0 \right) =a_N\\\end{cases}</script><p>其次根据R-K公式得到：</p><script type="math/tex; mode=display">\begin{cases}    y_{n+1}^{1}=y_{n}^{1}+\frac{h}{6}\left( K_{1}^{1}+2K_{2}^{1}+2K_{3}^{1}+K_{4}^{1} \right)\\    y_{n+1}^{2}=y_{n}^{2}+\frac{h}{6}\left( K_{1}^{2}+2K_{2}^{2}+2K_{3}^{2}+K_{4}^{2} \right)\\    \vdots\\    y_{n+1}^{N}=y_{n}^{N}+\frac{h}{6}\left( K_{1}^{N}+2K_{2}^{N}+2K_{3}^{N}+K_{4}^{N} \right)\\\end{cases}</script><p>其中：</p><script type="math/tex; mode=display">\begin{cases}    K_{1}^{i}=f_i\left( x,y_1,y_2\cdots y_N \right)\\    K_{2}^{i}=f_i\left( x+\frac{h}{2},y_1+\frac{h}{2}K_{1}^{1},y_2+\frac{h}{2}K_{1}^{2},\cdots ,y_N+\frac{h}{2}K_{1}^{N} \right)\\    K_{3}^{i}=f_i\left( x+\frac{h}{2},y_1+\frac{h}{2}K_{2}^{1},y_2+\frac{h}{2}K_{2}^{2},\cdots ,y_N+\frac{h}{2}K_{2}^{N} \right)\\    K_{4}^{i}=f_i\left( x+h,y_1+hK_{3}^{1},y_2+hK_{3}^{2},\cdots ,y_N+hK_{3}^{N} \right)\\\end{cases}</script><p>即可</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 微分方程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 微分方程 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>混沌优化算法</title>
      <link href="/posts/3c1f6d69.html"/>
      <url>/posts/3c1f6d69.html</url>
      
        <content type="html"><![CDATA[<h1 id="混沌优化算法"><a href="#混沌优化算法" class="headerlink" title="混沌优化算法"></a>混沌优化算法</h1><p>基于Logistic映射产生混沌运动轨道的遍历性，可将其混沌用于函数优化问题：<br>混沌优化算法的基本思想是将变量从混沌空间变换到解空间，然后利用混沌变量所具有的丰富的非线性动力学特征————随机性、遍历性、规律性的特点进行搜索。混沌优化算法易跳出局部最优解，不需要优化问题具有连续性和可微性。</p><h2 id="变尺度混沌优化算法"><a href="#变尺度混沌优化算法" class="headerlink" title="变尺度混沌优化算法"></a>变尺度混沌优化算法</h2><p>以求目标函数：</p><script type="math/tex; mode=display">\max f\left( x \right) =1-\left( 2x_{1}^{2}+2x_{2}^{2}+x_{3}^{2}+2x_1x_2+2x_1x_3-8x_1-6x_2-4x_3+9 \right)</script><p>带条件约束：</p><script type="math/tex; mode=display">\begin{equation}s.t.\begin{cases}    x_1+x_2+2x_3\le 3\\    x_1,x_2,x_3\ge 0\\\end{cases}\end{equation}</script><p>step 1:应用Logistic方程产生混沌变量进行优化搜索：</p><script type="math/tex; mode=display">x_{k+1} = \mu \cdot x_k (1.0 -x_k)</script><p>step 2:随机产生初始为[0,1]区间内n个相异初值<script type="math/tex">x_{0i}</script>，得到n个轨迹不同的混沌变量。计算性能指标<script type="math/tex">f(x_{0i})</script>，<script type="math/tex">x_{0i} =a+x_{0i}(b-a)</script>。</p><p>step 3:根据混沌方程进行迭代，产生新解<script type="math/tex">x_i\in \left[ 0,1 \right]</script>。计算相应的性能指标<script type="math/tex">f(x_{i})，x_{i} =a+x_{i}(b-a)</script>。</p><p>step 4:如果<script type="math/tex">f(x_i) < f(x_{i0})</script>则接受新解<script type="math/tex">x_{i0}=x_i</script>(用于复制运算)</p><p>step 5:随机对新种群中部分解按照概率<script type="math/tex">P_c</script>进行交换运算</p><p>step 6:随机对新种群中部分解按照概率<script type="math/tex">P_m</script>进行变异运算，经过4~6后得到<script type="math/tex">x_{1i} \in \left[ 0,1 \right]</script></p><p>step 7:如果<script type="math/tex">f(x_{1i}) < f(x_{i0})</script>，则接受新解<script type="math/tex">x_{i0}=x_{1i}</script>从中找出性能最好的个体<script type="math/tex">X_c</script>。转跳到步骤3.如果<script type="math/tex">X_c</script>在规定的迭代次数m里没有满足指定的误差要求搜索条件，则进行下一步骤8，否则结束。</p><p>step 8:以上一搜索结果<script type="math/tex">X_c</script>为中心，以<script type="math/tex">\frac{r}{a}</script>为半径，r为前一步的搜索半径，即前一步解空间的半径，a为r的衰减因子。以<script type="math/tex">x_{k+1} = \mu \cdot x_k (1.0 -x_k)</script>为搜索函数，重复步骤3~7，缩小范围进行搜索，直到满足误差要求。最终所得到的<script type="math/tex">X_c,f(X_c)</script>为全局最优。</p><p>注意：若<script type="math/tex">x_i</script>定义域包含负数，则可以通过混沌函数：<script type="math/tex">x_{k+1} = \mu \cdot x^{3}_n - \mu x_n+x_n</script>此时<script type="math/tex">x_i</script>在[-1,1]上遍历，目标函数为<script type="math/tex">f(X),X=ax</script></p><p>代码如下：</p><p>(1)函数定义：</p><pre><code class="lang-matlab">function myfunction=myfunction(x1,x2,x3)myfunction=1-(2*x1^2+2*x2^2+x3^2+2*x1*x2+2*x1*x3-8*x1-6*x2-4*x3+9)end</code></pre><p>（2）设置吸收域：</p><pre><code class="lang-matlab">function myjudge=myjudge(x1,x2,x3)a=-x1-x2-2*x3+3;if x1&gt;0&amp;&amp;x2&gt;0&amp;&amp;x3&gt;0&amp;&amp;a&gt;=0    myjudge=1;else    myjudge=0;end</code></pre><p>(3)主函数：</p><pre><code class="lang-matlab">%参数设置num=3;                                           %混沌变量r=2;for k=1:100for z=1:100    X=rand(num,1);    TempX = r * X;   if myjudge(TempX(1), TempX(2), TempX(3)) == 1   %当TempX变量满足可行域时跳出循环     break   endendMaxX = TempX;MaxF = myfun(MaxX(1), MaxX(2), MaxX(3));  %性能指标for i = 2:5000   for j =1:num                                        %X的每个变量做循环    X(j, i) = 4 * X(j, i - 1) * (1 - X(j, i - 1));   %logistic迭代   TempX(j) = r * X(j, i);                           %搜索半径为2   end   if myjudge(TempX(1), TempX(2), TempX(3))==1              %如果可行且性能更好，更换新解      TempF = myfun(TempX(1), TempX(2), TempX(3));        if TempF &gt; MaxF          MaxX(j) = TempX(j);         MaxF = TempF;        end   endend%二次载波X=rand(num,1);for i = 2:5000                          for j = 1:num    X(j, i) = 4 * X(j, i - 1) * (1 - X(j, i - 1));    endendfor i = 1:5000      TempX = MaxX + 0.0001 * X(:, i);    if myjudge(TempX(1), TempX(2), TempX(3))==1     TempF = myfun(TempX(1), TempX(2), TempX(3));        if TempF &gt; MaxF        MaxX(j) = TempX(j);        MaxF = TempF;       end    end endMaxF = vpa(MaxF, 4);                        %保留四位有效数字for i = 1:numMaxX(i) = vpa(MaxX(i), 4);endMax(k)=MaxF;endsz=subs(Max);[m,n]=max(sz);B=Max(n);%最大值X=MaxX;%解</code></pre>]]></content>
      
      
      <categories>
          
          <category> MATLAB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> 智能优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCT图像处理</title>
      <link href="/posts/72e90ff1.html"/>
      <url>/posts/72e90ff1.html</url>
      
        <content type="html"><![CDATA[<h2 id="图像处理————傅里叶变换"><a href="#图像处理————傅里叶变换" class="headerlink" title="图像处理————傅里叶变换"></a>图像处理————傅里叶变换</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><p>对于数字图像来说，它的储存方式主要为二维矩阵，因此引进连续二元函数的二维傅里叶变换及反变换定义(学过傅里叶变换的看应该很容易理解，就不细讲)：</p><script type="math/tex; mode=display">\begin{array}{l}F(u, v)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) \mathrm{e}^{-\mathrm{j} 2 \pi(u x+v y)} \mathrm{d} x \mathrm{d} y \\f(x, y)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} F(u, v) \mathrm{e}^{\mathrm{j} 2 \pi(u x+v y)} \mathrm{d} u \mathrm{d} v\end{array}</script><p>由于计算机对数字图像的储存为离散形式，因此得到离散形式的傅里叶变换与逆变换：</p><script type="math/tex; mode=display">\begin{array}{l}F(u, v)=\sum\limits_{m=0}^{M-1} \sum\limits_{n=0}^{N-1} f(m, n) \mathrm{e}^{-\mathrm{j} 2 \pi u m} \mathrm{e}^{-\mathrm{j} 2 \pi v n}, \quad m=0,1, \cdots, M-1, n=0,1, \cdots, N-1 \\f(m, n)=\sum\limits_{m=0}^{M-1} \sum\limits_{n=0}^{N-1} F(u, v) \mathrm{e}^{\mathrm{j} 2 \pi u m} \mathrm{e}^{\mathrm{j} 2 \pi v n}, \quad u=0,1, \cdots, M-1, v=0,1, \cdots, N-1\end{array}</script><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="二维离散傅里叶变换fft2"><a href="#二维离散傅里叶变换fft2" class="headerlink" title="二维离散傅里叶变换fft2"></a>二维离散傅里叶变换fft2</h3><pre><code>Y = fft2(X)Y = fft2(X , m, n)</code></pre><p>其中$X$为需变换的矩阵，$m,n$为返回$Y$的行数和列数，若$m,n$大于$X$的维数，则在$Y$相应的位置补0</p><h3 id="二维离散傅里叶逆变换ifft2"><a href="#二维离散傅里叶逆变换ifft2" class="headerlink" title="二维离散傅里叶逆变换ifft2"></a>二维离散傅里叶逆变换ifft2</h3><pre><code>Y = ifft2(X)Y = ifft2(X , m, n)</code></pre><p>其中$X$为需变换的矩阵，$m,n$为返回$Y$的行数和列数，若$m,n$大于$X$的维数，则在$Y$相应的位置补0</p><h3 id="n-维离散傅里叶变换fftn"><a href="#n-维离散傅里叶变换fftn" class="headerlink" title="$n$维离散傅里叶变换fftn"></a>$n$维离散傅里叶变换fftn</h3><pre><code>Y = fftn(X)Y = fftn(X , m, n)</code></pre><p>其中$X$为需变换的矩阵，$m,n$为返回$Y$的行数和列数，若$m,n$大于$X$的维数，则在$Y$相应的位置补0</p><h3 id="n维离散傅里叶逆变换ifftn"><a href="#n维离散傅里叶逆变换ifftn" class="headerlink" title="n维离散傅里叶逆变换ifftn"></a>n维离散傅里叶逆变换ifftn</h3><pre><code>Y = fft2(X)Y = fft2(X , m, n)</code></pre><p>其中$X$为需变换的矩阵，$m,n$为返回$Y$的行数和列数，若$m,n$大于$X$的维数，则在$Y$相应的位置补0</p><h3 id="将零频分量移到频谱中心fftshift"><a href="#将零频分量移到频谱中心fftshift" class="headerlink" title="将零频分量移到频谱中心fftshift"></a>将零频分量移到频谱中心fftshift</h3><p>通过将零频分量移动到数组中心，重新排列傅里叶变换 $X$，利于观察频谱。</p><pre><code>Y = fftshift(X)Y = fftshift(X,dim)</code></pre><p><img src="https://imgkr.cn-bj.ufileos.com/e2720805-54a5-45c6-b3cc-060db8102b37.png" alt=""></p><h3 id="频率响应freqz2-h"><a href="#频率响应freqz2-h" class="headerlink" title="频率响应freqz2(h)"></a>频率响应freqz2(h)</h3><p>freqz2用于求解频率响应，下图为高斯滤波器处理下的频响图<br><img src="https://imgkr.cn-bj.ufileos.com/db7ef4a4-4794-4e24-b625-316e373eb8ae.png" alt=""></p><h2 id="离散余弦变换DCT"><a href="#离散余弦变换DCT" class="headerlink" title="离散余弦变换DCT"></a>离散余弦变换DCT</h2><p>离散余弦变换DCT是图像压缩的一种常见的变换，它将图像表示为具有不同振幅和频率的正弦曲线和。类似于离散傅里叶变换，他利用傅里叶变换的对称性将图像变化成偶函数的形式，然后进行二维傅里叶变换，变换的结果仅包含余弦项，因此称为离散余弦变换。<br>我们可以根据离散余弦变换的几个系数表示出图像的大部分信息，可以用于图像的压缩。</p><h3 id="离散余弦变换DCT-1"><a href="#离散余弦变换DCT-1" class="headerlink" title="离散余弦变换DCT"></a>离散余弦变换DCT</h3><p>在图像处理中，我们经常利用二维DCT变换，而二维是建立在一维的基础之上，因此了解一维很重要，维基百科上给出八种常见形式，摘选出其中一种：</p><script type="math/tex; mode=display">f_{p}=\sum_{m=0}^{n-1} x_{m} \cos \left[\frac{\pi}{n} p\left(m+\frac{1}{2}\right)\right]</script><h3 id="二维DCT定义"><a href="#二维DCT定义" class="headerlink" title="二维DCT定义"></a>二维DCT定义</h3><p>对于一个$M\times N$的矩阵$A$，其DCT变换为(通过一维定义可以看出，严格证明此处省略):</p><script type="math/tex; mode=display">\begin{array}{l}B_{p q}=\alpha_{p} \alpha_{u} \sum\limits_{m=0}^{M-1} \sum\limits_{n=0}^{N-1} A_{m n} \cos \frac{\pi(2 m+1) p}{2 M} \cos \frac{\pi(2 n+1) q}{2 N}, 0 \leqslant p \leqslant M-1,0 \leqslant q \leqslant N-1 \\\alpha_{p}=\left\{\begin{array}{l}1 / \sqrt{M}, p=0 \\\sqrt{2 / M}, 1 \leqslant p \leqslant M-1\end{array}, \alpha_{q}=\left\{\begin{array}{l}1 / \sqrt{N}, q=0 \\\sqrt{2 / N}, 1 \leqslant q \leqslant N-1\end{array}\right.\right.\end{array}</script><p>这里的$B_{pq}$可以看做是基函数的权重</p><h3 id="DCT变换矩阵"><a href="#DCT变换矩阵" class="headerlink" title="DCT变换矩阵"></a>DCT变换矩阵</h3><p>计算DCT一般有两种方法：</p><ul><li>使用dct2()函数实现快速傅里叶变换<pre><code>B = dct2(A)B = dct2(A,m,n)B = dct2(A,[m n])</code></pre></li><li>使用DCT变换矩阵由函数dctmtx()返回<h3 id="DCT图像变换"><a href="#DCT图像变换" class="headerlink" title="DCT图像变换"></a>DCT图像变换</h3>DCT算法进行压缩即进行DCT变换后在进行逆变换。其中DCT系数：<br><img src="https://imgkr.cn-bj.ufileos.com/0bb1d0d6-fdac-48f2-9580-aadad72e01cb.png" alt=""><br>变换前后两张图为：</li></ul><p><img src="https://imgkr.cn-bj.ufileos.com/26f7d959-3422-4d70-9674-c6522fc1ce01.png" alt=""></p><p>代码如下：</p><pre><code>RGB = imread(&#39;公众号.jpg&#39;);I = rgb2gray(RGB);J = dct2(I);imshow(log(abs(J)),[]);colormap(jet(64));colorbarJ(abs(J) &lt; 10) =0;K = idct2(J);subplot(121),imshow(I);subplot(122),imshow(K,[0 255]);</code></pre>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> DCT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离散&amp;连续联合概率密度</title>
      <link href="/posts/d556bbf2.html"/>
      <url>/posts/d556bbf2.html</url>
      
        <content type="html"><![CDATA[<h2 id="连续与离散联合概率密度"><a href="#连续与离散联合概率密度" class="headerlink" title="连续与离散联合概率密度"></a>连续与离散联合概率密度</h2><p>在概率论中，教材介绍的随机变量都是连续或离散的联合概率密度，讨论了$Z=X+Y$,当$X,Y$都是离散型或都是连续型时概率分布的计算方法，下面按照我的理解方式来举例说明如何求解：</p><h3 id="设-X-U-0-1-Y-服从-0-1-分布，且-X-Y-相互独立，求-Z-的概率密度。"><a href="#设-X-U-0-1-Y-服从-0-1-分布，且-X-Y-相互独立，求-Z-的概率密度。" class="headerlink" title="设$X$~$U(0,1), Y$服从$0-1$分布，且$X,Y$相互独立，求$Z$的概率密度。"></a>设$X$~$U(0,1), Y$服从$0-1$分布，且$X,Y$相互独立，求$Z$的概率密度。</h3><p>方法一：常规计算(较麻烦)</p><script type="math/tex; mode=display">F_Z\left( z \right) =P\left\{ Z\le z \right\} =P\left\{ X+Y\le z \right\} =P\left\{ X\le z \right\} \left( 1-p \right) +P\left\{ X\le z-1 \right\} p</script><script type="math/tex; mode=display">=\begin{equation}\begin{cases}    0              \,\,\,\,\,           z\le 0\\    z\left( 1-p \right) \,\,\,\,\,\,            0<z\le  1\\    1-p+\left( z-1 \right) p \,\,\,\,\,\, 1<z\le  2\\    1                          z>2\\\end{cases}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}f_Z\left( z \right)=\begin{cases}    0     \,\,\,\,\,         z\le  0\\    1-p \,\,\,\,\,     0<z\le  1\\    p \,\,\,\,\,        1<z\le  2\\    0      \,\,\,\,\,         z>2\\\end{cases}\end{equation}</script><p>方法二:自定义冲激函数</p><script type="math/tex; mode=display">\begin{equation}\delta \left( t \right) =\begin{cases}    1 \,\,\,\,\,   t=0\\    0 \,\,\,\,\,  t\ne 0\\\end{cases}\end{equation}</script><p>( 注:这是我自定义，实际上冲激函数在$0$处冲激为无穷，冲激序列在$0$处冲激为1) </p><p>且有阶跃函数</p><script type="math/tex; mode=display">\begin{equation}\varepsilon \left( t \right) =\begin{cases}    1 \,\,\,\,\,  t>0\\    0 \,\,\,\,\,  t<0\\\end{cases}\end{equation}</script><p>利用阶跃函数卷积性质，易知</p><script type="math/tex; mode=display">\begin{equation}F_Y\left( y \right) =\begin{cases}    0    \,\,\,\,\,       y<0\\    1-p \,\,\,\,\,  0\le  y<1\\    1      \,\,\,\,\,     y\ge 1\\\end{cases}\end{equation}</script><script type="math/tex; mode=display">f_Z\left( z \right) =\left[ f_Y\left( z \right) \right] *\left[ \varepsilon \left( z \right) -\varepsilon \left( z-1 \right) \right]</script><script type="math/tex; mode=display">=F_Y\left( z \right) -F_Y\left( z-1 \right)</script><script type="math/tex; mode=display">\begin{equation}=\begin{cases}    0     \,\,\,\,\,      z<0\\    1-p \,\,\,\,\,  0\le  z<1\\    1      \,\,\,\,\,     z\ge  1\\\end{cases}-\begin{cases}    0      \,\,\,\,\,     z<1\\    1-p \,\,\,\,\,  1\le  z<2\\    1    \,\,\,\,\,       z\ge  2\\\end{cases}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}=\begin{cases}    0      \,\,\,\,\,        z \le  0\\    1-p \,\,\,\,\,     0<z \le 1\\    p  \,\,\,\,\,        1<z \le 2\\    0        \,\,\,\,\,       z>2\\\end{cases}\end{equation}</script><p><img src="https://imgkr.cn-bj.ufileos.com/641d7bf1-17db-410a-86a8-46941b33a252.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 联合概率密度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX与Turtle</title>
      <link href="/posts/a73c3196.html"/>
      <url>/posts/a73c3196.html</url>
      
        <content type="html"><![CDATA[<h2 id="turtle绘图笔记及LaTex示例"><a href="#turtle绘图笔记及LaTex示例" class="headerlink" title="turtle绘图笔记及LaTex示例"></a>turtle绘图笔记及LaTex示例</h2><h3 id="雨中漫步"><a href="#雨中漫步" class="headerlink" title="雨中漫步"></a>雨中漫步</h3><p>之前在公众号介绍过turtle绘图笔记<a href="https://mp.weixin.qq.com/s?__biz=MzU5OTg4NjkxNg==&amp;mid=2247484459&amp;idx=1&amp;sn=f29a2eef1b89999f444acb711835c095&amp;chksm=feaf5900c9d8d01655377f67df504aacabbcaaf324f37256215c6c93012a43fb46e7f0547733&amp;token=1232526020&amp;lang=zh_CN#rd" target="_blank" rel="noopener">turtle绘图笔记及LaTex示例</a><br>说到五角星不得不提基于命令行界面绘图tikz的实现，下面是源于latex工作室的开源代码：</p><h4 id="LaTex实现"><a href="#LaTex实现" class="headerlink" title="LaTex实现"></a>LaTex实现</h4><pre><code>\documentclass[tikz, svgnames]{standalone}\usepackage{tikz, verbatim}\usetikzlibrary{shapes.geometric}\def\largo{30}\def\alto{\largo/1.5}\definecolor{Fire Engine Red}{HTML}{CF142B}\definecolor{Gorse}{HTML}{FAE042}\tikzstyle{estrella}=[fill=Gorse, star point ratio=2.617, minimum size=2 cm]\begin{document}\begin{tikzpicture}    \fill[Fire Engine Red] rectangle (\largo, \alto);     \node[star,fill=Gorse, minimum size=6 cm, rotate=0, star point ratio=2.617] at (5,15) {};     \foreach \x/\y/\z in {10/18/50, 12/16/25, 12/13/0, 10/11/50}{    \node[star, estrella, rotate=\z] at (\x, \y){};    }\end{tikzpicture}\end{document}</code></pre><p>利用turtle也是可以完美解决，具体案例很多，可以去官网看看：</p><pre><code>import turtle as tt.setup(600, 400,0,0)t.color(&#39;yellow&#39;,&#39;yellow&#39;)t.bgcolor(&#39;red&#39;)def mygoto(x, y):    t.up()    t.goto(x, y)    t.down()def star(r):    t.begin_fill()    for i in range(5):        t.forward(r)        t.left(-144)    t.end_fill()mygoto(-230, 100)star(100)for i in range(4):    x = 1    if i in [0, 3]:        x = 0    mygoto(-120 + x * 50, 150 - i * 40)    t.left(15 - i * 15)    star(30)mygoto(0, 0)t.hideturtle()t.done()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 绘图 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 绘图 </tag>
            
            <tag> LaTeX </tag>
            
            <tag> Turtle </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>傅里叶</title>
      <link href="/posts/682b73b0.html"/>
      <url>/posts/682b73b0.html</url>
      
        <content type="html"><![CDATA[<h2 id="DFS，DTFT，DFT，FFT（一）"><a href="#DFS，DTFT，DFT，FFT（一）" class="headerlink" title="DFS，DTFT，DFT，FFT（一）"></a>DFS，DTFT，DFT，FFT（一）</h2><h3 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h3><h6 id="对于离散序列，我们首先得研究出标准序列的完备性正交性："><a href="#对于离散序列，我们首先得研究出标准序列的完备性正交性：" class="headerlink" title="对于离散序列，我们首先得研究出标准序列的完备性正交性："></a>对于离散序列，我们首先得研究出标准序列的完备性正交性：</h6><p>这里我们使用正交集$\phi _i$:</p><script type="math/tex; mode=display">\phi _i=\left( 1,e^{ji\frac{2\pi}{N}},\cdots ,e^{ji\frac{2\pi}{N}\left( N-1 \right)} \right) ^T</script><p>求出$\phi _l$与$\phi _s$内积</p><script type="math/tex; mode=display">( \phi _l,\phi _s ) =\sum_{n=< N>}{e^{j( \frac{2\pi}{N} ) ( l-s ) n}}</script><p>由于$0\leqslant l,s &lt;N $</p><script type="math/tex; mode=display">\because -\left( N-1 \right) \leqslant l-s\leqslant N-1</script><script type="math/tex; mode=display">\therefore -1<\frac{l-s}{N}<1</script><p>$\therefore l\ne s$时,我们有：</p><script type="math/tex; mode=display">e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right)}\ne 1</script><p>$\because e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right) N}=0$</p><p>$\therefore \left( \phi _l,\phi _s \right) =\sum_{n=&lt; N&gt;}{e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right) n}}$</p><script type="math/tex; mode=display">=\frac{1-e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right) n}}{1-e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right)}}=0</script><p>$l=s$时，即$e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right)}=1$</p><script type="math/tex; mode=display">\therefore \left( \phi _l,\phi _s \right)=\sum_{n=< N>}{e^{j\left( \frac{2\pi}{N} \right) \left( l-s \right) n}}=N</script><p>$\therefore \sum\limits_{n=&lt; N&gt;}{e^{jk\left( \frac{2\pi}{N} \right) n}}$是完备正交序列，且完备性为：</p><script type="math/tex; mode=display">\begin{cases}    N\,\,  k=\lambda N,\lambda 为\text{整数}\\    0    k=other\\\end{cases}</script><p>因此我们可以将周期为$N$的离散信号$x[n]$分解为有限$N$项完备正交序列:</p><script type="math/tex; mode=display">x\left[ n \right] =\sum_{k=< N>}{a_ke^{jk\left( \frac{2\pi}{N} \right) n}}</script><p>为了求出离散傅里叶系数，利用其正交性，上式同时乘$e^{-j\left( \frac{2\pi}{N} \right) mn}$，再进行求和。</p><script type="math/tex; mode=display">\sum_{n=< N>}{}{x\left[ n \right] e^{-j\left( \frac{2\pi}{N} \right) mn}}</script><script type="math/tex; mode=display">=\sum_{n=< N>}{}{\sum_{k=< N>}{}{a_k}e^{j\left( k-m \right) \left( \frac{2\pi}{N} \right) n}}</script><p>利用正交性即得到系数：</p><script type="math/tex; mode=display">a_m=\frac{1}{N}\sum_{n=< N>}{x\left[ n \right]}e^{-jm\left( \frac{2\pi}{N} \right) n}</script><p>这样我们就得到离散周期信号傅里叶级数$(DFS)$的结果：</p><script type="math/tex; mode=display">x\left[ n \right] =\sum_{k=< N>}{a_ke^{jk\left( \frac{2\pi}{N} \right) n}}</script><script type="math/tex; mode=display">a_k=\frac{1}{N}\sum_{n=< N>}{x\left[ n \right] e^{-jk\left( \frac{2\pi}{N} \right) n}}</script><p>对于有限长信号，我们可以将其长度延拓为周期$N$，从而得到离散信号$DFT$变换</p><div class="table-container"><table><thead><tr><th>类型</th><th>时间函数</th><th>频率函数</th><th>关系</th></tr></thead><tbody><tr><td>傅里叶级数$FS$</td><td>连续周期$T_0$</td><td>非周期离散$\varOmega _0$</td><td>$\varOmega _0=\frac{2\pi}{T_0}$</td></tr><tr><td>傅里叶变换$FT$</td><td>连续非周期</td><td>连续非周期</td><td>$NULL$</td></tr><tr><td>离散时间傅里叶变换$DTFT$</td><td>离散$N$非周期</td><td>连续周期$\varOmega$</td><td>$\varOmega=\frac{2\pi}{N}$</td></tr><tr><td>离散傅里叶变换$DFT$</td><td>离散$T_s$周期$T_0$</td><td>周期$\varOmega_s$离散$\varOmega_0$</td><td>$\varOmega _0=\frac{2\pi}{T_0}$,$\varOmega _s=\frac{2\pi}{T_s}$</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 数字信号处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字信号处理 </tag>
            
            <tag> 傅里叶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>π节</title>
      <link href="/posts/25152df9.html"/>
      <url>/posts/25152df9.html</url>
      
        <content type="html"><![CDATA[<h2 id="pi-的魅力："><a href="#pi-的魅力：" class="headerlink" title="$\pi$的魅力："></a>$\pi$的魅力：</h2><p>  今天是3.14，15点，正是圆周率π的前6位数字，由于探索π的漫长而又奇妙，从到一块古巴比伦石匾到割圆术再到最后π的级数展开，中间经历无数代数学家或爱好者的努力研究，下面就让我来介绍一些具代表性的。</p><h6 id="最美公式之一-———欧拉公式"><a href="#最美公式之一-———欧拉公式" class="headerlink" title="最美公式之一 ———欧拉公式"></a>最美公式之一 ———欧拉公式</h6><script type="math/tex; mode=display">e^{i\pi}+1=0</script><p>将$i,0,1,e,\pi$将最简单的虚数实数两个最具代表性无理数都包含进来。当然也开启了复分析的研究，可谓“天堑变通途”</p><h6 id="pi-的连分式展开"><a href="#pi-的连分式展开" class="headerlink" title="$\pi$的连分式展开"></a>$\pi$的连分式展开</h6><script type="math/tex; mode=display">\pi =\frac{4}{1+\frac{1^2}{2+\frac{3^2}{2+\frac{5^2}{2+\frac{7^2}{\cdots}}}}}</script><p>是不是很神奇呀</p><h6 id="pi-的展开"><a href="#pi-的展开" class="headerlink" title="$\pi$的展开"></a>$\pi$的展开</h6><script type="math/tex; mode=display">\frac{2}{\pi}=\sqrt{\frac{1}{2}}\cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}}\cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}}}\cdots</script><p>玩大学数学竞赛的同学肯定不陌生，这个叫啥公式我也忘了，证明也很巧妙但是都成经典手段：</p><h6 id="巴塞尔问题——zeta函数"><a href="#巴塞尔问题——zeta函数" class="headerlink" title="巴塞尔问题——zeta函数"></a>巴塞尔问题——zeta函数</h6><script type="math/tex; mode=display">\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots +\frac{1}{n^2}\cdots =\frac{\pi ^2}{6}</script><p>额，这个很出名吧！喜欢数学的人一定会被欧拉大神的思维惊艳（可能现在看来解法很多，但是当时那个年代，欧拉的著作是极具创造力的，而创造和创新能力上正是数学家与我们这些业余爱好者差距最大的地方，向欧拉大神致敬。</p><h6 id="pi-的泰勒展开"><a href="#pi-的泰勒展开" class="headerlink" title="$\pi$的泰勒展开"></a>$\pi$的泰勒展开</h6><script type="math/tex; mode=display">1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}\cdots =\frac{\pi}{4}</script><p>这个高等数学的知识，很容易吧！$arctanx$展开就出来了，不过现在技术根本不用它来计算$\pi$收敛速度太慢了。</p><h6 id="拉马努金恒等式"><a href="#拉马努金恒等式" class="headerlink" title="拉马努金恒等式"></a>拉马努金恒等式</h6><script type="math/tex; mode=display">2\sqrt{2}\sum_{k=0}^{\infty}{\frac{\left( 4k \right) !}{\left( k! \right) ^4}}\frac{1103+26390k}{396^{4k}}=\frac{99^2}{\pi}</script><p>这么鬼畜的等式，是人想象出来的？是”娜玛卡女神在梦中用公式向他的启示”，也就拉马努金最有直觉的数学家可以想出来吧！呜呜呜，看着大神的公式，感觉自己智商无穷小。</p><h6 id="wallis-公式"><a href="#wallis-公式" class="headerlink" title="$wallis$公式"></a>$wallis$公式</h6><script type="math/tex; mode=display">\frac{\pi}{2}=\frac{2\times 2\times 4\times 4\times 6\times 6\times 8\cdots}{1\times 3\times 3\times 5\times 5\times 7\times 7\cdots}</script><p>也是一个很神奇的公式，由$wallis$发现，大学生做高数经常用衍生出来的点火公式。</p><h6 id="斯特林公式"><a href="#斯特林公式" class="headerlink" title="斯特林公式"></a>斯特林公式</h6><script type="math/tex; mode=display">\lim_{x\rightarrow \infty} \frac{e^nn!}{n^n\sqrt{n}}=\sqrt{2\pi}</script><p>这个竞赛用的比较多，和$wallis$有联系。</p><h6 id="泊松积分"><a href="#泊松积分" class="headerlink" title="泊松积分"></a>泊松积分</h6><script type="math/tex; mode=display">\int_{-\infty}^{+\infty}{e^{-x^2}dx}=\sqrt{\pi}</script><p>证法也比较多，在正态分布中运用较多。</p><h6 id="无穷积分"><a href="#无穷积分" class="headerlink" title="无穷积分"></a>无穷积分</h6><script type="math/tex; mode=display">\int_0^{\infty}{\frac{\ln ^3xdx}{\left( 1+x^2 \right)}}=\frac{\pi ^3}{8}</script><p>很多积分出来都有$\pi$出现，这个也不例外</p><h6 id="拉马努金连分式"><a href="#拉马努金连分式" class="headerlink" title="拉马努金连分式"></a>拉马努金连分式</h6><script type="math/tex; mode=display">\sqrt{\frac{1+\sqrt{5}}{2}+2}-\frac{1+\sqrt{5}}{2}=\frac{e^{-\frac{2}{5}\pi}}{1+\frac{e^{-2\pi}}{1+\frac{e^{-4\pi}}{1+\frac{e^{-6\pi}}{\cdots}}}}</script><p>这么鬼畜当然是我们的灵感数学家啦！</p><h6 id="无穷级数"><a href="#无穷级数" class="headerlink" title="无穷级数"></a>无穷级数</h6><script type="math/tex; mode=display">\sum_{n=1}^{\infty}{\frac{1}{1+\left( n\pi \right) ^2}}=\frac{1}{e^2-1}</script><p>这个无穷级数咋之前介绍过，方法很多，蛮简单的。</p><h6 id="BBP公式"><a href="#BBP公式" class="headerlink" title="BBP公式"></a>BBP公式</h6><script type="math/tex; mode=display">\pi =\sum_{n=0}^{\infty}{\frac{1}{16^n}\left( \frac{4}{8n+1}-\frac{2}{8n+4}-\frac{1}{8n+5}-\frac{1}{8n+6} \right)}</script><p>很鬼畜，证法很巧妙，有兴趣可以看看证明，虽然证明过于奇巧用处不大，但是这个公式本身还是很有用很牛逼的，理论上可以将$\pi$任意位求出，下面贴上用JAVA实现BBP算法</p><pre><code>private static final BigDecimal ONE = BigDecimal.ONE;private static final BigDecimal TWO = new BigDecimal(2);private static final BigDecimal FOUR = new BigDecimal(4);private static final BigDecimal FIVE = new BigDecimal(5);private static final BigDecimal SIX = new BigDecimal(6);private static final BigDecimal EIGHT = new BigDecimal(8);private static final BigDecimal SIXTEEN = new BigDecimal(16);BigDecimal calc16dPI(int d) {    return FOUR.multiply(calc16dSj(d, 1)).add(BigDecimal.valueOf(3)).subtract(TWO.multiply(calc16dSj(d, 4)).divideAndRemainder(ONE)[1]).subtract(calc16dSj(d, 5).divideAndRemainder(ONE)[1]).subtract(calc16dSj(d, 6).divideAndRemainder(ONE)[1]).divideAndRemainder(ONE)[1];}BigDecimal calc16dSj(int d, int j) {    int ACCURACY = d + 10;    BigDecimal part1 = BigDecimal.ZERO;    BigDecimal part2 = BigDecimal.ZERO;    for (int k = 0; k &lt;= d; k++) {    part1 = part1.add(SIXTEEN.pow(d - k).divideAndRemainder(EIGHT.multiply(BigDecimal.valueOf(k)).add(BigDecimal.valueOf(j)))[1].divide(EIGHT.multiply(BigDecimal.valueOf(k)).add(BigDecimal.valueOf(j)), ACCURACY, BigDecimal.ROUND_HALF_UP));    }    for (int k = d + 1; k &lt; ACCURACY; k++) {        part2 = part2.add(ONE.divide(SIXTEEN.pow(k - d).multiply(EIGHT.multiply(BigDecimal.valueOf(k)).add(BigDecimal.valueOf(j))), ACCURACY, BigDecimal.ROUND_HALF_UP));    }    return part1.add(part2);}@Testpublic void mainCalc() {    for (int d = 0; d &lt; 100; d++) {        System.out.println(&quot;index of &quot; + (d + 1) + &quot;: &quot; + calc16dPI(d).multiply(SIXTEEN));    }}</code></pre><h6 id="蒙特卡洛算法"><a href="#蒙特卡洛算法" class="headerlink" title="蒙特卡洛算法"></a>蒙特卡洛算法</h6><p>基于无限大可重复实验的样本测试得到的概率进行估计$\pi$<br>方法比较简单可以用面积、蒲风投针、整数互质操作，下面是基于小学生的面积问题实现的，很简单：</p><pre><code>from random import random     def estimatePI(times):       hits = 0      for i in range(times):           x = random()*2 - 1           y = random()*2 - 1           if x*x + y*y &lt;= 1:                hits += 1      return 4.0 * hits/times   print(estimatePI(10000))   print(estimatePI(1000000))   print(estimatePI(100000000))   print(estimatePI(1000000000))</code></pre><p>  至于蒲风投针和整数互质emm还是敲一下吧</p><h6 id="任意取两个整数互质的概率为-frac-6-pi-2"><a href="#任意取两个整数互质的概率为-frac-6-pi-2" class="headerlink" title="任意取两个整数互质的概率为\frac{6}{\pi^2}"></a>任意取两个整数互质的概率为<script type="math/tex">\frac{6}{\pi^2}</script></h6><p>证明也很有意思，小编写着写着就开始越发觉得自己的卑微，科学的奇妙</p><h6 id="平面上画有等间距的平行线，间距为2a，向平面投掷一枚长为a的针，针与平行线相交的概率为-frac-1-pi"><a href="#平面上画有等间距的平行线，间距为2a，向平面投掷一枚长为a的针，针与平行线相交的概率为-frac-1-pi" class="headerlink" title="平面上画有等间距的平行线，间距为2a，向平面投掷一枚长为a的针，针与平行线相交的概率为$\frac{1}{\pi}$"></a>平面上画有等间距的平行线，间距为2a，向平面投掷一枚长为a的针，针与平行线相交的概率为$\frac{1}{\pi}$</h6><p>用几何概型求出来的，颇有意思。<br>今天介绍到这里，如果喜欢可以介绍一下身边喜欢数学的人，谢谢啦！</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学文化 </tag>
            
            <tag> π </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WHUT杯数学竞赛</title>
      <link href="/posts/655684ad.html"/>
      <url>/posts/655684ad.html</url>
      
        <content type="html"><![CDATA[<h2 id="WHUT杯数学竞赛好题几例"><a href="#WHUT杯数学竞赛好题几例" class="headerlink" title="WHUT杯数学竞赛好题几例"></a>WHUT杯数学竞赛好题几例</h2><h4 id="雨中漫步"><a href="#雨中漫步" class="headerlink" title="雨中漫步"></a>雨中漫步</h4><p>(1)$\displaystyle{\int_{-\infty}^{+\infty} \frac{d x}{\left(x^{2}+2 x+2\right)^{n}}}$</p><p>解：<br>$x+1=\tan t\text{进行换元}:$</p><script type="math/tex; mode=display">\int_{-\infty}^{+\infty}{\frac{dx}{\left( x^2+2x+2 \right) ^n}}=\int_{-\infty}^{+\infty}{\cos ^{2n-2}t}dt</script><p>$\text{根据}wallis\text{公式}:$</p><script type="math/tex; mode=display">\int_{-\infty}^{+\infty}{\cos ^{n-2}t}dt=2\cdot \frac{\left( 2n-3 \right) !!}{\left( 2n-2 \right) !!}\cdot \frac{\pi}{2}</script><script type="math/tex; mode=display">\qquad \qquad  =\frac{\left( 2n-3 \right) !!}{\left( 2n-2 \right) !!}\pi</script><p>(2)求极限: $\lim\limits_{n\rightarrow \infty} \left( b^{\frac{1}{n}}-1 \right) \sum\limits_{i=0}^{n-1}{b^{\frac{i}{n}}\sin b^{\frac{2i+1}{2n}}}\ \ \left( b&gt;1 \right)$</p><p>解：</p><script type="math/tex; mode=display">\lim_{n\rightarrow \infty} \left( b^{\frac{1}{n}}-1 \right) \sum_{i=0}^{n-1}{b^{\frac{i}{n}}\sin b^{\frac{2i+1}{2n}}}</script><script type="math/tex; mode=display">=\lim_{n\rightarrow \infty} \left( b^{\frac{i+1}{n}}-b^{\frac{i}{n}} \right) \sum_{i=0}^{n-1}{\sin b^{\frac{2i+1}{2n}}}</script><p>$\text{考虑}区\text{间划分}:$</p><script type="math/tex; mode=display">\Delta _i=b^{\frac{i+1}{n}}-b^{\frac{i}{n}}</script><p>$\text{由定积分定义}$</p><script type="math/tex; mode=display">\lim_{n\rightarrow \infty} \left( b^{\frac{1}{n}}-1 \right) \sum_{i=0}^{n-1}{b^{\frac{i}{n}}\sin b^{\frac{2i+1}{2n}}}=\int_1^b{\sin x}dx=\cos 1-\cos b</script><p>$\text{我们经常见到的黎曼积分定义求极限是}$</p><script type="math/tex; mode=display">\Delta _i=\frac{k+1}{n}-\frac{k}{n}=\frac{1}{n}</script><p>$\text{等间距划分，很少遇到这种}$</p><script type="math/tex; mode=display">\Delta _i=b^{\frac{i+1}{n}}-b^{\frac{i}{n}}</script><p>非线性划分，望大家在学习过程中多些思考理解，少些方法套路</p><p>(3)将5个A和5个a有序排列,其中有种序列任意前k个数k=1,2…10,A的个数多于a的个数,例如:(A,a,A,a,A,a,A,A,a,a)满足条件,而(A,a,A,a,A,a,a,A,A,a)不满足,因为当k=7时,有4个a,3个A,试问这种序列有$\underline{\ \ \ 42\ \ \ \ }$种</p><p>解:我们将A视为进栈，a视为出栈，则这种序列对应着一种5个数据元素进出栈的方式，栈的特点是先进后出，不可能空栈出，也不可能满栈进，我们先不考虑无效进出栈的方式，那么10个A或a有五个A总共有$C_{10}^{5}$种方式，而对于每种错误排序方式如下图：<br>|   A  |<br>| —- |<br>|   a  |<br>|  A   |<br>|    a |<br>|   A  |<br>|   a  |<br>|   A  |<br>|  a   |<br>|  <font color="#dd0000">a</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |</p><p>从第九个数据开始出现问题，前八个元素操作导致栈为空，不能再执行第九个元素对应的出栈操作，此时假设我们将之前的九个元素<font color="#dd0000">取反（A变为a，a变为A）</font><br /><br>则变为：<br>|  <font color="#dd0000">a</font><br />   |<br>| —- |<br>|  <font color="#dd0000">A</font><br />   |<br>|  <font color="#dd0000">a</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |<br>|  <font color="#dd0000">a</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |<br>|  <font color="#dd0000">a</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |<br>|  <font color="#dd0000">A</font><br />   |</p><p>显然这两种序列是一一对应的（把第二种从前往后累加到1时，前面取反就回到第一个序列），而第二种序列的有6个A和4个a，排列方式为$C_{10}^{4}$种，因此成功有效进出栈的次数只有<script type="math/tex">C_{10}^{5}-C_{10}^{4}=42\text{种}</script></p><p>(4)已知:方程<script type="math/tex">x^2\ln a=x^2\ln x+a\ln x</script>有三个实根,求$a$的取值范围</p><p>解:先将问题转化为<script type="math/tex">\frac{x^2\ln a}{x^2+a}-\ln x=0</script>有三个根</p><p>又$x&gt;0$,因此问题可以转化为<script type="math/tex">\frac{x\ln a}{x+a}-\frac{\ln x}{2}=0</script>有三个根，令<script type="math/tex">f\left( x \right) =\frac{x\ln a}{x+a}-\frac{\ln x}{2}</script>则</p><script type="math/tex; mode=display">f'\left( x \right) =\frac{a\ln a}{\left( x+a \right) ^2}-\frac{1}{2x}</script><script type="math/tex; mode=display">=\frac{-x^2+\left( 2a\ln a-2a \right) x-a^2}{2x\left( x+a \right) ^2}</script><p>$f’(x)$必须有两个正根，这样$f\left( x \right)$才能有两个极值点,从而才有可能有三个根</p><script type="math/tex; mode=display">\therefore \Delta =4a^2\left( \ln ^2a-2\ln a \right) >0</script><p>$\text{且}2a\ln a-2a&gt;0\left( \text{保证}为\text{正根} \right) $</p><p>我们从而可以得出这个必要条件:<script type="math/tex">a>e^2</script><br>此时: <script type="math/tex">x_1=a\ln a-a-a\sqrt{\ln ^2a-2\ln a},</script> <script type="math/tex">x_2=a\ln a-a+a\sqrt{\ln ^2a-2\ln a}</script>代入$f(x)$可得：<script type="math/tex">f\left( x_1 \right) <0\text{，}f\left( x_2 \right) >0</script><br>$\therefore \text{当} a&gt;e^2\text{时，一定有三个}\text{解}$</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 数学竞赛 </tag>
            
            <tag> 定积分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使桃心跳动起来</title>
      <link href="/posts/4225a725.html"/>
      <url>/posts/4225a725.html</url>
      
        <content type="html"><![CDATA[<p>代码如下，时间紧促，下次详细说明</p><pre><code class="lang-matlab">syms x y z;f=(x^2+9/4*y^2+z^2-1)^3-x^2*z^3-9/80*y^2*z^3;f=matlabFunction(f);[x,y,z] = meshgrid(-1.5:.02:1.5,-1:.02:1,-1.5:.02:1.5);    % 画图范围,设置[-1.5,1.5]区间v = f(x,y,z);h = patch(isosurface(x,y,z,v,0));    %创建补片对象的句柄isonormals(x,y,z,v,h)        %计算法向量set(h,&#39;FaceColor&#39;,&#39;r&#39;,&#39;EdgeColor&#39;,&#39;None&#39;);alpha(1)      %设置透明度grid on; view([1,1,1]); axis equal; camlight; lighting gouraud  %设置光照axis off;pos1=get(gca,&#39;position&#39;)pos2=pos1;pos2(2)=pos2(2)+0.01;pos2(1)=pos2(1)-0.03;pos2(3)=pos2(3)+0.08;pos2(4)=pos2(4)+0.08;for ii=1:10    pause(1)    set(gca,&#39;position&#39;,pos1)    pause(0.1)    set(gca,&#39;position&#39;,pos2)    pause(0.1)    set(gca,&#39;position&#39;,pos1)    pause(0.1)    set(gca,&#39;position&#39;,pos2)    pause(0.1)    set(gca,&#39;position&#39;,pos1)    pause(0.1)    set(gca,&#39;position&#39;,pos2)    pause(0.1)    set(gca,&#39;position&#39;,pos1)    pause(0.1)    set(gca,&#39;position&#39;,pos2)    pause(0.1)    set(gca,&#39;position&#39;,pos1)end</code></pre><p><img src="https://imgkr.cn-bj.ufileos.com/5f665075-497a-4bea-a27e-9f0f649c9e89.gif" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> MATALB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 绘图 </tag>
            
            <tag> MATALB </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
